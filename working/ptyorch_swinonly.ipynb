{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # What I can do more\n","\n"," - [Done] Implement K-Fold Cross Validation\n"," - [Done] Backbone Freezing\n"," - More model Ensemble\n"," - Feature Engineering"]},{"cell_type":"markdown","metadata":{},"source":["# Install Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T08:53:22.082698Z","iopub.status.busy":"2021-12-12T08:53:22.082375Z"},"trusted":true},"outputs":[],"source":["# ! pip install git+https://github.com/rwightman/pytorch-image-models\n","! pip install -U wandb albumentations timm"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import cv2\n","import copy\n","import time\n","import random\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# For data manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# Pytorch Imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda import amp\n","\n","# Utils\n","import joblib\n","from tqdm import tqdm\n","from collections import defaultdict\n","\n","# Sklearn Imports\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, KFold\n","\n","# Pytorch Image Model Library\n","import timm\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","import os\n","\n","# For descriptive error messages\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"]},{"cell_type":"markdown","metadata":{},"source":["# CONFIG"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ROOT_DIR = '../input/petfinder-pawpularity-score'\n","TRAIN_DIR = f'{ROOT_DIR}/train'\n","TEST_DIR = f'{ROOT_DIR}/test'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import wandb\n","os.environ[\"WANDB_API_KEY\"] = 'd60a4af56f6cd9cccec7d9da1dbced7960b61310'\n","os.environ[\"WANDB_MODE\"] = \"dryrun\"\n","wandb.login(key='d60a4af56f6cd9cccec7d9da1dbced7960b61310')\n","wandb.init(project=\"petfinder-pawpularity-score\", entity=\"jiwon7258\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["CONFIG = dict(\n","    seed=42,\n","    backbone='swin_base_patch4_window7_224',\n","    train_batch_size=16,\n","    valid_batch_size=32,\n","    img_size=224,\n","    epochs=3,\n","    epochs_=100,\n","    learning_rate=1e-4,\n","    scheduler='CosineAnnealingLR',\n","    min_lr=1e-6,\n","    T_max=100,\n","    weight_decay=1e-6,\n","    n_accumulate=1,\n","    n_fold=5,\n","    num_classes=1,\n","    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n","    competition='PetFinder',\n","    _wandb_kernel_='deb',\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Set Seed for Reproducibility"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def set_seed(seed = 42) :\n","    # numpy\n","    np.random.seed(seed)\n","    # python\n","    random.seed(seed)\n","    # torch\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    \n","set_seed(CONFIG['seed'])"]},{"cell_type":"markdown","metadata":{},"source":["# Read the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_train_file_path(id):\n","    return f'{TRAIN_DIR}/{id}.jpg'\n","def get_test_file_path(id):\n","    return f'{TEST_DIR}/{id}.jpg'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = pd.read_csv(f'{ROOT_DIR}/train.csv')\n","# file_path에 해당하는 column을 만든다\n","df['file_path'] = df['Id'].apply(get_train_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# feature_cols를 통해 사용할 feature 목록을 관리한다\n","feature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]"]},{"cell_type":"markdown","metadata":{},"source":["# Create Folds"]},{"cell_type":"markdown","metadata":{},"source":["Pawpularity는 0~100 사이의 정수 값을 가진다. Stratifed"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_folds(df, n_s=5, n_grp = None):\n","    df['kfold'] = -1\n","\n","    if n_grp is None:\n","        skf = KFold(n_splits=n_s, random_state=CONFIG['seed'])\n","        target = df['Pawpularity']\n","    else:\n","        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG['seed'])\n","        # Pawpularity를 구간별로, n_grp 수만큼 자른다\n","        # 따라서 Pawpularity와 grp의 히스토그램 분포는 동일하다\n","        df['grp'] = pd.cut(df['Pawpularity'], n_grp, labels=False)\n","        target = df['grp']\n","\n","    # n_grp의 분포를 기반으로 StratifiedKFold를 진행한다\n","    for fold_no, (t,v) in enumerate(skf.split(target,target)):\n","        df.loc[v, 'kfold'] = fold_no\n","\n","    df = df.drop('grp', axis = 1)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df = create_folds(df, n_s=CONFIG['n_fold'], n_grp=14)\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["Pawpularity는 0~100까지의 정수로 이루어져 있다. 이 분포 그대로 KFold를 진행하지 않는다. 대신 n_grp개 만큼, 일정한 길이별로 구간을 나눈 후, 이 분포를 이용하여 KFold를 진행한다.\n","\n","```df.Pawpularity.hist(bins=14) == df.grp.hist(bins=14)```\n","\n","![Pawpularity Histogram](./img/paupularity.png)\n","\n","![이미지](./img/pawpularity_and_grp_hist.png)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Class "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class PawpularityDataset(Dataset):\n","    def __init__(self, root_dir, df, transforms=None):\n","        self.root_dir = root_dir\n","        self.df = df\n","        self.file_names = df['file_path'].values    # numpy array\n","        self.targets = df['Pawpularity'].values     # numpy array\n","        self.transforms = transforms\n","\n","    # 데이터 프레임의 길이를 반환\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index) :\n","        img_path = self.file_names[index]\n","        img = cv2.imread(img_path)                  # numpy array\n","        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","        target=self.targets[index]\n","\n","        if self.transforms:\n","            img = self.transforms(image=img)['image']\n","        \n","        # 이미지 데이터, target label\n","        return img, target\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Augmentations"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_transforms = {\n","    \"train\": A.Compose([\n","        # 리사이징\n","        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n","        # 가로 반전\n","        A.HorizontalFlip(p=0.5),\n","        # 정규화\n","        A.Normalize(),\n","        ToTensorV2()\n","    ], p=1.),\n","\n","    \"valid\": A.Compose([\n","        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ])\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Create Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class PawpularityModel(nn.Module):\n","    def __init__ (self, backbone, pretrained=True):\n","        super().__init__()\n","        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n","        self.backbone.reset_classifier(0)\n","        o = self.backbone(torch.randn(1,3,CONFIG['img_size'],CONFIG['img_size']))\n","        self.n_features = o.shape[-1]\n","        for (name, param) in self.backbone.named_parameters():\n","            if 'layers.3.blocks' not in name:\n","                param.requires_grad=False\n","        self.backbone.norm.requires_grad_()\n","        self.fc = nn.Linear(self.n_features, CONFIG['num_classes'])\n","\n","    def forward (self, images):\n","        features = self.backbone(images)            # features = (batch size, embedding_size)\n","        output = self.fc(features)                  # outputs = (batch_size, num_classes)\n","        return output\n","\n","model = PawpularityModel(CONFIG['backbone'])\n","model.to(CONFIG['device'])"]},{"cell_type":"markdown","metadata":{},"source":["## Load Pretrained Weight"]},{"cell_type":"markdown","metadata":{},"source":["## Unfreeze"]},{"cell_type":"markdown","metadata":{},"source":["# Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def criterion(outputs, targets):\n","    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))"]},{"cell_type":"markdown","metadata":{},"source":["# Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n","    # train 모드로 변경\n","    model.train()\n","\n","    # for the Mixed Precision\n","    # Pytorch 예제 : https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples\n","    scaler = amp.GradScaler()\n","\n","    dataset_size = 0\n","    running_loss = 0\n","\n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","\n","    for step, (images, targets) in bar:\n","        images = images.to(device, dtype=torch.float)\n","        targets = targets.to(device, dtype=torch.float)\n","\n","        batch_size = images.size(0)\n","\n","        with amp.autocast(enabled=True):\n","            outputs = model(images)\n","            loss = criterion(outputs,targets)\n","            loss = loss / CONFIG['n_accumulate']\n","\n","        # loss를 Scale\n","        # Scaled Grdients를 계산(call)하기 위해 scaled loss를 backward()\n","        scaler.scale(loss).backward()\n","\n","        if (step + 1) % CONFIG['n_accumulate'] == 0:\n","            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n","            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n","            # otherwise, optimizer.step() is skipped.\n","            scaler.step(optimizer)\n","            # Updates the scale for next iteration.\n","            scaler.update()\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            if scheduler is not None:\n","                scheduler.step()\n","\n","        # loss.item()은 loss를 Python Float으로 반환\n","        # loss.item()은 batch data의 average loss이므로, sum of loss를 구하기 위해 batch_size를 곱해준다\n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","\n","        epoch_loss = running_loss / dataset_size\n","\n","        bar.set_postfix(Epoch = epoch, Train_Loss = epoch_loss, LR = optimizer.param_groups[0]['lr'])\n","\n","    # Garbage Collector\n","    gc.collect()\n","\n","    return epoch_loss\n"]},{"cell_type":"markdown","metadata":{},"source":["# Validation Function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def valid_one_epoch(model, dataloader, device, epoch):\n","    model.eval()\n","\n","    dataset_size= 0\n","    running_loss = 0\n","\n","    TARGETS= []\n","    PREDS = []\n","\n","    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n","\n","    for step, (images, targets) in bar:\n","        images = images.to(device, dtype=torch.float)\n","        targets = targets.to(device, dtype=torch.float)\n","\n","        batch_size = images.size(0)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, targets)\n","\n","        running_loss += (loss.item() * batch_size)\n","        dataset_size += batch_size\n","\n","        epoch_loss = running_loss / dataset_size\n","\n","        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n","        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n","\n","        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, LR = optimizer.param_groups[0]['lr'])\n","    \n","    TARGETS = np.concatenate(TARGETS)\n","    PREDS = np.concatenate(PREDS)\n","    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n","    \n","    gc.collect()\n","\n","\n","    return epoch_loss, val_rmse"]},{"cell_type":"markdown","metadata":{},"source":["# Run Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def run_training(model, optimizer, scheduler, device, num_epochs):\n","    # To automatically log graidents\n","    wandb.watch(model, log_freq=100)\n","\n","    if torch.cuda.is_available():\n","        print(\"[INFO] Using GPU:{}\\n\".format(torch.cuda.get_device_name()))\n","\n","    start = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_epoch_rmse=np.inf\n","    history=defaultdict(list)\n","\n","    # K-fold Validation : epoch마다 fold가 달라진다\n","    # num_epochs만큼, train과 val을 실행한다\n","    for epoch in range(1,num_epochs +1):\n","        gc.collect()\n","        \n","        fold = epoch % CONFIG['n_fold']\n","        # train one epoch\n","        print(f\"Validation Fold : {fold}\")\n","        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader=train_loader[fold], device=CONFIG['device'], epoch=epoch)\n","        val_epoch_loss, val_epoch_rmse = valid_one_epoch(model, valid_loader[fold], device=CONFIG['device'], epoch=epoch)\n","\n","        history['Train Loss'].append(train_epoch_loss)\n","        history['Valid Loss'].append(val_epoch_loss)\n","        history['Valid RMSE'].append(val_epoch_rmse)\n","\n","        # Log the metrics\n","        wandb.log({\"Train Loss\" : train_epoch_loss})\n","        wandb.log({'Valid Loss' : val_epoch_loss})\n","        wandb.log({'Valid RMSE' : val_epoch_rmse})\n","\n","        print(f'Valid RMSE : {val_epoch_rmse}')\n","\n","\n","        # deep copy the model\n","        if val_epoch_rmse <= best_epoch_rmse:\n","            print(f'Validation Loss improved( {best_epoch_rmse} ---> {val_epoch_rmse}  )')\n","            best_epoch_rmse = val_epoch_rmse\n","            # run.summary['Best RMSE'] = best_epoch_rmse\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            PATH = 'RMSE{:.4f}_epoch{:.0f}.bin'.format(best_epoch_rmse, epoch)\n","            torch.save(model.state_dict(), PATH)\n","            torch.save(model.state_dict(), 'best_wts.bin')\n","            # Save a model file from the current directory\n","            wandb.save(PATH)\n","            print(f'Model Saved')\n","\n","        print()\n","\n","    end = time.time()\n","    time_elapsed = end - start\n","    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n","    print('Best RMSE: {:.4f}'.format(best_epoch_rmse))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    return model, history"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''\n","fold : 여러개의 fold 중, validation set으로 이용할 fold #\n","fold 이외의 folds는 train set으로 이용된다\n","'''\n","def prepare_loaders(fold):\n","    df_train = df[df.kfold != fold].reset_index(drop=True)\n","    df_valid = df[df.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms=data_transforms['train'])\n","    valid_dataset = PawpularityDataset(TRAIN_DIR, df_valid, transforms=data_transforms['valid'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], num_workers=4, shuffle=True, pin_memory = True, drop_last = True)\n","    valid_loader = DataLoader (valid_dataset, batch_size=CONFIG['valid_batch_size'], num_workers=4, shuffle=False, pin_memory=True)\n","\n","    return train_loader, valid_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def fetch_scheduler(optimizer):\n","    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n","        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['T_max'], eta_min = CONFIG['min_lr'])\n","    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n","        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n","                                                             eta_min=CONFIG['min_lr'])\n","    elif CONFIG['scheduler'] == None:\n","        return None\n","        \n","    return scheduler"]},{"cell_type":"markdown","metadata":{},"source":["## Create Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_loader = dict()\n","valid_loader = dict()\n","\n","for f in range(CONFIG['n_fold']) :\n","    train_, valid_ = prepare_loaders(f)\n","    train_loader[f] = (train_)\n","    valid_loader[f] = (valid_)"]},{"cell_type":"markdown","metadata":{},"source":["## Define Optimizer and Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["optimizer = optim.Adam(model.parameters(), lr = CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n","scheduler = fetch_scheduler(optimizer)"]},{"cell_type":"markdown","metadata":{},"source":["## Start Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model, history = run_training(model, optimizer, scheduler, device=CONFIG['device'], num_epochs=CONFIG['epochs'])"]},{"cell_type":"markdown","metadata":{},"source":["## Load FineTuned Weight"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["MODEL_PATH = 'best_wts.bin'\n","model.load_state_dict(torch.load(MODEL_PATH ,map_location=CONFIG['device']))"]},{"cell_type":"markdown","metadata":{},"source":["## Unfreeze All"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for param in model.parameters():\n","    param.requires_grad=True()"]},{"cell_type":"markdown","metadata":{},"source":["## Start Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model, history = run_training(model, optimizer, scheduler, device=CONFIG['device'], num_epochs=CONFIG['epochs'])"]},{"cell_type":"markdown","metadata":{},"source":["# TEST and SUBMIT"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:09.957635Z","iopub.status.busy":"2021-12-12T07:14:09.956906Z","iopub.status.idle":"2021-12-12T07:14:10.244447Z","shell.execute_reply":"2021-12-12T07:14:10.2435Z","shell.execute_reply.started":"2021-12-12T07:14:09.957588Z"},"trusted":true},"outputs":[],"source":["MODEL_PATH = '../input/pretrained-model/RMSE4.8957_epoch27.bin'\n","model.load_state_dict(torch.load(MODEL_PATH ,map_location=CONFIG['device']))"]},{"cell_type":"markdown","metadata":{},"source":["## Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.246527Z","iopub.status.busy":"2021-12-12T07:14:10.246202Z","iopub.status.idle":"2021-12-12T07:14:10.256256Z","shell.execute_reply":"2021-12-12T07:14:10.255282Z","shell.execute_reply.started":"2021-12-12T07:14:10.246482Z"},"trusted":true},"outputs":[],"source":["class PawpularityTestDataset(Dataset):\n","    def __init__(self, root_dir, df, transforms=None):\n","        self.root_dir = root_dir\n","        self.df = df\n","        self.file_names = df['file_path'].values  # numpy array\n","        self.transforms = transforms\n","\n","    # 데이터 프레임의 길이를 반환\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        img_path = self.file_names[index]\n","        img = cv2.imread(img_path)  # numpy array\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        Id = self.df.Id[index]\n","\n","        if self.transforms:\n","            img = self.transforms(image=img)['image']\n","\n","        # 이미지 데이터, target label\n","        return img, Id\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.258376Z","iopub.status.busy":"2021-12-12T07:14:10.257802Z","iopub.status.idle":"2021-12-12T07:14:10.285445Z","shell.execute_reply":"2021-12-12T07:14:10.284338Z","shell.execute_reply.started":"2021-12-12T07:14:10.258308Z"},"trusted":true},"outputs":[],"source":["df_test = pd.read_csv(f'{ROOT_DIR}/test.csv')\n","# file_path에 해당하는 column을 만든다\n","df_test['file_path'] = df_test['Id'].apply(get_test_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.287398Z","iopub.status.busy":"2021-12-12T07:14:10.286892Z","iopub.status.idle":"2021-12-12T07:14:10.301186Z","shell.execute_reply":"2021-12-12T07:14:10.299835Z","shell.execute_reply.started":"2021-12-12T07:14:10.287356Z"},"trusted":true},"outputs":[],"source":["test_dataset = PawpularityTestDataset(root_dir = TRAIN_DIR, df = df_test, transforms=data_transforms['valid'])\n","test_loader = DataLoader(dataset = test_dataset, batch_size=CONFIG['valid_batch_size'], shuffle=False, pin_memory=True,num_workers=4)\n","# test_loader = DataLoader(dataset=test_dataset,\n","#                          batch_size=4,\n","#                          shuffle=False,\n","#                          pin_memory=True,\n","#                          num_workers=4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.303894Z","iopub.status.busy":"2021-12-12T07:14:10.302795Z","iopub.status.idle":"2021-12-12T07:14:10.332073Z","shell.execute_reply":"2021-12-12T07:14:10.330921Z","shell.execute_reply.started":"2021-12-12T07:14:10.303835Z"},"trusted":true},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.334599Z","iopub.status.busy":"2021-12-12T07:14:10.333741Z","iopub.status.idle":"2021-12-12T07:14:10.808287Z","shell.execute_reply":"2021-12-12T07:14:10.807185Z","shell.execute_reply.started":"2021-12-12T07:14:10.33455Z"},"trusted":true},"outputs":[],"source":["print(len(test_loader))\n","with torch.no_grad():\n","    model.eval()\n","\n","    total_size = 0\n","    outputs = []\n","    Ids = []\n","\n","    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n","\n","    for step, (images, Id) in bar:\n","        images = images.cuda()\n","\n","        batch_size = images.shape[0]\n","\n","        output = model(images)\n","\n","        total_size += batch_size\n","\n","        outputs.append(output.cpu())\n","        Ids.extend(Id)\n","\n","outputs = np.concatenate(outputs, axis = 0).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.810444Z","iopub.status.busy":"2021-12-12T07:14:10.810083Z","iopub.status.idle":"2021-12-12T07:14:10.820099Z","shell.execute_reply":"2021-12-12T07:14:10.819093Z","shell.execute_reply.started":"2021-12-12T07:14:10.810403Z"},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame({\n","    'Id' : Ids,\n","    'Pawpularity' : outputs\n","})\n","submission.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.822446Z","iopub.status.busy":"2021-12-12T07:14:10.821891Z","iopub.status.idle":"2021-12-12T07:14:10.843772Z","shell.execute_reply":"2021-12-12T07:14:10.840394Z","shell.execute_reply.started":"2021-12-12T07:14:10.822397Z"},"trusted":true},"outputs":[],"source":["submission"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
