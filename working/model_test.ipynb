{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiwon/anaconda3/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# model = timm.create_model('tf_efficientnetv2_s')  \n",
    "model = timm.create_model('swin_base_patch4_window12_384')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'patch_embed.proj.weight'\n",
      "'patch_embed.proj.bias'\n",
      "'patch_embed.norm.weight'\n",
      "'patch_embed.norm.bias'\n",
      "'layers.0.blocks.0.norm1.weight'\n",
      "'layers.0.blocks.0.norm1.bias'\n",
      "'layers.0.blocks.0.attn.relative_position_bias_table'\n",
      "'layers.0.blocks.0.attn.qkv.weight'\n",
      "'layers.0.blocks.0.attn.qkv.bias'\n",
      "'layers.0.blocks.0.attn.proj.weight'\n",
      "'layers.0.blocks.0.attn.proj.bias'\n",
      "'layers.0.blocks.0.norm2.weight'\n",
      "'layers.0.blocks.0.norm2.bias'\n",
      "'layers.0.blocks.0.mlp.fc1.weight'\n",
      "'layers.0.blocks.0.mlp.fc1.bias'\n",
      "'layers.0.blocks.0.mlp.fc2.weight'\n",
      "'layers.0.blocks.0.mlp.fc2.bias'\n",
      "'layers.0.blocks.1.norm1.weight'\n",
      "'layers.0.blocks.1.norm1.bias'\n",
      "'layers.0.blocks.1.attn.relative_position_bias_table'\n",
      "'layers.0.blocks.1.attn.qkv.weight'\n",
      "'layers.0.blocks.1.attn.qkv.bias'\n",
      "'layers.0.blocks.1.attn.proj.weight'\n",
      "'layers.0.blocks.1.attn.proj.bias'\n",
      "'layers.0.blocks.1.norm2.weight'\n",
      "'layers.0.blocks.1.norm2.bias'\n",
      "'layers.0.blocks.1.mlp.fc1.weight'\n",
      "'layers.0.blocks.1.mlp.fc1.bias'\n",
      "'layers.0.blocks.1.mlp.fc2.weight'\n",
      "'layers.0.blocks.1.mlp.fc2.bias'\n",
      "'layers.0.downsample.reduction.weight'\n",
      "'layers.0.downsample.norm.weight'\n",
      "'layers.0.downsample.norm.bias'\n",
      "'layers.1.blocks.0.norm1.weight'\n",
      "'layers.1.blocks.0.norm1.bias'\n",
      "'layers.1.blocks.0.attn.relative_position_bias_table'\n",
      "'layers.1.blocks.0.attn.qkv.weight'\n",
      "'layers.1.blocks.0.attn.qkv.bias'\n",
      "'layers.1.blocks.0.attn.proj.weight'\n",
      "'layers.1.blocks.0.attn.proj.bias'\n",
      "'layers.1.blocks.0.norm2.weight'\n",
      "'layers.1.blocks.0.norm2.bias'\n",
      "'layers.1.blocks.0.mlp.fc1.weight'\n",
      "'layers.1.blocks.0.mlp.fc1.bias'\n",
      "'layers.1.blocks.0.mlp.fc2.weight'\n",
      "'layers.1.blocks.0.mlp.fc2.bias'\n",
      "'layers.1.blocks.1.norm1.weight'\n",
      "'layers.1.blocks.1.norm1.bias'\n",
      "'layers.1.blocks.1.attn.relative_position_bias_table'\n",
      "'layers.1.blocks.1.attn.qkv.weight'\n",
      "'layers.1.blocks.1.attn.qkv.bias'\n",
      "'layers.1.blocks.1.attn.proj.weight'\n",
      "'layers.1.blocks.1.attn.proj.bias'\n",
      "'layers.1.blocks.1.norm2.weight'\n",
      "'layers.1.blocks.1.norm2.bias'\n",
      "'layers.1.blocks.1.mlp.fc1.weight'\n",
      "'layers.1.blocks.1.mlp.fc1.bias'\n",
      "'layers.1.blocks.1.mlp.fc2.weight'\n",
      "'layers.1.blocks.1.mlp.fc2.bias'\n",
      "'layers.1.downsample.reduction.weight'\n",
      "'layers.1.downsample.norm.weight'\n",
      "'layers.1.downsample.norm.bias'\n",
      "'layers.2.blocks.0.norm1.weight'\n",
      "'layers.2.blocks.0.norm1.bias'\n",
      "'layers.2.blocks.0.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.0.attn.qkv.weight'\n",
      "'layers.2.blocks.0.attn.qkv.bias'\n",
      "'layers.2.blocks.0.attn.proj.weight'\n",
      "'layers.2.blocks.0.attn.proj.bias'\n",
      "'layers.2.blocks.0.norm2.weight'\n",
      "'layers.2.blocks.0.norm2.bias'\n",
      "'layers.2.blocks.0.mlp.fc1.weight'\n",
      "'layers.2.blocks.0.mlp.fc1.bias'\n",
      "'layers.2.blocks.0.mlp.fc2.weight'\n",
      "'layers.2.blocks.0.mlp.fc2.bias'\n",
      "'layers.2.blocks.1.norm1.weight'\n",
      "'layers.2.blocks.1.norm1.bias'\n",
      "'layers.2.blocks.1.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.1.attn.qkv.weight'\n",
      "'layers.2.blocks.1.attn.qkv.bias'\n",
      "'layers.2.blocks.1.attn.proj.weight'\n",
      "'layers.2.blocks.1.attn.proj.bias'\n",
      "'layers.2.blocks.1.norm2.weight'\n",
      "'layers.2.blocks.1.norm2.bias'\n",
      "'layers.2.blocks.1.mlp.fc1.weight'\n",
      "'layers.2.blocks.1.mlp.fc1.bias'\n",
      "'layers.2.blocks.1.mlp.fc2.weight'\n",
      "'layers.2.blocks.1.mlp.fc2.bias'\n",
      "'layers.2.blocks.2.norm1.weight'\n",
      "'layers.2.blocks.2.norm1.bias'\n",
      "'layers.2.blocks.2.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.2.attn.qkv.weight'\n",
      "'layers.2.blocks.2.attn.qkv.bias'\n",
      "'layers.2.blocks.2.attn.proj.weight'\n",
      "'layers.2.blocks.2.attn.proj.bias'\n",
      "'layers.2.blocks.2.norm2.weight'\n",
      "'layers.2.blocks.2.norm2.bias'\n",
      "'layers.2.blocks.2.mlp.fc1.weight'\n",
      "'layers.2.blocks.2.mlp.fc1.bias'\n",
      "'layers.2.blocks.2.mlp.fc2.weight'\n",
      "'layers.2.blocks.2.mlp.fc2.bias'\n",
      "'layers.2.blocks.3.norm1.weight'\n",
      "'layers.2.blocks.3.norm1.bias'\n",
      "'layers.2.blocks.3.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.3.attn.qkv.weight'\n",
      "'layers.2.blocks.3.attn.qkv.bias'\n",
      "'layers.2.blocks.3.attn.proj.weight'\n",
      "'layers.2.blocks.3.attn.proj.bias'\n",
      "'layers.2.blocks.3.norm2.weight'\n",
      "'layers.2.blocks.3.norm2.bias'\n",
      "'layers.2.blocks.3.mlp.fc1.weight'\n",
      "'layers.2.blocks.3.mlp.fc1.bias'\n",
      "'layers.2.blocks.3.mlp.fc2.weight'\n",
      "'layers.2.blocks.3.mlp.fc2.bias'\n",
      "'layers.2.blocks.4.norm1.weight'\n",
      "'layers.2.blocks.4.norm1.bias'\n",
      "'layers.2.blocks.4.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.4.attn.qkv.weight'\n",
      "'layers.2.blocks.4.attn.qkv.bias'\n",
      "'layers.2.blocks.4.attn.proj.weight'\n",
      "'layers.2.blocks.4.attn.proj.bias'\n",
      "'layers.2.blocks.4.norm2.weight'\n",
      "'layers.2.blocks.4.norm2.bias'\n",
      "'layers.2.blocks.4.mlp.fc1.weight'\n",
      "'layers.2.blocks.4.mlp.fc1.bias'\n",
      "'layers.2.blocks.4.mlp.fc2.weight'\n",
      "'layers.2.blocks.4.mlp.fc2.bias'\n",
      "'layers.2.blocks.5.norm1.weight'\n",
      "'layers.2.blocks.5.norm1.bias'\n",
      "'layers.2.blocks.5.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.5.attn.qkv.weight'\n",
      "'layers.2.blocks.5.attn.qkv.bias'\n",
      "'layers.2.blocks.5.attn.proj.weight'\n",
      "'layers.2.blocks.5.attn.proj.bias'\n",
      "'layers.2.blocks.5.norm2.weight'\n",
      "'layers.2.blocks.5.norm2.bias'\n",
      "'layers.2.blocks.5.mlp.fc1.weight'\n",
      "'layers.2.blocks.5.mlp.fc1.bias'\n",
      "'layers.2.blocks.5.mlp.fc2.weight'\n",
      "'layers.2.blocks.5.mlp.fc2.bias'\n",
      "'layers.2.blocks.6.norm1.weight'\n",
      "'layers.2.blocks.6.norm1.bias'\n",
      "'layers.2.blocks.6.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.6.attn.qkv.weight'\n",
      "'layers.2.blocks.6.attn.qkv.bias'\n",
      "'layers.2.blocks.6.attn.proj.weight'\n",
      "'layers.2.blocks.6.attn.proj.bias'\n",
      "'layers.2.blocks.6.norm2.weight'\n",
      "'layers.2.blocks.6.norm2.bias'\n",
      "'layers.2.blocks.6.mlp.fc1.weight'\n",
      "'layers.2.blocks.6.mlp.fc1.bias'\n",
      "'layers.2.blocks.6.mlp.fc2.weight'\n",
      "'layers.2.blocks.6.mlp.fc2.bias'\n",
      "'layers.2.blocks.7.norm1.weight'\n",
      "'layers.2.blocks.7.norm1.bias'\n",
      "'layers.2.blocks.7.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.7.attn.qkv.weight'\n",
      "'layers.2.blocks.7.attn.qkv.bias'\n",
      "'layers.2.blocks.7.attn.proj.weight'\n",
      "'layers.2.blocks.7.attn.proj.bias'\n",
      "'layers.2.blocks.7.norm2.weight'\n",
      "'layers.2.blocks.7.norm2.bias'\n",
      "'layers.2.blocks.7.mlp.fc1.weight'\n",
      "'layers.2.blocks.7.mlp.fc1.bias'\n",
      "'layers.2.blocks.7.mlp.fc2.weight'\n",
      "'layers.2.blocks.7.mlp.fc2.bias'\n",
      "'layers.2.blocks.8.norm1.weight'\n",
      "'layers.2.blocks.8.norm1.bias'\n",
      "'layers.2.blocks.8.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.8.attn.qkv.weight'\n",
      "'layers.2.blocks.8.attn.qkv.bias'\n",
      "'layers.2.blocks.8.attn.proj.weight'\n",
      "'layers.2.blocks.8.attn.proj.bias'\n",
      "'layers.2.blocks.8.norm2.weight'\n",
      "'layers.2.blocks.8.norm2.bias'\n",
      "'layers.2.blocks.8.mlp.fc1.weight'\n",
      "'layers.2.blocks.8.mlp.fc1.bias'\n",
      "'layers.2.blocks.8.mlp.fc2.weight'\n",
      "'layers.2.blocks.8.mlp.fc2.bias'\n",
      "'layers.2.blocks.9.norm1.weight'\n",
      "'layers.2.blocks.9.norm1.bias'\n",
      "'layers.2.blocks.9.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.9.attn.qkv.weight'\n",
      "'layers.2.blocks.9.attn.qkv.bias'\n",
      "'layers.2.blocks.9.attn.proj.weight'\n",
      "'layers.2.blocks.9.attn.proj.bias'\n",
      "'layers.2.blocks.9.norm2.weight'\n",
      "'layers.2.blocks.9.norm2.bias'\n",
      "'layers.2.blocks.9.mlp.fc1.weight'\n",
      "'layers.2.blocks.9.mlp.fc1.bias'\n",
      "'layers.2.blocks.9.mlp.fc2.weight'\n",
      "'layers.2.blocks.9.mlp.fc2.bias'\n",
      "'layers.2.blocks.10.norm1.weight'\n",
      "'layers.2.blocks.10.norm1.bias'\n",
      "'layers.2.blocks.10.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.10.attn.qkv.weight'\n",
      "'layers.2.blocks.10.attn.qkv.bias'\n",
      "'layers.2.blocks.10.attn.proj.weight'\n",
      "'layers.2.blocks.10.attn.proj.bias'\n",
      "'layers.2.blocks.10.norm2.weight'\n",
      "'layers.2.blocks.10.norm2.bias'\n",
      "'layers.2.blocks.10.mlp.fc1.weight'\n",
      "'layers.2.blocks.10.mlp.fc1.bias'\n",
      "'layers.2.blocks.10.mlp.fc2.weight'\n",
      "'layers.2.blocks.10.mlp.fc2.bias'\n",
      "'layers.2.blocks.11.norm1.weight'\n",
      "'layers.2.blocks.11.norm1.bias'\n",
      "'layers.2.blocks.11.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.11.attn.qkv.weight'\n",
      "'layers.2.blocks.11.attn.qkv.bias'\n",
      "'layers.2.blocks.11.attn.proj.weight'\n",
      "'layers.2.blocks.11.attn.proj.bias'\n",
      "'layers.2.blocks.11.norm2.weight'\n",
      "'layers.2.blocks.11.norm2.bias'\n",
      "'layers.2.blocks.11.mlp.fc1.weight'\n",
      "'layers.2.blocks.11.mlp.fc1.bias'\n",
      "'layers.2.blocks.11.mlp.fc2.weight'\n",
      "'layers.2.blocks.11.mlp.fc2.bias'\n",
      "'layers.2.blocks.12.norm1.weight'\n",
      "'layers.2.blocks.12.norm1.bias'\n",
      "'layers.2.blocks.12.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.12.attn.qkv.weight'\n",
      "'layers.2.blocks.12.attn.qkv.bias'\n",
      "'layers.2.blocks.12.attn.proj.weight'\n",
      "'layers.2.blocks.12.attn.proj.bias'\n",
      "'layers.2.blocks.12.norm2.weight'\n",
      "'layers.2.blocks.12.norm2.bias'\n",
      "'layers.2.blocks.12.mlp.fc1.weight'\n",
      "'layers.2.blocks.12.mlp.fc1.bias'\n",
      "'layers.2.blocks.12.mlp.fc2.weight'\n",
      "'layers.2.blocks.12.mlp.fc2.bias'\n",
      "'layers.2.blocks.13.norm1.weight'\n",
      "'layers.2.blocks.13.norm1.bias'\n",
      "'layers.2.blocks.13.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.13.attn.qkv.weight'\n",
      "'layers.2.blocks.13.attn.qkv.bias'\n",
      "'layers.2.blocks.13.attn.proj.weight'\n",
      "'layers.2.blocks.13.attn.proj.bias'\n",
      "'layers.2.blocks.13.norm2.weight'\n",
      "'layers.2.blocks.13.norm2.bias'\n",
      "'layers.2.blocks.13.mlp.fc1.weight'\n",
      "'layers.2.blocks.13.mlp.fc1.bias'\n",
      "'layers.2.blocks.13.mlp.fc2.weight'\n",
      "'layers.2.blocks.13.mlp.fc2.bias'\n",
      "'layers.2.blocks.14.norm1.weight'\n",
      "'layers.2.blocks.14.norm1.bias'\n",
      "'layers.2.blocks.14.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.14.attn.qkv.weight'\n",
      "'layers.2.blocks.14.attn.qkv.bias'\n",
      "'layers.2.blocks.14.attn.proj.weight'\n",
      "'layers.2.blocks.14.attn.proj.bias'\n",
      "'layers.2.blocks.14.norm2.weight'\n",
      "'layers.2.blocks.14.norm2.bias'\n",
      "'layers.2.blocks.14.mlp.fc1.weight'\n",
      "'layers.2.blocks.14.mlp.fc1.bias'\n",
      "'layers.2.blocks.14.mlp.fc2.weight'\n",
      "'layers.2.blocks.14.mlp.fc2.bias'\n",
      "'layers.2.blocks.15.norm1.weight'\n",
      "'layers.2.blocks.15.norm1.bias'\n",
      "'layers.2.blocks.15.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.15.attn.qkv.weight'\n",
      "'layers.2.blocks.15.attn.qkv.bias'\n",
      "'layers.2.blocks.15.attn.proj.weight'\n",
      "'layers.2.blocks.15.attn.proj.bias'\n",
      "'layers.2.blocks.15.norm2.weight'\n",
      "'layers.2.blocks.15.norm2.bias'\n",
      "'layers.2.blocks.15.mlp.fc1.weight'\n",
      "'layers.2.blocks.15.mlp.fc1.bias'\n",
      "'layers.2.blocks.15.mlp.fc2.weight'\n",
      "'layers.2.blocks.15.mlp.fc2.bias'\n",
      "'layers.2.blocks.16.norm1.weight'\n",
      "'layers.2.blocks.16.norm1.bias'\n",
      "'layers.2.blocks.16.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.16.attn.qkv.weight'\n",
      "'layers.2.blocks.16.attn.qkv.bias'\n",
      "'layers.2.blocks.16.attn.proj.weight'\n",
      "'layers.2.blocks.16.attn.proj.bias'\n",
      "'layers.2.blocks.16.norm2.weight'\n",
      "'layers.2.blocks.16.norm2.bias'\n",
      "'layers.2.blocks.16.mlp.fc1.weight'\n",
      "'layers.2.blocks.16.mlp.fc1.bias'\n",
      "'layers.2.blocks.16.mlp.fc2.weight'\n",
      "'layers.2.blocks.16.mlp.fc2.bias'\n",
      "'layers.2.blocks.17.norm1.weight'\n",
      "'layers.2.blocks.17.norm1.bias'\n",
      "'layers.2.blocks.17.attn.relative_position_bias_table'\n",
      "'layers.2.blocks.17.attn.qkv.weight'\n",
      "'layers.2.blocks.17.attn.qkv.bias'\n",
      "'layers.2.blocks.17.attn.proj.weight'\n",
      "'layers.2.blocks.17.attn.proj.bias'\n",
      "'layers.2.blocks.17.norm2.weight'\n",
      "'layers.2.blocks.17.norm2.bias'\n",
      "'layers.2.blocks.17.mlp.fc1.weight'\n",
      "'layers.2.blocks.17.mlp.fc1.bias'\n",
      "'layers.2.blocks.17.mlp.fc2.weight'\n",
      "'layers.2.blocks.17.mlp.fc2.bias'\n",
      "'layers.2.downsample.reduction.weight'\n",
      "'layers.2.downsample.norm.weight'\n",
      "'layers.2.downsample.norm.bias'\n",
      "'layers.3.blocks.0.norm1.weight'\n",
      "'layers.3.blocks.0.norm1.bias'\n",
      "'layers.3.blocks.0.attn.relative_position_bias_table'\n",
      "'layers.3.blocks.0.attn.qkv.weight'\n",
      "'layers.3.blocks.0.attn.qkv.bias'\n",
      "'layers.3.blocks.0.attn.proj.weight'\n",
      "'layers.3.blocks.0.attn.proj.bias'\n",
      "'layers.3.blocks.0.norm2.weight'\n",
      "'layers.3.blocks.0.norm2.bias'\n",
      "'layers.3.blocks.0.mlp.fc1.weight'\n",
      "'layers.3.blocks.0.mlp.fc1.bias'\n",
      "'layers.3.blocks.0.mlp.fc2.weight'\n",
      "'layers.3.blocks.0.mlp.fc2.bias'\n",
      "'layers.3.blocks.1.norm1.weight'\n",
      "'layers.3.blocks.1.norm1.bias'\n",
      "'layers.3.blocks.1.attn.relative_position_bias_table'\n",
      "'layers.3.blocks.1.attn.qkv.weight'\n",
      "'layers.3.blocks.1.attn.qkv.bias'\n",
      "'layers.3.blocks.1.attn.proj.weight'\n",
      "'layers.3.blocks.1.attn.proj.bias'\n",
      "'layers.3.blocks.1.norm2.weight'\n",
      "'layers.3.blocks.1.norm2.bias'\n",
      "'layers.3.blocks.1.mlp.fc1.weight'\n",
      "'layers.3.blocks.1.mlp.fc1.bias'\n",
      "'layers.3.blocks.1.mlp.fc2.weight'\n",
      "'layers.3.blocks.1.mlp.fc2.bias'\n",
      "'norm.weight'\n",
      "'norm.bias'\n",
      "'head.weight'\n",
      "'head.bias'\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    pprint(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 96, 96]           6,272\n",
      "         LayerNorm-2            [-1, 9216, 128]             256\n",
      "        PatchEmbed-3            [-1, 9216, 128]               0\n",
      "           Dropout-4            [-1, 9216, 128]               0\n",
      "         LayerNorm-5            [-1, 9216, 128]             256\n",
      "            Linear-6             [-1, 144, 384]          49,536\n",
      "           Softmax-7          [-1, 4, 144, 144]               0\n",
      "           Dropout-8          [-1, 4, 144, 144]               0\n",
      "            Linear-9             [-1, 144, 128]          16,512\n",
      "          Dropout-10             [-1, 144, 128]               0\n",
      "  WindowAttention-11             [-1, 144, 128]               0\n",
      "         Identity-12            [-1, 9216, 128]               0\n",
      "        LayerNorm-13            [-1, 9216, 128]             256\n",
      "           Linear-14            [-1, 9216, 512]          66,048\n",
      "             GELU-15            [-1, 9216, 512]               0\n",
      "          Dropout-16            [-1, 9216, 512]               0\n",
      "           Linear-17            [-1, 9216, 128]          65,664\n",
      "          Dropout-18            [-1, 9216, 128]               0\n",
      "              Mlp-19            [-1, 9216, 128]               0\n",
      "         Identity-20            [-1, 9216, 128]               0\n",
      "SwinTransformerBlock-21            [-1, 9216, 128]               0\n",
      "        LayerNorm-22            [-1, 9216, 128]             256\n",
      "           Linear-23             [-1, 144, 384]          49,536\n",
      "          Softmax-24          [-1, 4, 144, 144]               0\n",
      "          Dropout-25          [-1, 4, 144, 144]               0\n",
      "           Linear-26             [-1, 144, 128]          16,512\n",
      "          Dropout-27             [-1, 144, 128]               0\n",
      "  WindowAttention-28             [-1, 144, 128]               0\n",
      "         DropPath-29            [-1, 9216, 128]               0\n",
      "        LayerNorm-30            [-1, 9216, 128]             256\n",
      "           Linear-31            [-1, 9216, 512]          66,048\n",
      "             GELU-32            [-1, 9216, 512]               0\n",
      "          Dropout-33            [-1, 9216, 512]               0\n",
      "           Linear-34            [-1, 9216, 128]          65,664\n",
      "          Dropout-35            [-1, 9216, 128]               0\n",
      "              Mlp-36            [-1, 9216, 128]               0\n",
      "         DropPath-37            [-1, 9216, 128]               0\n",
      "SwinTransformerBlock-38            [-1, 9216, 128]               0\n",
      "        LayerNorm-39            [-1, 2304, 512]           1,024\n",
      "           Linear-40            [-1, 2304, 256]         131,072\n",
      "     PatchMerging-41            [-1, 2304, 256]               0\n",
      "       BasicLayer-42            [-1, 2304, 256]               0\n",
      "        LayerNorm-43            [-1, 2304, 256]             512\n",
      "           Linear-44             [-1, 144, 768]         197,376\n",
      "          Softmax-45          [-1, 8, 144, 144]               0\n",
      "          Dropout-46          [-1, 8, 144, 144]               0\n",
      "           Linear-47             [-1, 144, 256]          65,792\n",
      "          Dropout-48             [-1, 144, 256]               0\n",
      "  WindowAttention-49             [-1, 144, 256]               0\n",
      "         DropPath-50            [-1, 2304, 256]               0\n",
      "        LayerNorm-51            [-1, 2304, 256]             512\n",
      "           Linear-52           [-1, 2304, 1024]         263,168\n",
      "             GELU-53           [-1, 2304, 1024]               0\n",
      "          Dropout-54           [-1, 2304, 1024]               0\n",
      "           Linear-55            [-1, 2304, 256]         262,400\n",
      "          Dropout-56            [-1, 2304, 256]               0\n",
      "              Mlp-57            [-1, 2304, 256]               0\n",
      "         DropPath-58            [-1, 2304, 256]               0\n",
      "SwinTransformerBlock-59            [-1, 2304, 256]               0\n",
      "        LayerNorm-60            [-1, 2304, 256]             512\n",
      "           Linear-61             [-1, 144, 768]         197,376\n",
      "          Softmax-62          [-1, 8, 144, 144]               0\n",
      "          Dropout-63          [-1, 8, 144, 144]               0\n",
      "           Linear-64             [-1, 144, 256]          65,792\n",
      "          Dropout-65             [-1, 144, 256]               0\n",
      "  WindowAttention-66             [-1, 144, 256]               0\n",
      "         DropPath-67            [-1, 2304, 256]               0\n",
      "        LayerNorm-68            [-1, 2304, 256]             512\n",
      "           Linear-69           [-1, 2304, 1024]         263,168\n",
      "             GELU-70           [-1, 2304, 1024]               0\n",
      "          Dropout-71           [-1, 2304, 1024]               0\n",
      "           Linear-72            [-1, 2304, 256]         262,400\n",
      "          Dropout-73            [-1, 2304, 256]               0\n",
      "              Mlp-74            [-1, 2304, 256]               0\n",
      "         DropPath-75            [-1, 2304, 256]               0\n",
      "SwinTransformerBlock-76            [-1, 2304, 256]               0\n",
      "        LayerNorm-77            [-1, 576, 1024]           2,048\n",
      "           Linear-78             [-1, 576, 512]         524,288\n",
      "     PatchMerging-79             [-1, 576, 512]               0\n",
      "       BasicLayer-80             [-1, 576, 512]               0\n",
      "        LayerNorm-81             [-1, 576, 512]           1,024\n",
      "           Linear-82            [-1, 144, 1536]         787,968\n",
      "          Softmax-83         [-1, 16, 144, 144]               0\n",
      "          Dropout-84         [-1, 16, 144, 144]               0\n",
      "           Linear-85             [-1, 144, 512]         262,656\n",
      "          Dropout-86             [-1, 144, 512]               0\n",
      "  WindowAttention-87             [-1, 144, 512]               0\n",
      "         DropPath-88             [-1, 576, 512]               0\n",
      "        LayerNorm-89             [-1, 576, 512]           1,024\n",
      "           Linear-90            [-1, 576, 2048]       1,050,624\n",
      "             GELU-91            [-1, 576, 2048]               0\n",
      "          Dropout-92            [-1, 576, 2048]               0\n",
      "           Linear-93             [-1, 576, 512]       1,049,088\n",
      "          Dropout-94             [-1, 576, 512]               0\n",
      "              Mlp-95             [-1, 576, 512]               0\n",
      "         DropPath-96             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-97             [-1, 576, 512]               0\n",
      "        LayerNorm-98             [-1, 576, 512]           1,024\n",
      "           Linear-99            [-1, 144, 1536]         787,968\n",
      "         Softmax-100         [-1, 16, 144, 144]               0\n",
      "         Dropout-101         [-1, 16, 144, 144]               0\n",
      "          Linear-102             [-1, 144, 512]         262,656\n",
      "         Dropout-103             [-1, 144, 512]               0\n",
      " WindowAttention-104             [-1, 144, 512]               0\n",
      "        DropPath-105             [-1, 576, 512]               0\n",
      "       LayerNorm-106             [-1, 576, 512]           1,024\n",
      "          Linear-107            [-1, 576, 2048]       1,050,624\n",
      "            GELU-108            [-1, 576, 2048]               0\n",
      "         Dropout-109            [-1, 576, 2048]               0\n",
      "          Linear-110             [-1, 576, 512]       1,049,088\n",
      "         Dropout-111             [-1, 576, 512]               0\n",
      "             Mlp-112             [-1, 576, 512]               0\n",
      "        DropPath-113             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-114             [-1, 576, 512]               0\n",
      "       LayerNorm-115             [-1, 576, 512]           1,024\n",
      "          Linear-116            [-1, 144, 1536]         787,968\n",
      "         Softmax-117         [-1, 16, 144, 144]               0\n",
      "         Dropout-118         [-1, 16, 144, 144]               0\n",
      "          Linear-119             [-1, 144, 512]         262,656\n",
      "         Dropout-120             [-1, 144, 512]               0\n",
      " WindowAttention-121             [-1, 144, 512]               0\n",
      "        DropPath-122             [-1, 576, 512]               0\n",
      "       LayerNorm-123             [-1, 576, 512]           1,024\n",
      "          Linear-124            [-1, 576, 2048]       1,050,624\n",
      "            GELU-125            [-1, 576, 2048]               0\n",
      "         Dropout-126            [-1, 576, 2048]               0\n",
      "          Linear-127             [-1, 576, 512]       1,049,088\n",
      "         Dropout-128             [-1, 576, 512]               0\n",
      "             Mlp-129             [-1, 576, 512]               0\n",
      "        DropPath-130             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-131             [-1, 576, 512]               0\n",
      "       LayerNorm-132             [-1, 576, 512]           1,024\n",
      "          Linear-133            [-1, 144, 1536]         787,968\n",
      "         Softmax-134         [-1, 16, 144, 144]               0\n",
      "         Dropout-135         [-1, 16, 144, 144]               0\n",
      "          Linear-136             [-1, 144, 512]         262,656\n",
      "         Dropout-137             [-1, 144, 512]               0\n",
      " WindowAttention-138             [-1, 144, 512]               0\n",
      "        DropPath-139             [-1, 576, 512]               0\n",
      "       LayerNorm-140             [-1, 576, 512]           1,024\n",
      "          Linear-141            [-1, 576, 2048]       1,050,624\n",
      "            GELU-142            [-1, 576, 2048]               0\n",
      "         Dropout-143            [-1, 576, 2048]               0\n",
      "          Linear-144             [-1, 576, 512]       1,049,088\n",
      "         Dropout-145             [-1, 576, 512]               0\n",
      "             Mlp-146             [-1, 576, 512]               0\n",
      "        DropPath-147             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-148             [-1, 576, 512]               0\n",
      "       LayerNorm-149             [-1, 576, 512]           1,024\n",
      "          Linear-150            [-1, 144, 1536]         787,968\n",
      "         Softmax-151         [-1, 16, 144, 144]               0\n",
      "         Dropout-152         [-1, 16, 144, 144]               0\n",
      "          Linear-153             [-1, 144, 512]         262,656\n",
      "         Dropout-154             [-1, 144, 512]               0\n",
      " WindowAttention-155             [-1, 144, 512]               0\n",
      "        DropPath-156             [-1, 576, 512]               0\n",
      "       LayerNorm-157             [-1, 576, 512]           1,024\n",
      "          Linear-158            [-1, 576, 2048]       1,050,624\n",
      "            GELU-159            [-1, 576, 2048]               0\n",
      "         Dropout-160            [-1, 576, 2048]               0\n",
      "          Linear-161             [-1, 576, 512]       1,049,088\n",
      "         Dropout-162             [-1, 576, 512]               0\n",
      "             Mlp-163             [-1, 576, 512]               0\n",
      "        DropPath-164             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-165             [-1, 576, 512]               0\n",
      "       LayerNorm-166             [-1, 576, 512]           1,024\n",
      "          Linear-167            [-1, 144, 1536]         787,968\n",
      "         Softmax-168         [-1, 16, 144, 144]               0\n",
      "         Dropout-169         [-1, 16, 144, 144]               0\n",
      "          Linear-170             [-1, 144, 512]         262,656\n",
      "         Dropout-171             [-1, 144, 512]               0\n",
      " WindowAttention-172             [-1, 144, 512]               0\n",
      "        DropPath-173             [-1, 576, 512]               0\n",
      "       LayerNorm-174             [-1, 576, 512]           1,024\n",
      "          Linear-175            [-1, 576, 2048]       1,050,624\n",
      "            GELU-176            [-1, 576, 2048]               0\n",
      "         Dropout-177            [-1, 576, 2048]               0\n",
      "          Linear-178             [-1, 576, 512]       1,049,088\n",
      "         Dropout-179             [-1, 576, 512]               0\n",
      "             Mlp-180             [-1, 576, 512]               0\n",
      "        DropPath-181             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-182             [-1, 576, 512]               0\n",
      "       LayerNorm-183             [-1, 576, 512]           1,024\n",
      "          Linear-184            [-1, 144, 1536]         787,968\n",
      "         Softmax-185         [-1, 16, 144, 144]               0\n",
      "         Dropout-186         [-1, 16, 144, 144]               0\n",
      "          Linear-187             [-1, 144, 512]         262,656\n",
      "         Dropout-188             [-1, 144, 512]               0\n",
      " WindowAttention-189             [-1, 144, 512]               0\n",
      "        DropPath-190             [-1, 576, 512]               0\n",
      "       LayerNorm-191             [-1, 576, 512]           1,024\n",
      "          Linear-192            [-1, 576, 2048]       1,050,624\n",
      "            GELU-193            [-1, 576, 2048]               0\n",
      "         Dropout-194            [-1, 576, 2048]               0\n",
      "          Linear-195             [-1, 576, 512]       1,049,088\n",
      "         Dropout-196             [-1, 576, 512]               0\n",
      "             Mlp-197             [-1, 576, 512]               0\n",
      "        DropPath-198             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-199             [-1, 576, 512]               0\n",
      "       LayerNorm-200             [-1, 576, 512]           1,024\n",
      "          Linear-201            [-1, 144, 1536]         787,968\n",
      "         Softmax-202         [-1, 16, 144, 144]               0\n",
      "         Dropout-203         [-1, 16, 144, 144]               0\n",
      "          Linear-204             [-1, 144, 512]         262,656\n",
      "         Dropout-205             [-1, 144, 512]               0\n",
      " WindowAttention-206             [-1, 144, 512]               0\n",
      "        DropPath-207             [-1, 576, 512]               0\n",
      "       LayerNorm-208             [-1, 576, 512]           1,024\n",
      "          Linear-209            [-1, 576, 2048]       1,050,624\n",
      "            GELU-210            [-1, 576, 2048]               0\n",
      "         Dropout-211            [-1, 576, 2048]               0\n",
      "          Linear-212             [-1, 576, 512]       1,049,088\n",
      "         Dropout-213             [-1, 576, 512]               0\n",
      "             Mlp-214             [-1, 576, 512]               0\n",
      "        DropPath-215             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-216             [-1, 576, 512]               0\n",
      "       LayerNorm-217             [-1, 576, 512]           1,024\n",
      "          Linear-218            [-1, 144, 1536]         787,968\n",
      "         Softmax-219         [-1, 16, 144, 144]               0\n",
      "         Dropout-220         [-1, 16, 144, 144]               0\n",
      "          Linear-221             [-1, 144, 512]         262,656\n",
      "         Dropout-222             [-1, 144, 512]               0\n",
      " WindowAttention-223             [-1, 144, 512]               0\n",
      "        DropPath-224             [-1, 576, 512]               0\n",
      "       LayerNorm-225             [-1, 576, 512]           1,024\n",
      "          Linear-226            [-1, 576, 2048]       1,050,624\n",
      "            GELU-227            [-1, 576, 2048]               0\n",
      "         Dropout-228            [-1, 576, 2048]               0\n",
      "          Linear-229             [-1, 576, 512]       1,049,088\n",
      "         Dropout-230             [-1, 576, 512]               0\n",
      "             Mlp-231             [-1, 576, 512]               0\n",
      "        DropPath-232             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-233             [-1, 576, 512]               0\n",
      "       LayerNorm-234             [-1, 576, 512]           1,024\n",
      "          Linear-235            [-1, 144, 1536]         787,968\n",
      "         Softmax-236         [-1, 16, 144, 144]               0\n",
      "         Dropout-237         [-1, 16, 144, 144]               0\n",
      "          Linear-238             [-1, 144, 512]         262,656\n",
      "         Dropout-239             [-1, 144, 512]               0\n",
      " WindowAttention-240             [-1, 144, 512]               0\n",
      "        DropPath-241             [-1, 576, 512]               0\n",
      "       LayerNorm-242             [-1, 576, 512]           1,024\n",
      "          Linear-243            [-1, 576, 2048]       1,050,624\n",
      "            GELU-244            [-1, 576, 2048]               0\n",
      "         Dropout-245            [-1, 576, 2048]               0\n",
      "          Linear-246             [-1, 576, 512]       1,049,088\n",
      "         Dropout-247             [-1, 576, 512]               0\n",
      "             Mlp-248             [-1, 576, 512]               0\n",
      "        DropPath-249             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-250             [-1, 576, 512]               0\n",
      "       LayerNorm-251             [-1, 576, 512]           1,024\n",
      "          Linear-252            [-1, 144, 1536]         787,968\n",
      "         Softmax-253         [-1, 16, 144, 144]               0\n",
      "         Dropout-254         [-1, 16, 144, 144]               0\n",
      "          Linear-255             [-1, 144, 512]         262,656\n",
      "         Dropout-256             [-1, 144, 512]               0\n",
      " WindowAttention-257             [-1, 144, 512]               0\n",
      "        DropPath-258             [-1, 576, 512]               0\n",
      "       LayerNorm-259             [-1, 576, 512]           1,024\n",
      "          Linear-260            [-1, 576, 2048]       1,050,624\n",
      "            GELU-261            [-1, 576, 2048]               0\n",
      "         Dropout-262            [-1, 576, 2048]               0\n",
      "          Linear-263             [-1, 576, 512]       1,049,088\n",
      "         Dropout-264             [-1, 576, 512]               0\n",
      "             Mlp-265             [-1, 576, 512]               0\n",
      "        DropPath-266             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-267             [-1, 576, 512]               0\n",
      "       LayerNorm-268             [-1, 576, 512]           1,024\n",
      "          Linear-269            [-1, 144, 1536]         787,968\n",
      "         Softmax-270         [-1, 16, 144, 144]               0\n",
      "         Dropout-271         [-1, 16, 144, 144]               0\n",
      "          Linear-272             [-1, 144, 512]         262,656\n",
      "         Dropout-273             [-1, 144, 512]               0\n",
      " WindowAttention-274             [-1, 144, 512]               0\n",
      "        DropPath-275             [-1, 576, 512]               0\n",
      "       LayerNorm-276             [-1, 576, 512]           1,024\n",
      "          Linear-277            [-1, 576, 2048]       1,050,624\n",
      "            GELU-278            [-1, 576, 2048]               0\n",
      "         Dropout-279            [-1, 576, 2048]               0\n",
      "          Linear-280             [-1, 576, 512]       1,049,088\n",
      "         Dropout-281             [-1, 576, 512]               0\n",
      "             Mlp-282             [-1, 576, 512]               0\n",
      "        DropPath-283             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-284             [-1, 576, 512]               0\n",
      "       LayerNorm-285             [-1, 576, 512]           1,024\n",
      "          Linear-286            [-1, 144, 1536]         787,968\n",
      "         Softmax-287         [-1, 16, 144, 144]               0\n",
      "         Dropout-288         [-1, 16, 144, 144]               0\n",
      "          Linear-289             [-1, 144, 512]         262,656\n",
      "         Dropout-290             [-1, 144, 512]               0\n",
      " WindowAttention-291             [-1, 144, 512]               0\n",
      "        DropPath-292             [-1, 576, 512]               0\n",
      "       LayerNorm-293             [-1, 576, 512]           1,024\n",
      "          Linear-294            [-1, 576, 2048]       1,050,624\n",
      "            GELU-295            [-1, 576, 2048]               0\n",
      "         Dropout-296            [-1, 576, 2048]               0\n",
      "          Linear-297             [-1, 576, 512]       1,049,088\n",
      "         Dropout-298             [-1, 576, 512]               0\n",
      "             Mlp-299             [-1, 576, 512]               0\n",
      "        DropPath-300             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-301             [-1, 576, 512]               0\n",
      "       LayerNorm-302             [-1, 576, 512]           1,024\n",
      "          Linear-303            [-1, 144, 1536]         787,968\n",
      "         Softmax-304         [-1, 16, 144, 144]               0\n",
      "         Dropout-305         [-1, 16, 144, 144]               0\n",
      "          Linear-306             [-1, 144, 512]         262,656\n",
      "         Dropout-307             [-1, 144, 512]               0\n",
      " WindowAttention-308             [-1, 144, 512]               0\n",
      "        DropPath-309             [-1, 576, 512]               0\n",
      "       LayerNorm-310             [-1, 576, 512]           1,024\n",
      "          Linear-311            [-1, 576, 2048]       1,050,624\n",
      "            GELU-312            [-1, 576, 2048]               0\n",
      "         Dropout-313            [-1, 576, 2048]               0\n",
      "          Linear-314             [-1, 576, 512]       1,049,088\n",
      "         Dropout-315             [-1, 576, 512]               0\n",
      "             Mlp-316             [-1, 576, 512]               0\n",
      "        DropPath-317             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-318             [-1, 576, 512]               0\n",
      "       LayerNorm-319             [-1, 576, 512]           1,024\n",
      "          Linear-320            [-1, 144, 1536]         787,968\n",
      "         Softmax-321         [-1, 16, 144, 144]               0\n",
      "         Dropout-322         [-1, 16, 144, 144]               0\n",
      "          Linear-323             [-1, 144, 512]         262,656\n",
      "         Dropout-324             [-1, 144, 512]               0\n",
      " WindowAttention-325             [-1, 144, 512]               0\n",
      "        DropPath-326             [-1, 576, 512]               0\n",
      "       LayerNorm-327             [-1, 576, 512]           1,024\n",
      "          Linear-328            [-1, 576, 2048]       1,050,624\n",
      "            GELU-329            [-1, 576, 2048]               0\n",
      "         Dropout-330            [-1, 576, 2048]               0\n",
      "          Linear-331             [-1, 576, 512]       1,049,088\n",
      "         Dropout-332             [-1, 576, 512]               0\n",
      "             Mlp-333             [-1, 576, 512]               0\n",
      "        DropPath-334             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-335             [-1, 576, 512]               0\n",
      "       LayerNorm-336             [-1, 576, 512]           1,024\n",
      "          Linear-337            [-1, 144, 1536]         787,968\n",
      "         Softmax-338         [-1, 16, 144, 144]               0\n",
      "         Dropout-339         [-1, 16, 144, 144]               0\n",
      "          Linear-340             [-1, 144, 512]         262,656\n",
      "         Dropout-341             [-1, 144, 512]               0\n",
      " WindowAttention-342             [-1, 144, 512]               0\n",
      "        DropPath-343             [-1, 576, 512]               0\n",
      "       LayerNorm-344             [-1, 576, 512]           1,024\n",
      "          Linear-345            [-1, 576, 2048]       1,050,624\n",
      "            GELU-346            [-1, 576, 2048]               0\n",
      "         Dropout-347            [-1, 576, 2048]               0\n",
      "          Linear-348             [-1, 576, 512]       1,049,088\n",
      "         Dropout-349             [-1, 576, 512]               0\n",
      "             Mlp-350             [-1, 576, 512]               0\n",
      "        DropPath-351             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-352             [-1, 576, 512]               0\n",
      "       LayerNorm-353             [-1, 576, 512]           1,024\n",
      "          Linear-354            [-1, 144, 1536]         787,968\n",
      "         Softmax-355         [-1, 16, 144, 144]               0\n",
      "         Dropout-356         [-1, 16, 144, 144]               0\n",
      "          Linear-357             [-1, 144, 512]         262,656\n",
      "         Dropout-358             [-1, 144, 512]               0\n",
      " WindowAttention-359             [-1, 144, 512]               0\n",
      "        DropPath-360             [-1, 576, 512]               0\n",
      "       LayerNorm-361             [-1, 576, 512]           1,024\n",
      "          Linear-362            [-1, 576, 2048]       1,050,624\n",
      "            GELU-363            [-1, 576, 2048]               0\n",
      "         Dropout-364            [-1, 576, 2048]               0\n",
      "          Linear-365             [-1, 576, 512]       1,049,088\n",
      "         Dropout-366             [-1, 576, 512]               0\n",
      "             Mlp-367             [-1, 576, 512]               0\n",
      "        DropPath-368             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-369             [-1, 576, 512]               0\n",
      "       LayerNorm-370             [-1, 576, 512]           1,024\n",
      "          Linear-371            [-1, 144, 1536]         787,968\n",
      "         Softmax-372         [-1, 16, 144, 144]               0\n",
      "         Dropout-373         [-1, 16, 144, 144]               0\n",
      "          Linear-374             [-1, 144, 512]         262,656\n",
      "         Dropout-375             [-1, 144, 512]               0\n",
      " WindowAttention-376             [-1, 144, 512]               0\n",
      "        DropPath-377             [-1, 576, 512]               0\n",
      "       LayerNorm-378             [-1, 576, 512]           1,024\n",
      "          Linear-379            [-1, 576, 2048]       1,050,624\n",
      "            GELU-380            [-1, 576, 2048]               0\n",
      "         Dropout-381            [-1, 576, 2048]               0\n",
      "          Linear-382             [-1, 576, 512]       1,049,088\n",
      "         Dropout-383             [-1, 576, 512]               0\n",
      "             Mlp-384             [-1, 576, 512]               0\n",
      "        DropPath-385             [-1, 576, 512]               0\n",
      "SwinTransformerBlock-386             [-1, 576, 512]               0\n",
      "       LayerNorm-387            [-1, 144, 2048]           4,096\n",
      "          Linear-388            [-1, 144, 1024]       2,097,152\n",
      "    PatchMerging-389            [-1, 144, 1024]               0\n",
      "      BasicLayer-390            [-1, 144, 1024]               0\n",
      "       LayerNorm-391            [-1, 144, 1024]           2,048\n",
      "          Linear-392            [-1, 144, 3072]       3,148,800\n",
      "         Softmax-393         [-1, 32, 144, 144]               0\n",
      "         Dropout-394         [-1, 32, 144, 144]               0\n",
      "          Linear-395            [-1, 144, 1024]       1,049,600\n",
      "         Dropout-396            [-1, 144, 1024]               0\n",
      " WindowAttention-397            [-1, 144, 1024]               0\n",
      "        DropPath-398            [-1, 144, 1024]               0\n",
      "       LayerNorm-399            [-1, 144, 1024]           2,048\n",
      "          Linear-400            [-1, 144, 4096]       4,198,400\n",
      "            GELU-401            [-1, 144, 4096]               0\n",
      "         Dropout-402            [-1, 144, 4096]               0\n",
      "          Linear-403            [-1, 144, 1024]       4,195,328\n",
      "         Dropout-404            [-1, 144, 1024]               0\n",
      "             Mlp-405            [-1, 144, 1024]               0\n",
      "        DropPath-406            [-1, 144, 1024]               0\n",
      "SwinTransformerBlock-407            [-1, 144, 1024]               0\n",
      "       LayerNorm-408            [-1, 144, 1024]           2,048\n",
      "          Linear-409            [-1, 144, 3072]       3,148,800\n",
      "         Softmax-410         [-1, 32, 144, 144]               0\n",
      "         Dropout-411         [-1, 32, 144, 144]               0\n",
      "          Linear-412            [-1, 144, 1024]       1,049,600\n",
      "         Dropout-413            [-1, 144, 1024]               0\n",
      " WindowAttention-414            [-1, 144, 1024]               0\n",
      "        DropPath-415            [-1, 144, 1024]               0\n",
      "       LayerNorm-416            [-1, 144, 1024]           2,048\n",
      "          Linear-417            [-1, 144, 4096]       4,198,400\n",
      "            GELU-418            [-1, 144, 4096]               0\n",
      "         Dropout-419            [-1, 144, 4096]               0\n",
      "          Linear-420            [-1, 144, 1024]       4,195,328\n",
      "         Dropout-421            [-1, 144, 1024]               0\n",
      "             Mlp-422            [-1, 144, 1024]               0\n",
      "        DropPath-423            [-1, 144, 1024]               0\n",
      "SwinTransformerBlock-424            [-1, 144, 1024]               0\n",
      "      BasicLayer-425            [-1, 144, 1024]               0\n",
      "       LayerNorm-426            [-1, 144, 1024]           2,048\n",
      "AdaptiveAvgPool1d-427              [-1, 1024, 1]               0\n",
      "        Identity-428                 [-1, 1024]               0\n",
      "================================================================\n",
      "Total params: 86,679,680\n",
      "Trainable params: 86,679,680\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.69\n",
      "Forward/backward pass size (MB): 1670.92\n",
      "Params size (MB): 330.66\n",
      "Estimated Total Size (MB): 2003.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model.reset_classifier(0)\n",
    "summary(model, (3,384,384))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9aea7b6a7444b8f7906e49a0ee5a1a1954c47b5d0459b9029b34272caa5266f3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
