{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 특징\n","\n","- swin_base_patch4_window12_384\n","- Scheduler : CosineAnnealingLR\n","- normal train/test split\n","- early stopping"]},{"cell_type":"markdown","metadata":{},"source":[" # What I can do more\n","\n"," - NO K-Fold\n"," - [Done] Backbone Freezing\n"," - EarlyStopping\n"," - More model Ensemble\n"," - Feature Engineering"]},{"cell_type":"markdown","metadata":{},"source":["# Install Required Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:26:44.561738Z","iopub.status.busy":"2021-12-12T21:26:44.561469Z","iopub.status.idle":"2021-12-12T21:26:53.882776Z","shell.execute_reply":"2021-12-12T21:26:53.881919Z","shell.execute_reply.started":"2021-12-12T21:26:44.561708Z"},"trusted":true},"outputs":[],"source":["# ! pip install git+https://github.com/rwightman/pytorch-image-models\n","! pip install -q -U wandb albumentations timm"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:26:53.886205Z","iopub.status.busy":"2021-12-12T21:26:53.885917Z","iopub.status.idle":"2021-12-12T21:26:56.194811Z","shell.execute_reply":"2021-12-12T21:26:56.193915Z","shell.execute_reply.started":"2021-12-12T21:26:53.886150Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import cv2\n","import copy\n","import time\n","import random\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# For data manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# Pytorch Imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda import amp\n","\n","# Utils\n","import joblib\n","from tqdm import tqdm\n","from collections import defaultdict\n","\n","# Sklearn Imports\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import StratifiedKFold, KFold\n","\n","# Pytorch Image Model Library\n","import timm\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","import os\n","\n","# For descriptive error messages\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# CONFIG"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:26:56.197108Z","iopub.status.busy":"2021-12-12T21:26:56.196209Z","iopub.status.idle":"2021-12-12T21:26:56.200767Z","shell.execute_reply":"2021-12-12T21:26:56.200108Z","shell.execute_reply.started":"2021-12-12T21:26:56.197076Z"},"trusted":true},"outputs":[],"source":["ROOT_DIR = \"../input/petfinder-pawpularity-score\"\n","MODEL_DIR = \"../input/pretrained-model\"\n","TRAIN_DIR = f\"{ROOT_DIR}/train\"\n","TEST_DIR = f\"{ROOT_DIR}/test\"\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:26:56.202495Z","iopub.status.busy":"2021-12-12T21:26:56.202043Z","iopub.status.idle":"2021-12-12T21:26:56.260211Z","shell.execute_reply":"2021-12-12T21:26:56.259262Z","shell.execute_reply.started":"2021-12-12T21:26:56.202456Z"},"trusted":true},"outputs":[],"source":["CONFIG = dict(\n","    seed=42,\n","    backbone=\"swin_base_patch4_window12_384\",\n","    train_batch_size=16,\n","    valid_batch_size=32,\n","    img_size=384,\n","    fine_tune_epochs=1,\n","    epochs=100,\n","    learning_rate=1e-4,\n","    scheduler=\"CosineAnnealingLR\",\n","    min_lr=1e-6,\n","    T_max=100,\n","    weight_decay=1e-6,\n","    n_accumulate=1,\n","    use_KFold=False,\n","    n_fold=5,\n","    num_classes=1,\n","    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n","    competition=\"PetFinder\",\n","    _wandb_kernel_=\"deb\",\n","    early_stopping=True,\n","    early_stopping_step=10,\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["## wandb config"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:26:56.265283Z","iopub.status.busy":"2021-12-12T21:26:56.265034Z","iopub.status.idle":"2021-12-12T21:27:03.418100Z","shell.execute_reply":"2021-12-12T21:27:03.417360Z","shell.execute_reply.started":"2021-12-12T21:26:56.265247Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjiwon7258\u001b[0m (use `wandb login --relogin` to force relogin)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jiwon/.netrc\n","2021-12-13 12:57:38.479679: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jiwon/anaconda3/lib/python3.9/site-packages/cv2/../../lib64:\n","2021-12-13 12:57:38.480969: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/jiwon7258/petfinder-pawpularity-score/runs/1zt0iq67\" target=\"_blank\">winter-pine-49</a></strong> to <a href=\"https://wandb.ai/jiwon7258/petfinder-pawpularity-score\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import wandb\n","\n","os.environ[\"WANDB_API_KEY\"] = \"d60a4af56f6cd9cccec7d9da1dbced7960b61310\"\n","# if want to run in offline mode\n","# os.environ[\"WANDB_MODE\"] = \"dryrun\"\n","wandb.login(key=\"d60a4af56f6cd9cccec7d9da1dbced7960b61310\")\n","wandb.init(project=\"petfinder-pawpularity-score\", entity=\"jiwon7258\")\n","wandb.run.name = \"swin384_1fold_finetuning-enabled\"\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Set Seed for Reproducibility"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.424641Z","iopub.status.busy":"2021-12-12T21:27:03.422247Z","iopub.status.idle":"2021-12-12T21:27:03.435870Z","shell.execute_reply":"2021-12-12T21:27:03.435093Z","shell.execute_reply.started":"2021-12-12T21:27:03.424601Z"},"trusted":true},"outputs":[],"source":["def set_seed(seed=42):\n","    # numpy\n","    np.random.seed(seed)\n","    # python\n","    random.seed(seed)\n","    # torch\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","\n","\n","set_seed(CONFIG[\"seed\"])\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Read the Data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.437516Z","iopub.status.busy":"2021-12-12T21:27:03.437279Z","iopub.status.idle":"2021-12-12T21:27:03.445960Z","shell.execute_reply":"2021-12-12T21:27:03.445335Z","shell.execute_reply.started":"2021-12-12T21:27:03.437483Z"},"trusted":true},"outputs":[],"source":["def get_train_file_path(id):\n","    return f\"{TRAIN_DIR}/{id}.jpg\"\n","\n","\n","def get_test_file_path(id):\n","    return f\"{TEST_DIR}/{id}.jpg\"\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.449369Z","iopub.status.busy":"2021-12-12T21:27:03.448725Z","iopub.status.idle":"2021-12-12T21:27:03.495670Z","shell.execute_reply":"2021-12-12T21:27:03.495001Z","shell.execute_reply.started":"2021-12-12T21:27:03.449329Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\n","# file_path에 해당하는 column을 만든다\n","df[\"file_path\"] = df[\"Id\"].apply(get_train_file_path)\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.497107Z","iopub.status.busy":"2021-12-12T21:27:03.496889Z","iopub.status.idle":"2021-12-12T21:27:03.516134Z","shell.execute_reply":"2021-12-12T21:27:03.515516Z","shell.execute_reply.started":"2021-12-12T21:27:03.497075Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Subject Focus</th>\n","      <th>Eyes</th>\n","      <th>Face</th>\n","      <th>Near</th>\n","      <th>Action</th>\n","      <th>Accessory</th>\n","      <th>Group</th>\n","      <th>Collage</th>\n","      <th>Human</th>\n","      <th>Occlusion</th>\n","      <th>Info</th>\n","      <th>Blur</th>\n","      <th>Pawpularity</th>\n","      <th>file_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>63</td>\n","      <td>../input/petfinder-pawpularity-score/train/000...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0009c66b9439883ba2750fb825e1d7db</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>42</td>\n","      <td>../input/petfinder-pawpularity-score/train/000...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>28</td>\n","      <td>../input/petfinder-pawpularity-score/train/001...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>15</td>\n","      <td>../input/petfinder-pawpularity-score/train/001...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>001dc955e10590d3ca4673f034feeef2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>72</td>\n","      <td>../input/petfinder-pawpularity-score/train/001...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n","0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n","1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n","2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n","3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n","4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n","\n","   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \\\n","0          0      1        0      0          0     0     0           63   \n","1          0      0        0      0          0     0     0           42   \n","2          0      0        0      1          1     0     0           28   \n","3          0      0        0      0          0     0     0           15   \n","4          0      1        0      0          0     0     0           72   \n","\n","                                           file_path  \n","0  ../input/petfinder-pawpularity-score/train/000...  \n","1  ../input/petfinder-pawpularity-score/train/000...  \n","2  ../input/petfinder-pawpularity-score/train/001...  \n","3  ../input/petfinder-pawpularity-score/train/001...  \n","4  ../input/petfinder-pawpularity-score/train/001...  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.head()\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.518099Z","iopub.status.busy":"2021-12-12T21:27:03.517271Z","iopub.status.idle":"2021-12-12T21:27:03.523131Z","shell.execute_reply":"2021-12-12T21:27:03.522335Z","shell.execute_reply.started":"2021-12-12T21:27:03.518050Z"},"trusted":true},"outputs":[],"source":["# feature_cols를 통해 사용할 feature 목록을 관리한다\n","feature_cols = [\n","    col for col in df.columns if col not in [\"Id\", \"Pawpularity\", \"file_path\"]\n","]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Create Folds"]},{"cell_type":"markdown","metadata":{},"source":["Pawpularity는 0~100 사이의 정수 값을 가진다. Stratifed"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.526482Z","iopub.status.busy":"2021-12-12T21:27:03.526098Z","iopub.status.idle":"2021-12-12T21:27:03.534443Z","shell.execute_reply":"2021-12-12T21:27:03.533497Z","shell.execute_reply.started":"2021-12-12T21:27:03.526453Z"},"trusted":true},"outputs":[],"source":["def create_folds(df, n_s=5, n_grp=None):\n","    df[\"kfold\"] = -1\n","\n","    if n_grp is None:\n","        skf = KFold(n_splits=n_s, random_state=CONFIG[\"seed\"])\n","        target = df[\"Pawpularity\"]\n","    else:\n","        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG[\"seed\"])\n","        # Pawpularity를 구간별로, n_grp 수만큼 자른다\n","        # 따라서 Pawpularity와 grp의 히스토그램 분포는 동일하다\n","        df[\"grp\"] = pd.cut(df[\"Pawpularity\"], n_grp, labels=False)\n","        target = df[\"grp\"]\n","\n","    # n_grp의 분포를 기반으로 StratifiedKFold를 진행한다\n","    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n","        df.loc[v, \"kfold\"] = fold_no\n","\n","    df = df.drop(\"grp\", axis=1)\n","    return df\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.536703Z","iopub.status.busy":"2021-12-12T21:27:03.536119Z","iopub.status.idle":"2021-12-12T21:27:03.576667Z","shell.execute_reply":"2021-12-12T21:27:03.576029Z","shell.execute_reply.started":"2021-12-12T21:27:03.536656Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Subject Focus</th>\n","      <th>Eyes</th>\n","      <th>Face</th>\n","      <th>Near</th>\n","      <th>Action</th>\n","      <th>Accessory</th>\n","      <th>Group</th>\n","      <th>Collage</th>\n","      <th>Human</th>\n","      <th>Occlusion</th>\n","      <th>Info</th>\n","      <th>Blur</th>\n","      <th>Pawpularity</th>\n","      <th>file_path</th>\n","      <th>kfold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>63</td>\n","      <td>../input/petfinder-pawpularity-score/train/000...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0009c66b9439883ba2750fb825e1d7db</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>42</td>\n","      <td>../input/petfinder-pawpularity-score/train/000...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>28</td>\n","      <td>../input/petfinder-pawpularity-score/train/001...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>15</td>\n","      <td>../input/petfinder-pawpularity-score/train/001...</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>001dc955e10590d3ca4673f034feeef2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>72</td>\n","      <td>../input/petfinder-pawpularity-score/train/001...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n","0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n","1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n","2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n","3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n","4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n","\n","   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \\\n","0          0      1        0      0          0     0     0           63   \n","1          0      0        0      0          0     0     0           42   \n","2          0      0        0      1          1     0     0           28   \n","3          0      0        0      0          0     0     0           15   \n","4          0      1        0      0          0     0     0           72   \n","\n","                                           file_path  kfold  \n","0  ../input/petfinder-pawpularity-score/train/000...      0  \n","1  ../input/petfinder-pawpularity-score/train/000...      2  \n","2  ../input/petfinder-pawpularity-score/train/001...      0  \n","3  ../input/petfinder-pawpularity-score/train/001...      3  \n","4  ../input/petfinder-pawpularity-score/train/001...      4  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df = create_folds(df, n_s=CONFIG[\"n_fold\"], n_grp=14)\n","df.head()\n"]},{"cell_type":"markdown","metadata":{},"source":["Pawpularity는 0~100까지의 정수로 이루어져 있다. 이 분포 그대로 KFold를 진행하지 않는다. 대신 n_grp개 만큼, 일정한 길이별로 구간을 나눈 후, 이 분포를 이용하여 KFold를 진행한다.\n","\n","```df.Pawpularity.hist(bins=14) == df.grp.hist(bins=14)```\n","\n","![Pawpularity Histogram](./img/paupularity.png)\n","\n","![이미지](./img/pawpularity_and_grp_hist.png)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Class "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.578263Z","iopub.status.busy":"2021-12-12T21:27:03.578000Z","iopub.status.idle":"2021-12-12T21:27:03.587850Z","shell.execute_reply":"2021-12-12T21:27:03.587015Z","shell.execute_reply.started":"2021-12-12T21:27:03.578229Z"},"trusted":true},"outputs":[],"source":["class PawpularityDataset(Dataset):\n","    def __init__(self, root_dir, df, transforms=None):\n","        self.root_dir = root_dir\n","        self.df = df\n","        self.file_names = df[\"file_path\"].values  # numpy array\n","        self.targets = df[\"Pawpularity\"].values  # numpy array\n","        self.transforms = transforms\n","\n","    # 데이터 프레임의 길이를 반환\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        img_path = self.file_names[index]\n","        img = cv2.imread(img_path)  # numpy array\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        target = self.targets[index]\n","\n","        if self.transforms:\n","            img = self.transforms(image=img)[\"image\"]\n","\n","        # 이미지 데이터, target label\n","        return img, target\n"]},{"cell_type":"markdown","metadata":{},"source":["# Augmentations"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:03.592892Z","iopub.status.busy":"2021-12-12T21:27:03.592122Z","iopub.status.idle":"2021-12-12T21:27:03.599372Z","shell.execute_reply":"2021-12-12T21:27:03.598572Z","shell.execute_reply.started":"2021-12-12T21:27:03.592855Z"},"trusted":true},"outputs":[],"source":["data_transforms = {\n","    \"train\": A.Compose(\n","        [\n","            # 리사이징\n","            A.Resize(CONFIG[\"img_size\"], CONFIG[\"img_size\"]),\n","            # 가로 반전\n","            A.HorizontalFlip(p=0.5),\n","            # 랜덤 밝기/대조\n","            A.RandomBrightnessContrast(p=0.3),\n","            # 정규화\n","            A.Normalize(),\n","            ToTensorV2(),\n","        ],\n","        p=1.0,\n","    ),\n","    \"valid\": A.Compose(\n","        [A.Resize(CONFIG[\"img_size\"], CONFIG[\"img_size\"]), A.Normalize(), ToTensorV2()]\n","    ),\n","}\n"]},{"cell_type":"markdown","metadata":{},"source":["# Create Model"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["PawpularityModel(\n","  (backbone): SwinTransformer(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n","      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (pos_drop): Dropout(p=0.0, inplace=False)\n","    (layers): Sequential(\n","      (0): BasicLayer(\n","        dim=128, input_resolution=(96, 96), depth=2\n","        (blocks): ModuleList(\n","          (0): SwinTransformerBlock(\n","            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=128, out_features=384, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=128, out_features=128, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): Identity()\n","            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=128, out_features=512, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=512, out_features=128, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (1): SwinTransformerBlock(\n","            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=128, out_features=384, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=128, out_features=128, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=128, out_features=512, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=512, out_features=128, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","        (downsample): PatchMerging(\n","          input_resolution=(96, 96), dim=128\n","          (reduction): Linear(in_features=512, out_features=256, bias=False)\n","          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (1): BasicLayer(\n","        dim=256, input_resolution=(48, 48), depth=2\n","        (blocks): ModuleList(\n","          (0): SwinTransformerBlock(\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=256, out_features=768, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=256, out_features=256, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (1): SwinTransformerBlock(\n","            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=256, out_features=768, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=256, out_features=256, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","        (downsample): PatchMerging(\n","          input_resolution=(48, 48), dim=256\n","          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (2): BasicLayer(\n","        dim=512, input_resolution=(24, 24), depth=18\n","        (blocks): ModuleList(\n","          (0): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (1): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (2): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (3): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (4): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (5): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (6): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (7): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (8): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (9): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (10): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (11): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (12): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (13): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (14): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (15): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (16): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (17): SwinTransformerBlock(\n","            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=512, out_features=512, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","        (downsample): PatchMerging(\n","          input_resolution=(24, 24), dim=512\n","          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n","          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (3): BasicLayer(\n","        dim=1024, input_resolution=(12, 12), depth=2\n","        (blocks): ModuleList(\n","          (0): SwinTransformerBlock(\n","            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","          (1): SwinTransformerBlock(\n","            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (attn): WindowAttention(\n","              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","              (softmax): Softmax(dim=-1)\n","            )\n","            (drop_path): DropPath()\n","            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","              (act): GELU()\n","              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","              (drop): Dropout(p=0.0, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","    )\n","    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (avgpool): AdaptiveAvgPool1d(output_size=1)\n","    (head): Identity()\n","  )\n","  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["class PawpularityModel(nn.Module):\n","    def __init__(self, backbone, pretrained=True):\n","        super().__init__()\n","        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n","        self.backbone.reset_classifier(0)\n","        o = self.backbone(torch.randn(1, 3, CONFIG[\"img_size\"], CONFIG[\"img_size\"]))\n","        self.n_features = o.shape[-1]\n","        # freeze backbone layer\n","        for (name, param) in self.backbone.named_parameters():\n","            if \"layers.3.blocks\" not in name:\n","                param.requires_grad = False\n","        self.backbone.norm.requires_grad_()\n","        self.fc = nn.Linear(self.n_features, CONFIG[\"num_classes\"])\n","\n","    def forward(self, images):\n","        features = self.backbone(images)  # features = (batch size, embedding_size)\n","        output = self.fc(features)  # outputs = (batch_size, num_classes)\n","        return output\n","\n","\n","model = PawpularityModel(CONFIG[\"backbone\"])\n","# model.load_state_dict(torch.load('RMSE4.8957_epoch27.bin'))\n","model.to(CONFIG[\"device\"])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Loss Function"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:17.013341Z","iopub.status.busy":"2021-12-12T21:27:17.012965Z","iopub.status.idle":"2021-12-12T21:27:17.018969Z","shell.execute_reply":"2021-12-12T21:27:17.018313Z","shell.execute_reply.started":"2021-12-12T21:27:17.013305Z"},"trusted":true},"outputs":[],"source":["def criterion(outputs: torch.tensor, targets: torch.tensor):\n","    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training Function"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:17.020923Z","iopub.status.busy":"2021-12-12T21:27:17.020395Z","iopub.status.idle":"2021-12-12T21:27:17.033171Z","shell.execute_reply":"2021-12-12T21:27:17.032450Z","shell.execute_reply.started":"2021-12-12T21:27:17.020887Z"},"trusted":true},"outputs":[],"source":["def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n","    # train 모드로 변경\n","    model.train()\n","\n","    # for the Mixed Precision\n","    # Pytorch 예제 : https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples\n","    scaler = amp.GradScaler()\n","\n","    dataset_size = 0\n","    running_loss = 0\n","\n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","\n","    for step, (images, targets) in bar:\n","        images = images.to(device, dtype=torch.float)\n","        targets = targets.to(device, dtype=torch.float)\n","\n","        batch_size = images.size(0)\n","\n","        with amp.autocast(enabled=True):\n","            outputs = model(images)\n","            loss = criterion(outputs, targets)\n","            loss = loss / CONFIG[\"n_accumulate\"]\n","\n","        # loss를 Scale\n","        # Scaled Grdients를 계산(call)하기 위해 scaled loss를 backward()\n","        scaler.scale(loss).backward()\n","\n","        if (step + 1) % CONFIG[\"n_accumulate\"] == 0:\n","            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n","            # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n","            # otherwise, optimizer.step() is skipped.\n","            scaler.step(optimizer)\n","            # Updates the scale for next iteration.\n","            scaler.update()\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # change learning rate by Scheduler\n","            if scheduler is not None:\n","                scheduler.step()\n","\n","        # loss.item()은 loss를 Python Float으로 반환\n","        # loss.item()은 batch data의 average loss이므로, sum of loss를 구하기 위해 batch_size를 곱해준다\n","        running_loss += loss.item() * batch_size\n","        dataset_size += batch_size\n","\n","        epoch_loss = running_loss / dataset_size\n","\n","        bar.set_postfix(\n","            Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0][\"lr\"]\n","        )\n","\n","    # Garbage Collector\n","    gc.collect()\n","\n","    return epoch_loss\n"]},{"cell_type":"markdown","metadata":{},"source":["# Validation Function"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:17.035135Z","iopub.status.busy":"2021-12-12T21:27:17.034349Z","iopub.status.idle":"2021-12-12T21:27:17.047519Z","shell.execute_reply":"2021-12-12T21:27:17.046853Z","shell.execute_reply.started":"2021-12-12T21:27:17.035100Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def valid_one_epoch(model, dataloader, device, epoch):\n","    model.eval()\n","\n","    dataset_size = 0\n","    running_loss = 0\n","\n","    TARGETS = []\n","    PREDS = []\n","\n","    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n","\n","    for step, (images, targets) in bar:\n","        images = images.to(device, dtype=torch.float)\n","        targets = targets.to(device, dtype=torch.float)\n","\n","        batch_size = images.size(0)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, targets)\n","\n","        running_loss += loss.item() * batch_size\n","        dataset_size += batch_size\n","\n","        # 실시간으로 정보를 표시하기 위한 epoch loss\n","        epoch_loss = running_loss / dataset_size\n","\n","        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n","        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n","\n","        bar.set_postfix(\n","            Epoch=epoch, Valid_Loss=epoch_loss, LR=optimizer.param_groups[0][\"lr\"]\n","        )\n","\n","    TARGETS = np.concatenate(TARGETS)\n","    PREDS = np.concatenate(PREDS)\n","    # 실제 epoch loss는 다음과 같이 계산한다\n","    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n","\n","    gc.collect()\n","\n","    return epoch_loss, val_rmse\n"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare DataLoader and Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:17.067136Z","iopub.status.busy":"2021-12-12T21:27:17.066391Z","iopub.status.idle":"2021-12-12T21:27:17.078230Z","shell.execute_reply":"2021-12-12T21:27:17.077528Z","shell.execute_reply.started":"2021-12-12T21:27:17.067098Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","fold : 여러개의 fold 중, validation set으로 이용할 fold #\n","fold 이외의 folds는 train set으로 이용된다\n","\"\"\"\n","def prepare_loaders(fold):\n","    df_train = df[df.kfold != fold].reset_index(drop=True)\n","    df_valid = df[df.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = PawpularityDataset(\n","        TRAIN_DIR, df_train, transforms=data_transforms[\"train\"]\n","    )\n","    valid_dataset = PawpularityDataset(\n","        TRAIN_DIR, df_valid, transforms=data_transforms[\"valid\"]\n","    )\n","\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=CONFIG[\"train_batch_size\"],\n","        num_workers=0,\n","        shuffle=True,\n","        pin_memory=True,\n","        drop_last=True,\n","    )\n","    valid_loader = DataLoader(\n","        valid_dataset,\n","        batch_size=CONFIG[\"valid_batch_size\"],\n","        num_workers=0,\n","        shuffle=False,\n","        pin_memory=True,\n","    )\n","\n","    return train_loader, valid_loader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:17.083412Z","iopub.status.busy":"2021-12-12T21:27:17.079572Z","iopub.status.idle":"2021-12-12T21:27:17.094612Z","shell.execute_reply":"2021-12-12T21:27:17.093829Z","shell.execute_reply.started":"2021-12-12T21:27:17.083376Z"},"trusted":true},"outputs":[],"source":["def fetch_scheduler(optimizer):\n","    if CONFIG[\"scheduler\"] == \"CosineAnnealingLR\":\n","        scheduler = lr_scheduler.CosineAnnealingLR(\n","            optimizer, T_max=CONFIG[\"T_max\"], eta_min=CONFIG[\"min_lr\"]\n","        )\n","    elif CONFIG[\"scheduler\"] == \"CosineAnnealingWarmRestarts\":\n","        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n","            optimizer, T_0=CONFIG[\"T_0\"], eta_min=CONFIG[\"min_lr\"]\n","        )\n","    elif CONFIG[\"scheduler\"] == None:\n","        return None\n","\n","    return scheduler\n"]},{"cell_type":"markdown","metadata":{},"source":["## Create Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:17.097730Z","iopub.status.busy":"2021-12-12T21:27:17.097421Z","iopub.status.idle":"2021-12-12T21:27:17.128455Z","shell.execute_reply":"2021-12-12T21:27:17.126922Z","shell.execute_reply.started":"2021-12-12T21:27:17.097703Z"},"trusted":true},"outputs":[],"source":["train_loader = dict()\n","valid_loader = dict()\n","\n","for f in range(CONFIG[\"n_fold\"]):\n","    train_, valid_ = prepare_loaders(f)\n","    train_loader[f] = train_\n","    valid_loader[f] = valid_"]},{"cell_type":"markdown","metadata":{},"source":["## Define Optimizer and Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:27:17.129689Z","iopub.status.busy":"2021-12-12T21:27:17.129452Z","iopub.status.idle":"2021-12-12T21:27:17.143864Z","shell.execute_reply":"2021-12-12T21:27:17.142915Z","shell.execute_reply.started":"2021-12-12T21:27:17.129657Z"},"trusted":true},"outputs":[],"source":["optimizer = optim.Adam(\n","    model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"]\n",")\n","scheduler = fetch_scheduler(optimizer)"]},{"cell_type":"markdown","metadata":{},"source":["# Control Training Func"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_training(\n","    model,\n","    optimizer,\n","    scheduler,\n","    device,\n","    num_epochs,\n","    use_KFold: bool,\n","    use_finetune: bool,\n","    early_stopping=True,\n","    early_stopping_step=10,\n","):\n","    # To automatically log graidents\n","    wandb.watch(model, log_freq=100)\n","\n","    if torch.cuda.is_available():\n","        print(\"[INFO] Using GPU:{}\\n\".format(torch.cuda.get_device_name()))\n","\n","    start = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_epoch_rmse = np.inf\n","    history = defaultdict(list)\n","    early_stop_counter = 0\n","\n","    # K-fold Validation : epoch마다 fold가 달라진다\n","    # num_epochs만큼, train과 val을 실행한다\n","    for epoch in range(1, num_epochs + 1):\n","        gc.collect()\n","\n","        # use if K-Fold validation : fold == valdiation fold #\n","        if use_KFold:\n","            fold = epoch % CONFIG[\"n_fold\"]\n","        else:\n","            fold = 0\n","\n","        # train & valid one epoch\n","        print(f\"Validation Fold : {fold}\")\n","\n","        train_epoch_loss = train_one_epoch(\n","            model,\n","            optimizer,\n","            scheduler,\n","            dataloader=train_loader[fold],\n","            device=device,\n","            epoch=epoch,\n","        )\n","        \n","        val_epoch_loss, val_epoch_rmse = valid_one_epoch(\n","            model, valid_loader[fold], device=device, epoch=epoch\n","        )\n","\n","        finetune = \"\"\n","        if use_finetune:\n","            finetune = \"finetune_\"\n","\n","        history[f\"{finetune}Train Loss\"].append(train_epoch_loss)\n","        history[f\"{finetune}Valid Loss\"].append(val_epoch_loss)\n","        history[f\"{finetune}Valid RMSE\"].append(val_epoch_rmse)\n","\n","        # Log the metrics\n","        wandb.log({f\"{finetune}Train Loss\": train_epoch_loss})\n","        wandb.log({f\"{finetune}Valid Loss\": val_epoch_loss})\n","        wandb.log({f\"{finetune}Valid RMSE\": val_epoch_rmse})\n","\n","        print(f\"Valid RMSE : {val_epoch_rmse}\")\n","\n","        # deep copy the model\n","        if val_epoch_rmse <= best_epoch_rmse:\n","            early_stop_counter = 0\n","            print(\n","                f\"Validation Loss improved( {best_epoch_rmse} ---> {val_epoch_rmse}  )\"\n","            )\n","            best_epoch_rmse = val_epoch_rmse\n","            # run.summary['Best RMSE'] = best_epoch_rmse\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            PATH = \"{}RMSE{:.4f}_epoch{:.0f}.bin\".format(\n","                finetune, best_epoch_rmse, epoch\n","            )\n","            torch.save(model.state_dict(), PATH)\n","            torch.save(model.state_dict(), f\"{finetune}best_wts.bin\")\n","            # Save a model file from the current directory\n","            wandb.save(PATH)\n","            print(f\"Model Saved\")\n","        \n","        elif early_stopping:\n","            early_stop_counter += 1\n","            if early_stop_counter > early_stopping_step:\n","                break\n","\n","        print()\n","\n","    end = time.time()\n","    time_elapsed = end - start\n","    print(\n","        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n","            time_elapsed // 3600,\n","            (time_elapsed % 3600) // 60,\n","            (time_elapsed % 3600) % 60,\n","        )\n","    )\n","    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    return model, history\n"]},{"cell_type":"markdown","metadata":{},"source":["# Fine Tuning\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:29:18.499787Z","iopub.status.busy":"2021-12-12T21:29:18.499218Z","iopub.status.idle":"2021-12-12T21:42:19.192516Z","shell.execute_reply":"2021-12-12T21:42:19.191616Z","shell.execute_reply.started":"2021-12-12T21:29:18.499751Z"},"trusted":true},"outputs":[],"source":["model, history = run_training(\n","    model,\n","    optimizer,\n","    scheduler,\n","    device=CONFIG[\"device\"],\n","    num_epochs=CONFIG[\"fine_tune_epochs\"],\n","    use_KFold=CONFIG[\"use_KFold\"],\n","    use_finetune=False,\n","    early_stopping=CONFIG['early_stopping'],\n","    early_stopping_step=CONFIG['early_stopping_step'],\n","\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Whole Tuning (Training)"]},{"cell_type":"markdown","metadata":{},"source":["## Load FineTuned Weight"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:42:19.493097Z","iopub.status.busy":"2021-12-12T21:42:19.492618Z","iopub.status.idle":"2021-12-12T21:42:19.792791Z","shell.execute_reply":"2021-12-12T21:42:19.792129Z","shell.execute_reply.started":"2021-12-12T21:42:19.493059Z"},"trusted":true},"outputs":[],"source":["MODEL_PATH = \"best_wts.bin\"\n","model.load_state_dict(torch.load(MODEL_PATH, map_location=CONFIG[\"device\"]))"]},{"cell_type":"markdown","metadata":{},"source":["## Unfreeze All layer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:43:34.939732Z","iopub.status.busy":"2021-12-12T21:43:34.938837Z","iopub.status.idle":"2021-12-12T21:43:34.952520Z","shell.execute_reply":"2021-12-12T21:43:34.947931Z","shell.execute_reply.started":"2021-12-12T21:43:34.939669Z"},"trusted":true},"outputs":[],"source":["for param in model.parameters():\n","    param.requires_grad = True\n"]},{"cell_type":"markdown","metadata":{},"source":["## Start Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T21:44:26.541511Z","iopub.status.busy":"2021-12-12T21:44:26.541089Z"},"trusted":true},"outputs":[],"source":["model, history = run_training(\n","    model,\n","    optimizer,\n","    scheduler,\n","    device=CONFIG[\"device\"],\n","    num_epochs=CONFIG[\"epochs\"],\n","    use_Kfold=CONFIG[\"use_KFold\"],\n","    use_finetune=False,\n","    early_stopping=CONFIG['early_stopping'],\n","    early_stopping_step=CONFIG['early_stopping_step'],\n",")\n"]},{"cell_type":"markdown","metadata":{},"source":["# TEST and SUBMIT"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:09.957635Z","iopub.status.busy":"2021-12-12T07:14:09.956906Z","iopub.status.idle":"2021-12-12T07:14:10.244447Z","shell.execute_reply":"2021-12-12T07:14:10.2435Z","shell.execute_reply.started":"2021-12-12T07:14:09.957588Z"},"trusted":true},"outputs":[],"source":["TEST_MODEL_PATH = f\"{MODEL_DIR}/RMSE4.8957_epoch27.bin\"\n","model.load_state_dict(torch.load(TEST_MODEL_PATH, map_location=CONFIG[\"device\"]))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.246527Z","iopub.status.busy":"2021-12-12T07:14:10.246202Z","iopub.status.idle":"2021-12-12T07:14:10.256256Z","shell.execute_reply":"2021-12-12T07:14:10.255282Z","shell.execute_reply.started":"2021-12-12T07:14:10.246482Z"},"trusted":true},"outputs":[],"source":["class PawpularityTestDataset(Dataset):\n","    def __init__(self, root_dir, df, transforms=None):\n","        self.root_dir = root_dir\n","        self.df = df\n","        self.file_names = df[\"file_path\"].values  # numpy array\n","        self.transforms = transforms\n","\n","    # 데이터 프레임의 길이를 반환\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        img_path = self.file_names[index]\n","        img = cv2.imread(img_path)  # numpy array\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        Id = self.df.Id[index]\n","\n","        if self.transforms:\n","            img = self.transforms(image=img)[\"image\"]\n","\n","        # 이미지 데이터, target label\n","        return img, Id\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.258376Z","iopub.status.busy":"2021-12-12T07:14:10.257802Z","iopub.status.idle":"2021-12-12T07:14:10.285445Z","shell.execute_reply":"2021-12-12T07:14:10.284338Z","shell.execute_reply.started":"2021-12-12T07:14:10.258308Z"},"trusted":true},"outputs":[],"source":["df_test = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\n","# file_path에 해당하는 column을 만든다\n","df_test[\"file_path\"] = df_test[\"Id\"].apply(get_test_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.287398Z","iopub.status.busy":"2021-12-12T07:14:10.286892Z","iopub.status.idle":"2021-12-12T07:14:10.301186Z","shell.execute_reply":"2021-12-12T07:14:10.299835Z","shell.execute_reply.started":"2021-12-12T07:14:10.287356Z"},"trusted":true},"outputs":[],"source":["test_dataset = PawpularityTestDataset(\n","    root_dir=TRAIN_DIR, df=df_test, transforms=data_transforms[\"valid\"]\n",")\n","test_loader = DataLoader(\n","    dataset=test_dataset,\n","    batch_size=CONFIG[\"valid_batch_size\"],\n","    shuffle=False,\n","    pin_memory=True,\n","    num_workers=4,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.303894Z","iopub.status.busy":"2021-12-12T07:14:10.302795Z","iopub.status.idle":"2021-12-12T07:14:10.332073Z","shell.execute_reply":"2021-12-12T07:14:10.330921Z","shell.execute_reply.started":"2021-12-12T07:14:10.303835Z"},"trusted":true},"outputs":[],"source":["df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.334599Z","iopub.status.busy":"2021-12-12T07:14:10.333741Z","iopub.status.idle":"2021-12-12T07:14:10.808287Z","shell.execute_reply":"2021-12-12T07:14:10.807185Z","shell.execute_reply.started":"2021-12-12T07:14:10.33455Z"},"trusted":true},"outputs":[],"source":["print(len(test_loader))\n","with torch.no_grad():\n","    model.eval()\n","\n","    total_size = 0\n","    outputs = []\n","    Ids = []\n","\n","    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n","\n","    for step, (images, Id) in bar:\n","        images = images.cuda()\n","\n","        batch_size = images.shape[0]\n","\n","        output = model(images)\n","\n","        total_size += batch_size\n","\n","        outputs.append(output.cpu())\n","        Ids.extend(Id)\n","\n","outputs = np.concatenate(outputs, axis=0).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.810444Z","iopub.status.busy":"2021-12-12T07:14:10.810083Z","iopub.status.idle":"2021-12-12T07:14:10.820099Z","shell.execute_reply":"2021-12-12T07:14:10.819093Z","shell.execute_reply.started":"2021-12-12T07:14:10.810403Z"},"trusted":true},"outputs":[],"source":["submission = pd.DataFrame({\"Id\": Ids, \"Pawpularity\": outputs})\n","submission.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-12T07:14:10.822446Z","iopub.status.busy":"2021-12-12T07:14:10.821891Z","iopub.status.idle":"2021-12-12T07:14:10.843772Z","shell.execute_reply":"2021-12-12T07:14:10.840394Z","shell.execute_reply.started":"2021-12-12T07:14:10.822397Z"},"trusted":true},"outputs":[],"source":["submission"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":4}
