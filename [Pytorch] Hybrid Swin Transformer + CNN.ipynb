{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[Pytorch] Hybrid Swin Transformer + CNN](https://www.kaggle.com/debarshichanda/pytorch-hybrid-swin-transformer-cnn/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/rwightman/pytorch-image-models\n",
    "! pip install -q -U wandb albumentations timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# Pytorch Image Model Library\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \n",
      "Get your W&B access token from here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjiwon7258\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/jiwon7258/petfinder-pawpularity-score/runs/35x0oeb5\" target=\"_blank\">solar-tree-7</a></strong> to <a href=\"https://wandb.ai/jiwon7258/petfinder-pawpularity-score\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/jiwon7258/petfinder-pawpularity-score/runs/35x0oeb5?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbe46689d90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# for kaggle\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"wandb_api\")\n",
    "    wandb.login(key=api_key)\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "# for local\n",
    "wandb.login(key='d60a4af56f6cd9cccec7d9da1dbced7960b61310')\n",
    "wandb.init(project=\"petfinder-pawpularity-score\", entity=\"jiwon7258\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './input/'\n",
    "TRAIN_DIR = './input/train'\n",
    "TEST_DIR = './input//test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = dict(\n",
    "    seed=42,\n",
    "    backbone='swin_base_patch4_window7_224',\n",
    "    embedder='tf_efficientnet_b4_ns',\n",
    "    train_batch_size=16,\n",
    "    valid_batch_size=32,\n",
    "    img_size=448,\n",
    "    epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "    scheduler='CosineAnnealingLR',\n",
    "    min_lr=1e-6,\n",
    "    T_max=100,\n",
    "    weight_decay=1e-6,\n",
    "    n_accumulate=1,\n",
    "    n_fold=5,\n",
    "    num_classes=1,\n",
    "    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    competition='PetFinder',\n",
    "    _wandb_kernel_='deb',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42) :\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_file_path(id):\n",
    "    return f'{TRAIN_DIR}/{id}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{ROOT_DIR}/train.csv')\n",
    "# file_path에 해당하는 column을 만든다\n",
    "df['file_path'] = df['Id'].apply(get_train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>./input/train/0007de18844b0dbbb5e1f607da0606e0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>./input/train/0009c66b9439883ba2750fb825e1d7db...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>./input/train/0013fd999caf9a3efe1352ca1b0d937e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>./input/train/0018df346ac9c1d8413cfcc888ca8246...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>./input/train/001dc955e10590d3ca4673f034feeef2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  ...                                          file_path\n",
       "0  0007de18844b0dbbb5e1f607da0606e0  ...  ./input/train/0007de18844b0dbbb5e1f607da0606e0...\n",
       "1  0009c66b9439883ba2750fb825e1d7db  ...  ./input/train/0009c66b9439883ba2750fb825e1d7db...\n",
       "2  0013fd999caf9a3efe1352ca1b0d937e  ...  ./input/train/0013fd999caf9a3efe1352ca1b0d937e...\n",
       "3  0018df346ac9c1d8413cfcc888ca8246  ...  ./input/train/0018df346ac9c1d8413cfcc888ca8246...\n",
       "4  001dc955e10590d3ca4673f034feeef2  ...  ./input/train/001dc955e10590d3ca4673f034feeef2...\n",
       "\n",
       "[5 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols를 통해 사용할 feature 목록을 관리한다\n",
    "feature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pawpularity는 0~100 사이의 정수 값을 가진다. Stratifed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(df, n_s=5, n_grp = None):\n",
    "    df['kfold'] = -1\n",
    "\n",
    "    if n_grp is None:\n",
    "        skf = KFold(n_splits=n_s, random_state=CONFIG['seed'])\n",
    "        target = df['Pawpularity']\n",
    "    else:\n",
    "        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG['seed'])\n",
    "        # Pawpularity를 구간별로, n_grp 수만큼 자른다\n",
    "        # 따라서 Pawpularity와 grp의 히스토그램 분포는 동일하다\n",
    "        df['grp'] = pd.cut(df['Pawpularity'], n_grp, labels=False)\n",
    "        target = df['grp']\n",
    "\n",
    "    # n_grp의 분포를 기반으로 StratifiedKFold를 진행한다\n",
    "    for fold_no, (t,v) in enumerate(skf.split(target,target)):\n",
    "        df.loc[v, 'kfold'] = fold_no\n",
    "\n",
    "    df = df.drop('grp', axis = 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>file_path</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>./input/train/0007de18844b0dbbb5e1f607da0606e0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>./input/train/0009c66b9439883ba2750fb825e1d7db...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>./input/train/0013fd999caf9a3efe1352ca1b0d937e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>./input/train/0018df346ac9c1d8413cfcc888ca8246...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>./input/train/001dc955e10590d3ca4673f034feeef2...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  ...  kfold\n",
       "0  0007de18844b0dbbb5e1f607da0606e0  ...      0\n",
       "1  0009c66b9439883ba2750fb825e1d7db  ...      2\n",
       "2  0013fd999caf9a3efe1352ca1b0d937e  ...      0\n",
       "3  0018df346ac9c1d8413cfcc888ca8246  ...      3\n",
       "4  001dc955e10590d3ca4673f034feeef2  ...      4\n",
       "\n",
       "[5 rows x 16 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_folds(df, n_s=CONFIG['n_fold'], n_grp=14)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pawpularity는 0~100까지의 정수로 이루어져 있다. 이 분포 그대로 KFold를 진행하지 않는다. 대신 n_grp개 만큼, 일정한 길이별로 구간을 나눈 후, 이 분포를 이용하여 KFold를 진행한다.\n",
    "\n",
    "```df.Pawpularity.hist(bins=14) == df.grp.hist(bins=14)```\n",
    "\n",
    "![Pawpularity Histogram](./img/paupularity.png)\n",
    "\n",
    "![이미지](./img/pawpularity_and_grp_hist.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PawpularityDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.file_names = df['file_path'].values    # numpy array\n",
    "        self.targets = df['Pawpularity'].values     # numpy array\n",
    "        self.transforms = transforms\n",
    "\n",
    "    # 데이터 프레임의 길이를 반환\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        img_path = self.file_names[index]\n",
    "        img = cv2.imread(img_path)                  # numpy array\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        target=self.targets[index]\n",
    "\n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        # 이미지 데이터, target label\n",
    "        return img, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        # 리사이징\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        # 가로 반전\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # 정규화\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ], p=1.),\n",
    "\n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEmbed(nn.Module):\n",
    "    '''\n",
    "    CNN Feature Map\n",
    "    Extract feature map from CNN, flatten, project to embedding idm.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, backbone, img_size=224, patch_size=1, feature_size=None, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        assert isinstance(backbone, nn.Module)\n",
    "\n",
    "        img_size = (img_size,img_size)\n",
    "        patch_size =(patch_size, patch_size)\n",
    "        \n",
    "        self.img_size=img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # backbone의 output feature_size를 모르는 경우, 가장 확실한 방법은 forward시켜 보는 것이다\n",
    "        # zero tensor를 생성해서 forward pass 시켜보자\n",
    "        if feature_size is None :\n",
    "            with torch.no_grad() :\n",
    "                # NOTE Most reliable way of determinig output idms is to run forward pass\n",
    "                training = backbone.training\n",
    "                if training:\n",
    "                    backbone.eval()\n",
    "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))           # bs, in_channel, feature_size\n",
    "                if isinstance(o, (list, tuple)):        \n",
    "                    o = o[-1] # last batch if backbone outputs list/tuple of features\n",
    "                feature_size = o.shape[-2:]\n",
    "                feature_dim = o.shape[1]\n",
    "                backbone.train(training)\n",
    "        \n",
    "        else :\n",
    "            feature_size = (feature_size, feature_size)\n",
    "            if hasattr(self.backbone, 'feature_info'):\n",
    "                feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "            else:\n",
    "                feature_dim = self.backbone.num_features\n",
    "        assert feature_size[0] % patch_size[0] == 0 and feature_size[1] % patch_size[1] == 0\n",
    "        self.grid_size = (feature_size[0] // patch_size[0], feature_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(in_channels=feature_dim, out_channels=embed_dim, kernel_size=patch_size, stride = patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        # if x is list or tuple\n",
    "        if isinstance(x, (list,tuple)):\n",
    "            x = x[-1] \n",
    "        x = self.proj(x).flatten(start_dim=2).transpose(1,2)\n",
    "        return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PawpularityModel(\n",
       "  (backbone): SwinTransformer(\n",
       "    (patch_embed): HybridEmbed(\n",
       "      (backbone): EfficientNetFeatures(\n",
       "        (conv_stem): Conv2dSame(3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (blocks): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): DepthwiseSeparableConv(\n",
       "              (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "              (bn1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): Identity()\n",
       "            )\n",
       "            (1): DepthwiseSeparableConv(\n",
       "              (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "              (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): Identity()\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2dSame(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "              (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "              (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): InvertedResidual(\n",
       "              (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "              (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): InvertedResidual(\n",
       "              (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "              (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2dSame(192, 192, kernel_size=(5, 5), stride=(2, 2), groups=192, bias=False)\n",
       "              (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
       "              (bn2): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): InvertedResidual(\n",
       "              (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
       "              (bn2): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): InvertedResidual(\n",
       "              (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
       "              (bn2): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2dSame(336, 336, kernel_size=(3, 3), stride=(2, 2), groups=336, bias=False)\n",
       "              (bn2): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): InvertedResidual(\n",
       "              (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): InvertedResidual(\n",
       "              (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (4): InvertedResidual(\n",
       "              (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (5): InvertedResidual(\n",
       "              (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "              (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "              (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (4): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (5): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2dSame(960, 960, kernel_size=(5, 5), stride=(2, 2), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (4): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (5): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (6): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (7): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(2688, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(2688, 2688, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2688, bias=False)\n",
       "              (bn2): BatchNorm2d(2688, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(2688, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(112, 2688, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (proj): Conv2d(56, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (0): BasicLayer(\n",
       "        dim=128, input_resolution=(56, 56), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(56, 56), dim=128\n",
       "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=256, input_resolution=(28, 28), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(28, 28), dim=256\n",
       "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=512, input_resolution=(14, 14), depth=18\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          input_resolution=(14, 14), dim=512\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=1024, input_resolution=(7, 7), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath()\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (embedder): EfficientNetFeatures(\n",
       "    (conv_stem): Conv2dSame(3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): SiLU(inplace=True)\n",
       "    (blocks): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "          (bn1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "        (1): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "          (bn1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(192, 192, kernel_size=(5, 5), stride=(2, 2), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
       "          (bn2): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
       "          (bn2): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(336, 336, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=336, bias=False)\n",
       "          (bn2): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(336, 336, kernel_size=(3, 3), stride=(2, 2), groups=336, bias=False)\n",
       "          (bn2): BatchNorm2d(336, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(336, 14, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(14, 336, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "          (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "          (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "          (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "          (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "          (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2dSame(960, 960, kernel_size=(5, 5), stride=(2, 2), groups=960, bias=False)\n",
       "          (bn2): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "          (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "          (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "          (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "          (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "          (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (6): InvertedResidual(\n",
       "          (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "          (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (7): InvertedResidual(\n",
       "          (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1632, 1632, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1632, bias=False)\n",
       "          (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(272, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "          (bn2): BatchNorm2d(1632, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(2688, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_dw): Conv2d(2688, 2688, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2688, bias=False)\n",
       "          (bn2): BatchNorm2d(2688, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): SiLU(inplace=True)\n",
       "          (se): SqueezeExcite(\n",
       "            (conv_reduce): Conv2d(2688, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act1): SiLU(inplace=True)\n",
       "            (conv_expand): Conv2d(112, 2688, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (gate): Sigmoid()\n",
       "          )\n",
       "          (conv_pwl): Conv2d(2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PawpularityModel(nn.Module):\n",
    "    def __init__ (self, backbone, embedder, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
    "        self.embedder = timm.create_model(embedder, features_only=True, out_indices=[2], pretrained=pretrained)\n",
    "        # backbone 모델 안에 있는 patch_embed block을 재설정한다\n",
    "        self.backbone.patch_embed=HybridEmbed(backbone = self.embedder, img_size=CONFIG['img_size'], embed_dim=128)\n",
    "        self.n_features = self.backbone.head.in_features\n",
    "        self.backbone.reset_classifier(0)\n",
    "        self.fc = nn.Linear(self.n_features, CONFIG['num_classes'])\n",
    "\n",
    "    def forward (self, images):\n",
    "        features = self.backbone(images)            # features = (batch size, embedding_size)\n",
    "        output = self.fc(features)                  # outputs = (batch_size, num_classes)\n",
    "        return output\n",
    "\n",
    "model = PawpularityModel(CONFIG['backbone'], CONFIG['embedder'])\n",
    "model.to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "        Conv2dSame-1         [-1, 48, 224, 224]           1,296\n",
      "        Conv2dSame-2         [-1, 48, 224, 224]           1,296\n",
      "       BatchNorm2d-3         [-1, 48, 224, 224]              96\n",
      "       BatchNorm2d-4         [-1, 48, 224, 224]              96\n",
      "              SiLU-5         [-1, 48, 224, 224]               0\n",
      "              SiLU-6         [-1, 48, 224, 224]               0\n",
      "            Conv2d-7         [-1, 48, 224, 224]             432\n",
      "            Conv2d-8         [-1, 48, 224, 224]             432\n",
      "       BatchNorm2d-9         [-1, 48, 224, 224]              96\n",
      "      BatchNorm2d-10         [-1, 48, 224, 224]              96\n",
      "             SiLU-11         [-1, 48, 224, 224]               0\n",
      "             SiLU-12         [-1, 48, 224, 224]               0\n",
      "           Conv2d-13             [-1, 12, 1, 1]             588\n",
      "           Conv2d-14             [-1, 12, 1, 1]             588\n",
      "             SiLU-15             [-1, 12, 1, 1]               0\n",
      "             SiLU-16             [-1, 12, 1, 1]               0\n",
      "           Conv2d-17             [-1, 48, 1, 1]             624\n",
      "           Conv2d-18             [-1, 48, 1, 1]             624\n",
      "          Sigmoid-19             [-1, 48, 1, 1]               0\n",
      "          Sigmoid-20             [-1, 48, 1, 1]               0\n",
      "    SqueezeExcite-21         [-1, 48, 224, 224]               0\n",
      "    SqueezeExcite-22         [-1, 48, 224, 224]               0\n",
      "           Conv2d-23         [-1, 24, 224, 224]           1,152\n",
      "           Conv2d-24         [-1, 24, 224, 224]           1,152\n",
      "      BatchNorm2d-25         [-1, 24, 224, 224]              48\n",
      "      BatchNorm2d-26         [-1, 24, 224, 224]              48\n",
      "         Identity-27         [-1, 24, 224, 224]               0\n",
      "         Identity-28         [-1, 24, 224, 224]               0\n",
      "DepthwiseSeparableConv-29         [-1, 24, 224, 224]               0\n",
      "DepthwiseSeparableConv-30         [-1, 24, 224, 224]               0\n",
      "           Conv2d-31         [-1, 24, 224, 224]             216\n",
      "           Conv2d-32         [-1, 24, 224, 224]             216\n",
      "      BatchNorm2d-33         [-1, 24, 224, 224]              48\n",
      "      BatchNorm2d-34         [-1, 24, 224, 224]              48\n",
      "             SiLU-35         [-1, 24, 224, 224]               0\n",
      "             SiLU-36         [-1, 24, 224, 224]               0\n",
      "           Conv2d-37              [-1, 6, 1, 1]             150\n",
      "           Conv2d-38              [-1, 6, 1, 1]             150\n",
      "             SiLU-39              [-1, 6, 1, 1]               0\n",
      "             SiLU-40              [-1, 6, 1, 1]               0\n",
      "           Conv2d-41             [-1, 24, 1, 1]             168\n",
      "           Conv2d-42             [-1, 24, 1, 1]             168\n",
      "          Sigmoid-43             [-1, 24, 1, 1]               0\n",
      "          Sigmoid-44             [-1, 24, 1, 1]               0\n",
      "    SqueezeExcite-45         [-1, 24, 224, 224]               0\n",
      "    SqueezeExcite-46         [-1, 24, 224, 224]               0\n",
      "           Conv2d-47         [-1, 24, 224, 224]             576\n",
      "           Conv2d-48         [-1, 24, 224, 224]             576\n",
      "      BatchNorm2d-49         [-1, 24, 224, 224]              48\n",
      "      BatchNorm2d-50         [-1, 24, 224, 224]              48\n",
      "         Identity-51         [-1, 24, 224, 224]               0\n",
      "         Identity-52         [-1, 24, 224, 224]               0\n",
      "DepthwiseSeparableConv-53         [-1, 24, 224, 224]               0\n",
      "DepthwiseSeparableConv-54         [-1, 24, 224, 224]               0\n",
      "           Conv2d-55        [-1, 144, 224, 224]           3,456\n",
      "           Conv2d-56        [-1, 144, 224, 224]           3,456\n",
      "      BatchNorm2d-57        [-1, 144, 224, 224]             288\n",
      "      BatchNorm2d-58        [-1, 144, 224, 224]             288\n",
      "             SiLU-59        [-1, 144, 224, 224]               0\n",
      "             SiLU-60        [-1, 144, 224, 224]               0\n",
      "       Conv2dSame-61        [-1, 144, 112, 112]           1,296\n",
      "       Conv2dSame-62        [-1, 144, 112, 112]           1,296\n",
      "      BatchNorm2d-63        [-1, 144, 112, 112]             288\n",
      "      BatchNorm2d-64        [-1, 144, 112, 112]             288\n",
      "             SiLU-65        [-1, 144, 112, 112]               0\n",
      "             SiLU-66        [-1, 144, 112, 112]               0\n",
      "           Conv2d-67              [-1, 6, 1, 1]             870\n",
      "           Conv2d-68              [-1, 6, 1, 1]             870\n",
      "             SiLU-69              [-1, 6, 1, 1]               0\n",
      "             SiLU-70              [-1, 6, 1, 1]               0\n",
      "           Conv2d-71            [-1, 144, 1, 1]           1,008\n",
      "           Conv2d-72            [-1, 144, 1, 1]           1,008\n",
      "          Sigmoid-73            [-1, 144, 1, 1]               0\n",
      "          Sigmoid-74            [-1, 144, 1, 1]               0\n",
      "    SqueezeExcite-75        [-1, 144, 112, 112]               0\n",
      "    SqueezeExcite-76        [-1, 144, 112, 112]               0\n",
      "           Conv2d-77         [-1, 32, 112, 112]           4,608\n",
      "           Conv2d-78         [-1, 32, 112, 112]           4,608\n",
      "      BatchNorm2d-79         [-1, 32, 112, 112]              64\n",
      "      BatchNorm2d-80         [-1, 32, 112, 112]              64\n",
      " InvertedResidual-81         [-1, 32, 112, 112]               0\n",
      " InvertedResidual-82         [-1, 32, 112, 112]               0\n",
      "           Conv2d-83        [-1, 192, 112, 112]           6,144\n",
      "           Conv2d-84        [-1, 192, 112, 112]           6,144\n",
      "      BatchNorm2d-85        [-1, 192, 112, 112]             384\n",
      "      BatchNorm2d-86        [-1, 192, 112, 112]             384\n",
      "             SiLU-87        [-1, 192, 112, 112]               0\n",
      "             SiLU-88        [-1, 192, 112, 112]               0\n",
      "           Conv2d-89        [-1, 192, 112, 112]           1,728\n",
      "           Conv2d-90        [-1, 192, 112, 112]           1,728\n",
      "      BatchNorm2d-91        [-1, 192, 112, 112]             384\n",
      "      BatchNorm2d-92        [-1, 192, 112, 112]             384\n",
      "             SiLU-93        [-1, 192, 112, 112]               0\n",
      "             SiLU-94        [-1, 192, 112, 112]               0\n",
      "           Conv2d-95              [-1, 8, 1, 1]           1,544\n",
      "           Conv2d-96              [-1, 8, 1, 1]           1,544\n",
      "             SiLU-97              [-1, 8, 1, 1]               0\n",
      "             SiLU-98              [-1, 8, 1, 1]               0\n",
      "           Conv2d-99            [-1, 192, 1, 1]           1,728\n",
      "          Conv2d-100            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-101            [-1, 192, 1, 1]               0\n",
      "         Sigmoid-102            [-1, 192, 1, 1]               0\n",
      "   SqueezeExcite-103        [-1, 192, 112, 112]               0\n",
      "   SqueezeExcite-104        [-1, 192, 112, 112]               0\n",
      "          Conv2d-105         [-1, 32, 112, 112]           6,144\n",
      "          Conv2d-106         [-1, 32, 112, 112]           6,144\n",
      "     BatchNorm2d-107         [-1, 32, 112, 112]              64\n",
      "     BatchNorm2d-108         [-1, 32, 112, 112]              64\n",
      "InvertedResidual-109         [-1, 32, 112, 112]               0\n",
      "InvertedResidual-110         [-1, 32, 112, 112]               0\n",
      "          Conv2d-111        [-1, 192, 112, 112]           6,144\n",
      "          Conv2d-112        [-1, 192, 112, 112]           6,144\n",
      "     BatchNorm2d-113        [-1, 192, 112, 112]             384\n",
      "     BatchNorm2d-114        [-1, 192, 112, 112]             384\n",
      "            SiLU-115        [-1, 192, 112, 112]               0\n",
      "            SiLU-116        [-1, 192, 112, 112]               0\n",
      "          Conv2d-117        [-1, 192, 112, 112]           1,728\n",
      "          Conv2d-118        [-1, 192, 112, 112]           1,728\n",
      "     BatchNorm2d-119        [-1, 192, 112, 112]             384\n",
      "     BatchNorm2d-120        [-1, 192, 112, 112]             384\n",
      "            SiLU-121        [-1, 192, 112, 112]               0\n",
      "            SiLU-122        [-1, 192, 112, 112]               0\n",
      "          Conv2d-123              [-1, 8, 1, 1]           1,544\n",
      "          Conv2d-124              [-1, 8, 1, 1]           1,544\n",
      "            SiLU-125              [-1, 8, 1, 1]               0\n",
      "            SiLU-126              [-1, 8, 1, 1]               0\n",
      "          Conv2d-127            [-1, 192, 1, 1]           1,728\n",
      "          Conv2d-128            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-129            [-1, 192, 1, 1]               0\n",
      "         Sigmoid-130            [-1, 192, 1, 1]               0\n",
      "   SqueezeExcite-131        [-1, 192, 112, 112]               0\n",
      "   SqueezeExcite-132        [-1, 192, 112, 112]               0\n",
      "          Conv2d-133         [-1, 32, 112, 112]           6,144\n",
      "          Conv2d-134         [-1, 32, 112, 112]           6,144\n",
      "     BatchNorm2d-135         [-1, 32, 112, 112]              64\n",
      "     BatchNorm2d-136         [-1, 32, 112, 112]              64\n",
      "InvertedResidual-137         [-1, 32, 112, 112]               0\n",
      "InvertedResidual-138         [-1, 32, 112, 112]               0\n",
      "          Conv2d-139        [-1, 192, 112, 112]           6,144\n",
      "          Conv2d-140        [-1, 192, 112, 112]           6,144\n",
      "     BatchNorm2d-141        [-1, 192, 112, 112]             384\n",
      "     BatchNorm2d-142        [-1, 192, 112, 112]             384\n",
      "            SiLU-143        [-1, 192, 112, 112]               0\n",
      "            SiLU-144        [-1, 192, 112, 112]               0\n",
      "          Conv2d-145        [-1, 192, 112, 112]           1,728\n",
      "          Conv2d-146        [-1, 192, 112, 112]           1,728\n",
      "     BatchNorm2d-147        [-1, 192, 112, 112]             384\n",
      "     BatchNorm2d-148        [-1, 192, 112, 112]             384\n",
      "            SiLU-149        [-1, 192, 112, 112]               0\n",
      "            SiLU-150        [-1, 192, 112, 112]               0\n",
      "          Conv2d-151              [-1, 8, 1, 1]           1,544\n",
      "          Conv2d-152              [-1, 8, 1, 1]           1,544\n",
      "            SiLU-153              [-1, 8, 1, 1]               0\n",
      "            SiLU-154              [-1, 8, 1, 1]               0\n",
      "          Conv2d-155            [-1, 192, 1, 1]           1,728\n",
      "          Conv2d-156            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-157            [-1, 192, 1, 1]               0\n",
      "         Sigmoid-158            [-1, 192, 1, 1]               0\n",
      "   SqueezeExcite-159        [-1, 192, 112, 112]               0\n",
      "   SqueezeExcite-160        [-1, 192, 112, 112]               0\n",
      "          Conv2d-161         [-1, 32, 112, 112]           6,144\n",
      "          Conv2d-162         [-1, 32, 112, 112]           6,144\n",
      "     BatchNorm2d-163         [-1, 32, 112, 112]              64\n",
      "     BatchNorm2d-164         [-1, 32, 112, 112]              64\n",
      "InvertedResidual-165         [-1, 32, 112, 112]               0\n",
      "InvertedResidual-166         [-1, 32, 112, 112]               0\n",
      "          Conv2d-167        [-1, 192, 112, 112]           6,144\n",
      "          Conv2d-168        [-1, 192, 112, 112]           6,144\n",
      "     BatchNorm2d-169        [-1, 192, 112, 112]             384\n",
      "     BatchNorm2d-170        [-1, 192, 112, 112]             384\n",
      "            SiLU-171        [-1, 192, 112, 112]               0\n",
      "            SiLU-172        [-1, 192, 112, 112]               0\n",
      "      Conv2dSame-173          [-1, 192, 56, 56]           4,800\n",
      "      Conv2dSame-174          [-1, 192, 56, 56]           4,800\n",
      "     BatchNorm2d-175          [-1, 192, 56, 56]             384\n",
      "     BatchNorm2d-176          [-1, 192, 56, 56]             384\n",
      "            SiLU-177          [-1, 192, 56, 56]               0\n",
      "            SiLU-178          [-1, 192, 56, 56]               0\n",
      "          Conv2d-179              [-1, 8, 1, 1]           1,544\n",
      "          Conv2d-180              [-1, 8, 1, 1]           1,544\n",
      "            SiLU-181              [-1, 8, 1, 1]               0\n",
      "            SiLU-182              [-1, 8, 1, 1]               0\n",
      "          Conv2d-183            [-1, 192, 1, 1]           1,728\n",
      "          Conv2d-184            [-1, 192, 1, 1]           1,728\n",
      "         Sigmoid-185            [-1, 192, 1, 1]               0\n",
      "         Sigmoid-186            [-1, 192, 1, 1]               0\n",
      "   SqueezeExcite-187          [-1, 192, 56, 56]               0\n",
      "   SqueezeExcite-188          [-1, 192, 56, 56]               0\n",
      "          Conv2d-189           [-1, 56, 56, 56]          10,752\n",
      "          Conv2d-190           [-1, 56, 56, 56]          10,752\n",
      "     BatchNorm2d-191           [-1, 56, 56, 56]             112\n",
      "     BatchNorm2d-192           [-1, 56, 56, 56]             112\n",
      "InvertedResidual-193           [-1, 56, 56, 56]               0\n",
      "InvertedResidual-194           [-1, 56, 56, 56]               0\n",
      "          Conv2d-195          [-1, 336, 56, 56]          18,816\n",
      "          Conv2d-196          [-1, 336, 56, 56]          18,816\n",
      "     BatchNorm2d-197          [-1, 336, 56, 56]             672\n",
      "     BatchNorm2d-198          [-1, 336, 56, 56]             672\n",
      "            SiLU-199          [-1, 336, 56, 56]               0\n",
      "            SiLU-200          [-1, 336, 56, 56]               0\n",
      "          Conv2d-201          [-1, 336, 56, 56]           8,400\n",
      "          Conv2d-202          [-1, 336, 56, 56]           8,400\n",
      "     BatchNorm2d-203          [-1, 336, 56, 56]             672\n",
      "     BatchNorm2d-204          [-1, 336, 56, 56]             672\n",
      "            SiLU-205          [-1, 336, 56, 56]               0\n",
      "            SiLU-206          [-1, 336, 56, 56]               0\n",
      "          Conv2d-207             [-1, 14, 1, 1]           4,718\n",
      "          Conv2d-208             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-209             [-1, 14, 1, 1]               0\n",
      "            SiLU-210             [-1, 14, 1, 1]               0\n",
      "          Conv2d-211            [-1, 336, 1, 1]           5,040\n",
      "          Conv2d-212            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-213            [-1, 336, 1, 1]               0\n",
      "         Sigmoid-214            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-215          [-1, 336, 56, 56]               0\n",
      "   SqueezeExcite-216          [-1, 336, 56, 56]               0\n",
      "          Conv2d-217           [-1, 56, 56, 56]          18,816\n",
      "          Conv2d-218           [-1, 56, 56, 56]          18,816\n",
      "     BatchNorm2d-219           [-1, 56, 56, 56]             112\n",
      "     BatchNorm2d-220           [-1, 56, 56, 56]             112\n",
      "InvertedResidual-221           [-1, 56, 56, 56]               0\n",
      "InvertedResidual-222           [-1, 56, 56, 56]               0\n",
      "          Conv2d-223          [-1, 336, 56, 56]          18,816\n",
      "          Conv2d-224          [-1, 336, 56, 56]          18,816\n",
      "     BatchNorm2d-225          [-1, 336, 56, 56]             672\n",
      "     BatchNorm2d-226          [-1, 336, 56, 56]             672\n",
      "            SiLU-227          [-1, 336, 56, 56]               0\n",
      "            SiLU-228          [-1, 336, 56, 56]               0\n",
      "          Conv2d-229          [-1, 336, 56, 56]           8,400\n",
      "          Conv2d-230          [-1, 336, 56, 56]           8,400\n",
      "     BatchNorm2d-231          [-1, 336, 56, 56]             672\n",
      "     BatchNorm2d-232          [-1, 336, 56, 56]             672\n",
      "            SiLU-233          [-1, 336, 56, 56]               0\n",
      "            SiLU-234          [-1, 336, 56, 56]               0\n",
      "          Conv2d-235             [-1, 14, 1, 1]           4,718\n",
      "          Conv2d-236             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-237             [-1, 14, 1, 1]               0\n",
      "            SiLU-238             [-1, 14, 1, 1]               0\n",
      "          Conv2d-239            [-1, 336, 1, 1]           5,040\n",
      "          Conv2d-240            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-241            [-1, 336, 1, 1]               0\n",
      "         Sigmoid-242            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-243          [-1, 336, 56, 56]               0\n",
      "   SqueezeExcite-244          [-1, 336, 56, 56]               0\n",
      "          Conv2d-245           [-1, 56, 56, 56]          18,816\n",
      "          Conv2d-246           [-1, 56, 56, 56]          18,816\n",
      "     BatchNorm2d-247           [-1, 56, 56, 56]             112\n",
      "     BatchNorm2d-248           [-1, 56, 56, 56]             112\n",
      "InvertedResidual-249           [-1, 56, 56, 56]               0\n",
      "InvertedResidual-250           [-1, 56, 56, 56]               0\n",
      "          Conv2d-251          [-1, 336, 56, 56]          18,816\n",
      "          Conv2d-252          [-1, 336, 56, 56]          18,816\n",
      "     BatchNorm2d-253          [-1, 336, 56, 56]             672\n",
      "     BatchNorm2d-254          [-1, 336, 56, 56]             672\n",
      "            SiLU-255          [-1, 336, 56, 56]               0\n",
      "            SiLU-256          [-1, 336, 56, 56]               0\n",
      "          Conv2d-257          [-1, 336, 56, 56]           8,400\n",
      "          Conv2d-258          [-1, 336, 56, 56]           8,400\n",
      "     BatchNorm2d-259          [-1, 336, 56, 56]             672\n",
      "     BatchNorm2d-260          [-1, 336, 56, 56]             672\n",
      "            SiLU-261          [-1, 336, 56, 56]               0\n",
      "            SiLU-262          [-1, 336, 56, 56]               0\n",
      "          Conv2d-263             [-1, 14, 1, 1]           4,718\n",
      "          Conv2d-264             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-265             [-1, 14, 1, 1]               0\n",
      "            SiLU-266             [-1, 14, 1, 1]               0\n",
      "          Conv2d-267            [-1, 336, 1, 1]           5,040\n",
      "          Conv2d-268            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-269            [-1, 336, 1, 1]               0\n",
      "         Sigmoid-270            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-271          [-1, 336, 56, 56]               0\n",
      "   SqueezeExcite-272          [-1, 336, 56, 56]               0\n",
      "          Conv2d-273           [-1, 56, 56, 56]          18,816\n",
      "          Conv2d-274           [-1, 56, 56, 56]          18,816\n",
      "     BatchNorm2d-275           [-1, 56, 56, 56]             112\n",
      "     BatchNorm2d-276           [-1, 56, 56, 56]             112\n",
      "InvertedResidual-277           [-1, 56, 56, 56]               0\n",
      "InvertedResidual-278           [-1, 56, 56, 56]               0\n",
      "          Conv2d-279          [-1, 336, 56, 56]          18,816\n",
      "          Conv2d-280          [-1, 336, 56, 56]          18,816\n",
      "     BatchNorm2d-281          [-1, 336, 56, 56]             672\n",
      "     BatchNorm2d-282          [-1, 336, 56, 56]             672\n",
      "            SiLU-283          [-1, 336, 56, 56]               0\n",
      "            SiLU-284          [-1, 336, 56, 56]               0\n",
      "      Conv2dSame-285          [-1, 336, 28, 28]           3,024\n",
      "      Conv2dSame-286          [-1, 336, 28, 28]           3,024\n",
      "     BatchNorm2d-287          [-1, 336, 28, 28]             672\n",
      "     BatchNorm2d-288          [-1, 336, 28, 28]             672\n",
      "            SiLU-289          [-1, 336, 28, 28]               0\n",
      "            SiLU-290          [-1, 336, 28, 28]               0\n",
      "          Conv2d-291             [-1, 14, 1, 1]           4,718\n",
      "          Conv2d-292             [-1, 14, 1, 1]           4,718\n",
      "            SiLU-293             [-1, 14, 1, 1]               0\n",
      "            SiLU-294             [-1, 14, 1, 1]               0\n",
      "          Conv2d-295            [-1, 336, 1, 1]           5,040\n",
      "          Conv2d-296            [-1, 336, 1, 1]           5,040\n",
      "         Sigmoid-297            [-1, 336, 1, 1]               0\n",
      "         Sigmoid-298            [-1, 336, 1, 1]               0\n",
      "   SqueezeExcite-299          [-1, 336, 28, 28]               0\n",
      "   SqueezeExcite-300          [-1, 336, 28, 28]               0\n",
      "          Conv2d-301          [-1, 112, 28, 28]          37,632\n",
      "          Conv2d-302          [-1, 112, 28, 28]          37,632\n",
      "     BatchNorm2d-303          [-1, 112, 28, 28]             224\n",
      "     BatchNorm2d-304          [-1, 112, 28, 28]             224\n",
      "InvertedResidual-305          [-1, 112, 28, 28]               0\n",
      "InvertedResidual-306          [-1, 112, 28, 28]               0\n",
      "          Conv2d-307          [-1, 672, 28, 28]          75,264\n",
      "          Conv2d-308          [-1, 672, 28, 28]          75,264\n",
      "     BatchNorm2d-309          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-310          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-311          [-1, 672, 28, 28]               0\n",
      "            SiLU-312          [-1, 672, 28, 28]               0\n",
      "          Conv2d-313          [-1, 672, 28, 28]           6,048\n",
      "          Conv2d-314          [-1, 672, 28, 28]           6,048\n",
      "     BatchNorm2d-315          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-316          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-317          [-1, 672, 28, 28]               0\n",
      "            SiLU-318          [-1, 672, 28, 28]               0\n",
      "          Conv2d-319             [-1, 28, 1, 1]          18,844\n",
      "          Conv2d-320             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-321             [-1, 28, 1, 1]               0\n",
      "            SiLU-322             [-1, 28, 1, 1]               0\n",
      "          Conv2d-323            [-1, 672, 1, 1]          19,488\n",
      "          Conv2d-324            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-325            [-1, 672, 1, 1]               0\n",
      "         Sigmoid-326            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-327          [-1, 672, 28, 28]               0\n",
      "   SqueezeExcite-328          [-1, 672, 28, 28]               0\n",
      "          Conv2d-329          [-1, 112, 28, 28]          75,264\n",
      "          Conv2d-330          [-1, 112, 28, 28]          75,264\n",
      "     BatchNorm2d-331          [-1, 112, 28, 28]             224\n",
      "     BatchNorm2d-332          [-1, 112, 28, 28]             224\n",
      "InvertedResidual-333          [-1, 112, 28, 28]               0\n",
      "InvertedResidual-334          [-1, 112, 28, 28]               0\n",
      "          Conv2d-335          [-1, 672, 28, 28]          75,264\n",
      "          Conv2d-336          [-1, 672, 28, 28]          75,264\n",
      "     BatchNorm2d-337          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-338          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-339          [-1, 672, 28, 28]               0\n",
      "            SiLU-340          [-1, 672, 28, 28]               0\n",
      "          Conv2d-341          [-1, 672, 28, 28]           6,048\n",
      "          Conv2d-342          [-1, 672, 28, 28]           6,048\n",
      "     BatchNorm2d-343          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-344          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-345          [-1, 672, 28, 28]               0\n",
      "            SiLU-346          [-1, 672, 28, 28]               0\n",
      "          Conv2d-347             [-1, 28, 1, 1]          18,844\n",
      "          Conv2d-348             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-349             [-1, 28, 1, 1]               0\n",
      "            SiLU-350             [-1, 28, 1, 1]               0\n",
      "          Conv2d-351            [-1, 672, 1, 1]          19,488\n",
      "          Conv2d-352            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-353            [-1, 672, 1, 1]               0\n",
      "         Sigmoid-354            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-355          [-1, 672, 28, 28]               0\n",
      "   SqueezeExcite-356          [-1, 672, 28, 28]               0\n",
      "          Conv2d-357          [-1, 112, 28, 28]          75,264\n",
      "          Conv2d-358          [-1, 112, 28, 28]          75,264\n",
      "     BatchNorm2d-359          [-1, 112, 28, 28]             224\n",
      "     BatchNorm2d-360          [-1, 112, 28, 28]             224\n",
      "InvertedResidual-361          [-1, 112, 28, 28]               0\n",
      "InvertedResidual-362          [-1, 112, 28, 28]               0\n",
      "          Conv2d-363          [-1, 672, 28, 28]          75,264\n",
      "          Conv2d-364          [-1, 672, 28, 28]          75,264\n",
      "     BatchNorm2d-365          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-366          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-367          [-1, 672, 28, 28]               0\n",
      "            SiLU-368          [-1, 672, 28, 28]               0\n",
      "          Conv2d-369          [-1, 672, 28, 28]           6,048\n",
      "          Conv2d-370          [-1, 672, 28, 28]           6,048\n",
      "     BatchNorm2d-371          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-372          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-373          [-1, 672, 28, 28]               0\n",
      "            SiLU-374          [-1, 672, 28, 28]               0\n",
      "          Conv2d-375             [-1, 28, 1, 1]          18,844\n",
      "          Conv2d-376             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-377             [-1, 28, 1, 1]               0\n",
      "            SiLU-378             [-1, 28, 1, 1]               0\n",
      "          Conv2d-379            [-1, 672, 1, 1]          19,488\n",
      "          Conv2d-380            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-381            [-1, 672, 1, 1]               0\n",
      "         Sigmoid-382            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-383          [-1, 672, 28, 28]               0\n",
      "   SqueezeExcite-384          [-1, 672, 28, 28]               0\n",
      "          Conv2d-385          [-1, 112, 28, 28]          75,264\n",
      "          Conv2d-386          [-1, 112, 28, 28]          75,264\n",
      "     BatchNorm2d-387          [-1, 112, 28, 28]             224\n",
      "     BatchNorm2d-388          [-1, 112, 28, 28]             224\n",
      "InvertedResidual-389          [-1, 112, 28, 28]               0\n",
      "InvertedResidual-390          [-1, 112, 28, 28]               0\n",
      "          Conv2d-391          [-1, 672, 28, 28]          75,264\n",
      "          Conv2d-392          [-1, 672, 28, 28]          75,264\n",
      "     BatchNorm2d-393          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-394          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-395          [-1, 672, 28, 28]               0\n",
      "            SiLU-396          [-1, 672, 28, 28]               0\n",
      "          Conv2d-397          [-1, 672, 28, 28]           6,048\n",
      "          Conv2d-398          [-1, 672, 28, 28]           6,048\n",
      "     BatchNorm2d-399          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-400          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-401          [-1, 672, 28, 28]               0\n",
      "            SiLU-402          [-1, 672, 28, 28]               0\n",
      "          Conv2d-403             [-1, 28, 1, 1]          18,844\n",
      "          Conv2d-404             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-405             [-1, 28, 1, 1]               0\n",
      "            SiLU-406             [-1, 28, 1, 1]               0\n",
      "          Conv2d-407            [-1, 672, 1, 1]          19,488\n",
      "          Conv2d-408            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-409            [-1, 672, 1, 1]               0\n",
      "         Sigmoid-410            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-411          [-1, 672, 28, 28]               0\n",
      "   SqueezeExcite-412          [-1, 672, 28, 28]               0\n",
      "          Conv2d-413          [-1, 112, 28, 28]          75,264\n",
      "          Conv2d-414          [-1, 112, 28, 28]          75,264\n",
      "     BatchNorm2d-415          [-1, 112, 28, 28]             224\n",
      "     BatchNorm2d-416          [-1, 112, 28, 28]             224\n",
      "InvertedResidual-417          [-1, 112, 28, 28]               0\n",
      "InvertedResidual-418          [-1, 112, 28, 28]               0\n",
      "          Conv2d-419          [-1, 672, 28, 28]          75,264\n",
      "          Conv2d-420          [-1, 672, 28, 28]          75,264\n",
      "     BatchNorm2d-421          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-422          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-423          [-1, 672, 28, 28]               0\n",
      "            SiLU-424          [-1, 672, 28, 28]               0\n",
      "          Conv2d-425          [-1, 672, 28, 28]           6,048\n",
      "          Conv2d-426          [-1, 672, 28, 28]           6,048\n",
      "     BatchNorm2d-427          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-428          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-429          [-1, 672, 28, 28]               0\n",
      "            SiLU-430          [-1, 672, 28, 28]               0\n",
      "          Conv2d-431             [-1, 28, 1, 1]          18,844\n",
      "          Conv2d-432             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-433             [-1, 28, 1, 1]               0\n",
      "            SiLU-434             [-1, 28, 1, 1]               0\n",
      "          Conv2d-435            [-1, 672, 1, 1]          19,488\n",
      "          Conv2d-436            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-437            [-1, 672, 1, 1]               0\n",
      "         Sigmoid-438            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-439          [-1, 672, 28, 28]               0\n",
      "   SqueezeExcite-440          [-1, 672, 28, 28]               0\n",
      "          Conv2d-441          [-1, 112, 28, 28]          75,264\n",
      "          Conv2d-442          [-1, 112, 28, 28]          75,264\n",
      "     BatchNorm2d-443          [-1, 112, 28, 28]             224\n",
      "     BatchNorm2d-444          [-1, 112, 28, 28]             224\n",
      "InvertedResidual-445          [-1, 112, 28, 28]               0\n",
      "InvertedResidual-446          [-1, 112, 28, 28]               0\n",
      "          Conv2d-447          [-1, 672, 28, 28]          75,264\n",
      "          Conv2d-448          [-1, 672, 28, 28]          75,264\n",
      "     BatchNorm2d-449          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-450          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-451          [-1, 672, 28, 28]               0\n",
      "            SiLU-452          [-1, 672, 28, 28]               0\n",
      "          Conv2d-453          [-1, 672, 28, 28]          16,800\n",
      "          Conv2d-454          [-1, 672, 28, 28]          16,800\n",
      "     BatchNorm2d-455          [-1, 672, 28, 28]           1,344\n",
      "     BatchNorm2d-456          [-1, 672, 28, 28]           1,344\n",
      "            SiLU-457          [-1, 672, 28, 28]               0\n",
      "            SiLU-458          [-1, 672, 28, 28]               0\n",
      "          Conv2d-459             [-1, 28, 1, 1]          18,844\n",
      "          Conv2d-460             [-1, 28, 1, 1]          18,844\n",
      "            SiLU-461             [-1, 28, 1, 1]               0\n",
      "            SiLU-462             [-1, 28, 1, 1]               0\n",
      "          Conv2d-463            [-1, 672, 1, 1]          19,488\n",
      "          Conv2d-464            [-1, 672, 1, 1]          19,488\n",
      "         Sigmoid-465            [-1, 672, 1, 1]               0\n",
      "         Sigmoid-466            [-1, 672, 1, 1]               0\n",
      "   SqueezeExcite-467          [-1, 672, 28, 28]               0\n",
      "   SqueezeExcite-468          [-1, 672, 28, 28]               0\n",
      "          Conv2d-469          [-1, 160, 28, 28]         107,520\n",
      "          Conv2d-470          [-1, 160, 28, 28]         107,520\n",
      "     BatchNorm2d-471          [-1, 160, 28, 28]             320\n",
      "     BatchNorm2d-472          [-1, 160, 28, 28]             320\n",
      "InvertedResidual-473          [-1, 160, 28, 28]               0\n",
      "InvertedResidual-474          [-1, 160, 28, 28]               0\n",
      "          Conv2d-475          [-1, 960, 28, 28]         153,600\n",
      "          Conv2d-476          [-1, 960, 28, 28]         153,600\n",
      "     BatchNorm2d-477          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-478          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-479          [-1, 960, 28, 28]               0\n",
      "            SiLU-480          [-1, 960, 28, 28]               0\n",
      "          Conv2d-481          [-1, 960, 28, 28]          24,000\n",
      "          Conv2d-482          [-1, 960, 28, 28]          24,000\n",
      "     BatchNorm2d-483          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-484          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-485          [-1, 960, 28, 28]               0\n",
      "            SiLU-486          [-1, 960, 28, 28]               0\n",
      "          Conv2d-487             [-1, 40, 1, 1]          38,440\n",
      "          Conv2d-488             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-489             [-1, 40, 1, 1]               0\n",
      "            SiLU-490             [-1, 40, 1, 1]               0\n",
      "          Conv2d-491            [-1, 960, 1, 1]          39,360\n",
      "          Conv2d-492            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-493            [-1, 960, 1, 1]               0\n",
      "         Sigmoid-494            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-495          [-1, 960, 28, 28]               0\n",
      "   SqueezeExcite-496          [-1, 960, 28, 28]               0\n",
      "          Conv2d-497          [-1, 160, 28, 28]         153,600\n",
      "          Conv2d-498          [-1, 160, 28, 28]         153,600\n",
      "     BatchNorm2d-499          [-1, 160, 28, 28]             320\n",
      "     BatchNorm2d-500          [-1, 160, 28, 28]             320\n",
      "InvertedResidual-501          [-1, 160, 28, 28]               0\n",
      "InvertedResidual-502          [-1, 160, 28, 28]               0\n",
      "          Conv2d-503          [-1, 960, 28, 28]         153,600\n",
      "          Conv2d-504          [-1, 960, 28, 28]         153,600\n",
      "     BatchNorm2d-505          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-506          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-507          [-1, 960, 28, 28]               0\n",
      "            SiLU-508          [-1, 960, 28, 28]               0\n",
      "          Conv2d-509          [-1, 960, 28, 28]          24,000\n",
      "          Conv2d-510          [-1, 960, 28, 28]          24,000\n",
      "     BatchNorm2d-511          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-512          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-513          [-1, 960, 28, 28]               0\n",
      "            SiLU-514          [-1, 960, 28, 28]               0\n",
      "          Conv2d-515             [-1, 40, 1, 1]          38,440\n",
      "          Conv2d-516             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-517             [-1, 40, 1, 1]               0\n",
      "            SiLU-518             [-1, 40, 1, 1]               0\n",
      "          Conv2d-519            [-1, 960, 1, 1]          39,360\n",
      "          Conv2d-520            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-521            [-1, 960, 1, 1]               0\n",
      "         Sigmoid-522            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-523          [-1, 960, 28, 28]               0\n",
      "   SqueezeExcite-524          [-1, 960, 28, 28]               0\n",
      "          Conv2d-525          [-1, 160, 28, 28]         153,600\n",
      "          Conv2d-526          [-1, 160, 28, 28]         153,600\n",
      "     BatchNorm2d-527          [-1, 160, 28, 28]             320\n",
      "     BatchNorm2d-528          [-1, 160, 28, 28]             320\n",
      "InvertedResidual-529          [-1, 160, 28, 28]               0\n",
      "InvertedResidual-530          [-1, 160, 28, 28]               0\n",
      "          Conv2d-531          [-1, 960, 28, 28]         153,600\n",
      "          Conv2d-532          [-1, 960, 28, 28]         153,600\n",
      "     BatchNorm2d-533          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-534          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-535          [-1, 960, 28, 28]               0\n",
      "            SiLU-536          [-1, 960, 28, 28]               0\n",
      "          Conv2d-537          [-1, 960, 28, 28]          24,000\n",
      "          Conv2d-538          [-1, 960, 28, 28]          24,000\n",
      "     BatchNorm2d-539          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-540          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-541          [-1, 960, 28, 28]               0\n",
      "            SiLU-542          [-1, 960, 28, 28]               0\n",
      "          Conv2d-543             [-1, 40, 1, 1]          38,440\n",
      "          Conv2d-544             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-545             [-1, 40, 1, 1]               0\n",
      "            SiLU-546             [-1, 40, 1, 1]               0\n",
      "          Conv2d-547            [-1, 960, 1, 1]          39,360\n",
      "          Conv2d-548            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-549            [-1, 960, 1, 1]               0\n",
      "         Sigmoid-550            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-551          [-1, 960, 28, 28]               0\n",
      "   SqueezeExcite-552          [-1, 960, 28, 28]               0\n",
      "          Conv2d-553          [-1, 160, 28, 28]         153,600\n",
      "          Conv2d-554          [-1, 160, 28, 28]         153,600\n",
      "     BatchNorm2d-555          [-1, 160, 28, 28]             320\n",
      "     BatchNorm2d-556          [-1, 160, 28, 28]             320\n",
      "InvertedResidual-557          [-1, 160, 28, 28]               0\n",
      "InvertedResidual-558          [-1, 160, 28, 28]               0\n",
      "          Conv2d-559          [-1, 960, 28, 28]         153,600\n",
      "          Conv2d-560          [-1, 960, 28, 28]         153,600\n",
      "     BatchNorm2d-561          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-562          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-563          [-1, 960, 28, 28]               0\n",
      "            SiLU-564          [-1, 960, 28, 28]               0\n",
      "          Conv2d-565          [-1, 960, 28, 28]          24,000\n",
      "          Conv2d-566          [-1, 960, 28, 28]          24,000\n",
      "     BatchNorm2d-567          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-568          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-569          [-1, 960, 28, 28]               0\n",
      "            SiLU-570          [-1, 960, 28, 28]               0\n",
      "          Conv2d-571             [-1, 40, 1, 1]          38,440\n",
      "          Conv2d-572             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-573             [-1, 40, 1, 1]               0\n",
      "            SiLU-574             [-1, 40, 1, 1]               0\n",
      "          Conv2d-575            [-1, 960, 1, 1]          39,360\n",
      "          Conv2d-576            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-577            [-1, 960, 1, 1]               0\n",
      "         Sigmoid-578            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-579          [-1, 960, 28, 28]               0\n",
      "   SqueezeExcite-580          [-1, 960, 28, 28]               0\n",
      "          Conv2d-581          [-1, 160, 28, 28]         153,600\n",
      "          Conv2d-582          [-1, 160, 28, 28]         153,600\n",
      "     BatchNorm2d-583          [-1, 160, 28, 28]             320\n",
      "     BatchNorm2d-584          [-1, 160, 28, 28]             320\n",
      "InvertedResidual-585          [-1, 160, 28, 28]               0\n",
      "InvertedResidual-586          [-1, 160, 28, 28]               0\n",
      "          Conv2d-587          [-1, 960, 28, 28]         153,600\n",
      "          Conv2d-588          [-1, 960, 28, 28]         153,600\n",
      "     BatchNorm2d-589          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-590          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-591          [-1, 960, 28, 28]               0\n",
      "            SiLU-592          [-1, 960, 28, 28]               0\n",
      "          Conv2d-593          [-1, 960, 28, 28]          24,000\n",
      "          Conv2d-594          [-1, 960, 28, 28]          24,000\n",
      "     BatchNorm2d-595          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-596          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-597          [-1, 960, 28, 28]               0\n",
      "            SiLU-598          [-1, 960, 28, 28]               0\n",
      "          Conv2d-599             [-1, 40, 1, 1]          38,440\n",
      "          Conv2d-600             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-601             [-1, 40, 1, 1]               0\n",
      "            SiLU-602             [-1, 40, 1, 1]               0\n",
      "          Conv2d-603            [-1, 960, 1, 1]          39,360\n",
      "          Conv2d-604            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-605            [-1, 960, 1, 1]               0\n",
      "         Sigmoid-606            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-607          [-1, 960, 28, 28]               0\n",
      "   SqueezeExcite-608          [-1, 960, 28, 28]               0\n",
      "          Conv2d-609          [-1, 160, 28, 28]         153,600\n",
      "          Conv2d-610          [-1, 160, 28, 28]         153,600\n",
      "     BatchNorm2d-611          [-1, 160, 28, 28]             320\n",
      "     BatchNorm2d-612          [-1, 160, 28, 28]             320\n",
      "InvertedResidual-613          [-1, 160, 28, 28]               0\n",
      "InvertedResidual-614          [-1, 160, 28, 28]               0\n",
      "          Conv2d-615          [-1, 960, 28, 28]         153,600\n",
      "          Conv2d-616          [-1, 960, 28, 28]         153,600\n",
      "     BatchNorm2d-617          [-1, 960, 28, 28]           1,920\n",
      "     BatchNorm2d-618          [-1, 960, 28, 28]           1,920\n",
      "            SiLU-619          [-1, 960, 28, 28]               0\n",
      "            SiLU-620          [-1, 960, 28, 28]               0\n",
      "      Conv2dSame-621          [-1, 960, 14, 14]          24,000\n",
      "      Conv2dSame-622          [-1, 960, 14, 14]          24,000\n",
      "     BatchNorm2d-623          [-1, 960, 14, 14]           1,920\n",
      "     BatchNorm2d-624          [-1, 960, 14, 14]           1,920\n",
      "            SiLU-625          [-1, 960, 14, 14]               0\n",
      "            SiLU-626          [-1, 960, 14, 14]               0\n",
      "          Conv2d-627             [-1, 40, 1, 1]          38,440\n",
      "          Conv2d-628             [-1, 40, 1, 1]          38,440\n",
      "            SiLU-629             [-1, 40, 1, 1]               0\n",
      "            SiLU-630             [-1, 40, 1, 1]               0\n",
      "          Conv2d-631            [-1, 960, 1, 1]          39,360\n",
      "          Conv2d-632            [-1, 960, 1, 1]          39,360\n",
      "         Sigmoid-633            [-1, 960, 1, 1]               0\n",
      "         Sigmoid-634            [-1, 960, 1, 1]               0\n",
      "   SqueezeExcite-635          [-1, 960, 14, 14]               0\n",
      "   SqueezeExcite-636          [-1, 960, 14, 14]               0\n",
      "          Conv2d-637          [-1, 272, 14, 14]         261,120\n",
      "          Conv2d-638          [-1, 272, 14, 14]         261,120\n",
      "     BatchNorm2d-639          [-1, 272, 14, 14]             544\n",
      "     BatchNorm2d-640          [-1, 272, 14, 14]             544\n",
      "InvertedResidual-641          [-1, 272, 14, 14]               0\n",
      "InvertedResidual-642          [-1, 272, 14, 14]               0\n",
      "          Conv2d-643         [-1, 1632, 14, 14]         443,904\n",
      "          Conv2d-644         [-1, 1632, 14, 14]         443,904\n",
      "     BatchNorm2d-645         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-646         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-647         [-1, 1632, 14, 14]               0\n",
      "            SiLU-648         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-649         [-1, 1632, 14, 14]          40,800\n",
      "          Conv2d-650         [-1, 1632, 14, 14]          40,800\n",
      "     BatchNorm2d-651         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-652         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-653         [-1, 1632, 14, 14]               0\n",
      "            SiLU-654         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-655             [-1, 68, 1, 1]         111,044\n",
      "          Conv2d-656             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-657             [-1, 68, 1, 1]               0\n",
      "            SiLU-658             [-1, 68, 1, 1]               0\n",
      "          Conv2d-659           [-1, 1632, 1, 1]         112,608\n",
      "          Conv2d-660           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-661           [-1, 1632, 1, 1]               0\n",
      "         Sigmoid-662           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-663         [-1, 1632, 14, 14]               0\n",
      "   SqueezeExcite-664         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-665          [-1, 272, 14, 14]         443,904\n",
      "          Conv2d-666          [-1, 272, 14, 14]         443,904\n",
      "     BatchNorm2d-667          [-1, 272, 14, 14]             544\n",
      "     BatchNorm2d-668          [-1, 272, 14, 14]             544\n",
      "InvertedResidual-669          [-1, 272, 14, 14]               0\n",
      "InvertedResidual-670          [-1, 272, 14, 14]               0\n",
      "          Conv2d-671         [-1, 1632, 14, 14]         443,904\n",
      "          Conv2d-672         [-1, 1632, 14, 14]         443,904\n",
      "     BatchNorm2d-673         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-674         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-675         [-1, 1632, 14, 14]               0\n",
      "            SiLU-676         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-677         [-1, 1632, 14, 14]          40,800\n",
      "          Conv2d-678         [-1, 1632, 14, 14]          40,800\n",
      "     BatchNorm2d-679         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-680         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-681         [-1, 1632, 14, 14]               0\n",
      "            SiLU-682         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-683             [-1, 68, 1, 1]         111,044\n",
      "          Conv2d-684             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-685             [-1, 68, 1, 1]               0\n",
      "            SiLU-686             [-1, 68, 1, 1]               0\n",
      "          Conv2d-687           [-1, 1632, 1, 1]         112,608\n",
      "          Conv2d-688           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-689           [-1, 1632, 1, 1]               0\n",
      "         Sigmoid-690           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-691         [-1, 1632, 14, 14]               0\n",
      "   SqueezeExcite-692         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-693          [-1, 272, 14, 14]         443,904\n",
      "          Conv2d-694          [-1, 272, 14, 14]         443,904\n",
      "     BatchNorm2d-695          [-1, 272, 14, 14]             544\n",
      "     BatchNorm2d-696          [-1, 272, 14, 14]             544\n",
      "InvertedResidual-697          [-1, 272, 14, 14]               0\n",
      "InvertedResidual-698          [-1, 272, 14, 14]               0\n",
      "          Conv2d-699         [-1, 1632, 14, 14]         443,904\n",
      "          Conv2d-700         [-1, 1632, 14, 14]         443,904\n",
      "     BatchNorm2d-701         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-702         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-703         [-1, 1632, 14, 14]               0\n",
      "            SiLU-704         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-705         [-1, 1632, 14, 14]          40,800\n",
      "          Conv2d-706         [-1, 1632, 14, 14]          40,800\n",
      "     BatchNorm2d-707         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-708         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-709         [-1, 1632, 14, 14]               0\n",
      "            SiLU-710         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-711             [-1, 68, 1, 1]         111,044\n",
      "          Conv2d-712             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-713             [-1, 68, 1, 1]               0\n",
      "            SiLU-714             [-1, 68, 1, 1]               0\n",
      "          Conv2d-715           [-1, 1632, 1, 1]         112,608\n",
      "          Conv2d-716           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-717           [-1, 1632, 1, 1]               0\n",
      "         Sigmoid-718           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-719         [-1, 1632, 14, 14]               0\n",
      "   SqueezeExcite-720         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-721          [-1, 272, 14, 14]         443,904\n",
      "          Conv2d-722          [-1, 272, 14, 14]         443,904\n",
      "     BatchNorm2d-723          [-1, 272, 14, 14]             544\n",
      "     BatchNorm2d-724          [-1, 272, 14, 14]             544\n",
      "InvertedResidual-725          [-1, 272, 14, 14]               0\n",
      "InvertedResidual-726          [-1, 272, 14, 14]               0\n",
      "          Conv2d-727         [-1, 1632, 14, 14]         443,904\n",
      "          Conv2d-728         [-1, 1632, 14, 14]         443,904\n",
      "     BatchNorm2d-729         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-730         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-731         [-1, 1632, 14, 14]               0\n",
      "            SiLU-732         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-733         [-1, 1632, 14, 14]          40,800\n",
      "          Conv2d-734         [-1, 1632, 14, 14]          40,800\n",
      "     BatchNorm2d-735         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-736         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-737         [-1, 1632, 14, 14]               0\n",
      "            SiLU-738         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-739             [-1, 68, 1, 1]         111,044\n",
      "          Conv2d-740             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-741             [-1, 68, 1, 1]               0\n",
      "            SiLU-742             [-1, 68, 1, 1]               0\n",
      "          Conv2d-743           [-1, 1632, 1, 1]         112,608\n",
      "          Conv2d-744           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-745           [-1, 1632, 1, 1]               0\n",
      "         Sigmoid-746           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-747         [-1, 1632, 14, 14]               0\n",
      "   SqueezeExcite-748         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-749          [-1, 272, 14, 14]         443,904\n",
      "          Conv2d-750          [-1, 272, 14, 14]         443,904\n",
      "     BatchNorm2d-751          [-1, 272, 14, 14]             544\n",
      "     BatchNorm2d-752          [-1, 272, 14, 14]             544\n",
      "InvertedResidual-753          [-1, 272, 14, 14]               0\n",
      "InvertedResidual-754          [-1, 272, 14, 14]               0\n",
      "          Conv2d-755         [-1, 1632, 14, 14]         443,904\n",
      "          Conv2d-756         [-1, 1632, 14, 14]         443,904\n",
      "     BatchNorm2d-757         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-758         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-759         [-1, 1632, 14, 14]               0\n",
      "            SiLU-760         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-761         [-1, 1632, 14, 14]          40,800\n",
      "          Conv2d-762         [-1, 1632, 14, 14]          40,800\n",
      "     BatchNorm2d-763         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-764         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-765         [-1, 1632, 14, 14]               0\n",
      "            SiLU-766         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-767             [-1, 68, 1, 1]         111,044\n",
      "          Conv2d-768             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-769             [-1, 68, 1, 1]               0\n",
      "            SiLU-770             [-1, 68, 1, 1]               0\n",
      "          Conv2d-771           [-1, 1632, 1, 1]         112,608\n",
      "          Conv2d-772           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-773           [-1, 1632, 1, 1]               0\n",
      "         Sigmoid-774           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-775         [-1, 1632, 14, 14]               0\n",
      "   SqueezeExcite-776         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-777          [-1, 272, 14, 14]         443,904\n",
      "          Conv2d-778          [-1, 272, 14, 14]         443,904\n",
      "     BatchNorm2d-779          [-1, 272, 14, 14]             544\n",
      "     BatchNorm2d-780          [-1, 272, 14, 14]             544\n",
      "InvertedResidual-781          [-1, 272, 14, 14]               0\n",
      "InvertedResidual-782          [-1, 272, 14, 14]               0\n",
      "          Conv2d-783         [-1, 1632, 14, 14]         443,904\n",
      "          Conv2d-784         [-1, 1632, 14, 14]         443,904\n",
      "     BatchNorm2d-785         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-786         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-787         [-1, 1632, 14, 14]               0\n",
      "            SiLU-788         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-789         [-1, 1632, 14, 14]          40,800\n",
      "          Conv2d-790         [-1, 1632, 14, 14]          40,800\n",
      "     BatchNorm2d-791         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-792         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-793         [-1, 1632, 14, 14]               0\n",
      "            SiLU-794         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-795             [-1, 68, 1, 1]         111,044\n",
      "          Conv2d-796             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-797             [-1, 68, 1, 1]               0\n",
      "            SiLU-798             [-1, 68, 1, 1]               0\n",
      "          Conv2d-799           [-1, 1632, 1, 1]         112,608\n",
      "          Conv2d-800           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-801           [-1, 1632, 1, 1]               0\n",
      "         Sigmoid-802           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-803         [-1, 1632, 14, 14]               0\n",
      "   SqueezeExcite-804         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-805          [-1, 272, 14, 14]         443,904\n",
      "          Conv2d-806          [-1, 272, 14, 14]         443,904\n",
      "     BatchNorm2d-807          [-1, 272, 14, 14]             544\n",
      "     BatchNorm2d-808          [-1, 272, 14, 14]             544\n",
      "InvertedResidual-809          [-1, 272, 14, 14]               0\n",
      "InvertedResidual-810          [-1, 272, 14, 14]               0\n",
      "          Conv2d-811         [-1, 1632, 14, 14]         443,904\n",
      "          Conv2d-812         [-1, 1632, 14, 14]         443,904\n",
      "     BatchNorm2d-813         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-814         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-815         [-1, 1632, 14, 14]               0\n",
      "            SiLU-816         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-817         [-1, 1632, 14, 14]          40,800\n",
      "          Conv2d-818         [-1, 1632, 14, 14]          40,800\n",
      "     BatchNorm2d-819         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-820         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-821         [-1, 1632, 14, 14]               0\n",
      "            SiLU-822         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-823             [-1, 68, 1, 1]         111,044\n",
      "          Conv2d-824             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-825             [-1, 68, 1, 1]               0\n",
      "            SiLU-826             [-1, 68, 1, 1]               0\n",
      "          Conv2d-827           [-1, 1632, 1, 1]         112,608\n",
      "          Conv2d-828           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-829           [-1, 1632, 1, 1]               0\n",
      "         Sigmoid-830           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-831         [-1, 1632, 14, 14]               0\n",
      "   SqueezeExcite-832         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-833          [-1, 272, 14, 14]         443,904\n",
      "          Conv2d-834          [-1, 272, 14, 14]         443,904\n",
      "     BatchNorm2d-835          [-1, 272, 14, 14]             544\n",
      "     BatchNorm2d-836          [-1, 272, 14, 14]             544\n",
      "InvertedResidual-837          [-1, 272, 14, 14]               0\n",
      "InvertedResidual-838          [-1, 272, 14, 14]               0\n",
      "          Conv2d-839         [-1, 1632, 14, 14]         443,904\n",
      "          Conv2d-840         [-1, 1632, 14, 14]         443,904\n",
      "     BatchNorm2d-841         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-842         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-843         [-1, 1632, 14, 14]               0\n",
      "            SiLU-844         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-845         [-1, 1632, 14, 14]          14,688\n",
      "          Conv2d-846         [-1, 1632, 14, 14]          14,688\n",
      "     BatchNorm2d-847         [-1, 1632, 14, 14]           3,264\n",
      "     BatchNorm2d-848         [-1, 1632, 14, 14]           3,264\n",
      "            SiLU-849         [-1, 1632, 14, 14]               0\n",
      "            SiLU-850         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-851             [-1, 68, 1, 1]         111,044\n",
      "          Conv2d-852             [-1, 68, 1, 1]         111,044\n",
      "            SiLU-853             [-1, 68, 1, 1]               0\n",
      "            SiLU-854             [-1, 68, 1, 1]               0\n",
      "          Conv2d-855           [-1, 1632, 1, 1]         112,608\n",
      "          Conv2d-856           [-1, 1632, 1, 1]         112,608\n",
      "         Sigmoid-857           [-1, 1632, 1, 1]               0\n",
      "         Sigmoid-858           [-1, 1632, 1, 1]               0\n",
      "   SqueezeExcite-859         [-1, 1632, 14, 14]               0\n",
      "   SqueezeExcite-860         [-1, 1632, 14, 14]               0\n",
      "          Conv2d-861          [-1, 448, 14, 14]         731,136\n",
      "          Conv2d-862          [-1, 448, 14, 14]         731,136\n",
      "     BatchNorm2d-863          [-1, 448, 14, 14]             896\n",
      "     BatchNorm2d-864          [-1, 448, 14, 14]             896\n",
      "InvertedResidual-865          [-1, 448, 14, 14]               0\n",
      "InvertedResidual-866          [-1, 448, 14, 14]               0\n",
      "          Conv2d-867         [-1, 2688, 14, 14]       1,204,224\n",
      "          Conv2d-868         [-1, 2688, 14, 14]       1,204,224\n",
      "     BatchNorm2d-869         [-1, 2688, 14, 14]           5,376\n",
      "     BatchNorm2d-870         [-1, 2688, 14, 14]           5,376\n",
      "            SiLU-871         [-1, 2688, 14, 14]               0\n",
      "            SiLU-872         [-1, 2688, 14, 14]               0\n",
      "          Conv2d-873         [-1, 2688, 14, 14]          24,192\n",
      "          Conv2d-874         [-1, 2688, 14, 14]          24,192\n",
      "     BatchNorm2d-875         [-1, 2688, 14, 14]           5,376\n",
      "     BatchNorm2d-876         [-1, 2688, 14, 14]           5,376\n",
      "            SiLU-877         [-1, 2688, 14, 14]               0\n",
      "            SiLU-878         [-1, 2688, 14, 14]               0\n",
      "          Conv2d-879            [-1, 112, 1, 1]         301,168\n",
      "          Conv2d-880            [-1, 112, 1, 1]         301,168\n",
      "            SiLU-881            [-1, 112, 1, 1]               0\n",
      "            SiLU-882            [-1, 112, 1, 1]               0\n",
      "          Conv2d-883           [-1, 2688, 1, 1]         303,744\n",
      "          Conv2d-884           [-1, 2688, 1, 1]         303,744\n",
      "         Sigmoid-885           [-1, 2688, 1, 1]               0\n",
      "         Sigmoid-886           [-1, 2688, 1, 1]               0\n",
      "   SqueezeExcite-887         [-1, 2688, 14, 14]               0\n",
      "   SqueezeExcite-888         [-1, 2688, 14, 14]               0\n",
      "          Conv2d-889          [-1, 448, 14, 14]       1,204,224\n",
      "          Conv2d-890          [-1, 448, 14, 14]       1,204,224\n",
      "     BatchNorm2d-891          [-1, 448, 14, 14]             896\n",
      "     BatchNorm2d-892          [-1, 448, 14, 14]             896\n",
      "InvertedResidual-893          [-1, 448, 14, 14]               0\n",
      "InvertedResidual-894          [-1, 448, 14, 14]               0\n",
      "EfficientNetFeatures-895         [[-1, 56, 56, 56]]               0\n",
      "EfficientNetFeatures-896         [[-1, 56, 56, 56]]               0\n",
      "          Conv2d-897          [-1, 128, 56, 56]           7,296\n",
      "     HybridEmbed-898            [-1, 3136, 128]               0\n",
      "         Dropout-899            [-1, 3136, 128]               0\n",
      "       LayerNorm-900            [-1, 3136, 128]             256\n",
      "          Linear-901              [-1, 49, 384]          49,536\n",
      "         Softmax-902            [-1, 4, 49, 49]               0\n",
      "         Dropout-903            [-1, 4, 49, 49]               0\n",
      "          Linear-904              [-1, 49, 128]          16,512\n",
      "         Dropout-905              [-1, 49, 128]               0\n",
      " WindowAttention-906              [-1, 49, 128]               0\n",
      "        Identity-907            [-1, 3136, 128]               0\n",
      "       LayerNorm-908            [-1, 3136, 128]             256\n",
      "          Linear-909            [-1, 3136, 512]          66,048\n",
      "            GELU-910            [-1, 3136, 512]               0\n",
      "         Dropout-911            [-1, 3136, 512]               0\n",
      "          Linear-912            [-1, 3136, 128]          65,664\n",
      "         Dropout-913            [-1, 3136, 128]               0\n",
      "             Mlp-914            [-1, 3136, 128]               0\n",
      "        Identity-915            [-1, 3136, 128]               0\n",
      "SwinTransformerBlock-916            [-1, 3136, 128]               0\n",
      "       LayerNorm-917            [-1, 3136, 128]             256\n",
      "          Linear-918              [-1, 49, 384]          49,536\n",
      "         Softmax-919            [-1, 4, 49, 49]               0\n",
      "         Dropout-920            [-1, 4, 49, 49]               0\n",
      "          Linear-921              [-1, 49, 128]          16,512\n",
      "         Dropout-922              [-1, 49, 128]               0\n",
      " WindowAttention-923              [-1, 49, 128]               0\n",
      "        DropPath-924            [-1, 3136, 128]               0\n",
      "       LayerNorm-925            [-1, 3136, 128]             256\n",
      "          Linear-926            [-1, 3136, 512]          66,048\n",
      "            GELU-927            [-1, 3136, 512]               0\n",
      "         Dropout-928            [-1, 3136, 512]               0\n",
      "          Linear-929            [-1, 3136, 128]          65,664\n",
      "         Dropout-930            [-1, 3136, 128]               0\n",
      "             Mlp-931            [-1, 3136, 128]               0\n",
      "        DropPath-932            [-1, 3136, 128]               0\n",
      "SwinTransformerBlock-933            [-1, 3136, 128]               0\n",
      "       LayerNorm-934             [-1, 784, 512]           1,024\n",
      "          Linear-935             [-1, 784, 256]         131,072\n",
      "    PatchMerging-936             [-1, 784, 256]               0\n",
      "      BasicLayer-937             [-1, 784, 256]               0\n",
      "       LayerNorm-938             [-1, 784, 256]             512\n",
      "          Linear-939              [-1, 49, 768]         197,376\n",
      "         Softmax-940            [-1, 8, 49, 49]               0\n",
      "         Dropout-941            [-1, 8, 49, 49]               0\n",
      "          Linear-942              [-1, 49, 256]          65,792\n",
      "         Dropout-943              [-1, 49, 256]               0\n",
      " WindowAttention-944              [-1, 49, 256]               0\n",
      "        DropPath-945             [-1, 784, 256]               0\n",
      "       LayerNorm-946             [-1, 784, 256]             512\n",
      "          Linear-947            [-1, 784, 1024]         263,168\n",
      "            GELU-948            [-1, 784, 1024]               0\n",
      "         Dropout-949            [-1, 784, 1024]               0\n",
      "          Linear-950             [-1, 784, 256]         262,400\n",
      "         Dropout-951             [-1, 784, 256]               0\n",
      "             Mlp-952             [-1, 784, 256]               0\n",
      "        DropPath-953             [-1, 784, 256]               0\n",
      "SwinTransformerBlock-954             [-1, 784, 256]               0\n",
      "       LayerNorm-955             [-1, 784, 256]             512\n",
      "          Linear-956              [-1, 49, 768]         197,376\n",
      "         Softmax-957            [-1, 8, 49, 49]               0\n",
      "         Dropout-958            [-1, 8, 49, 49]               0\n",
      "          Linear-959              [-1, 49, 256]          65,792\n",
      "         Dropout-960              [-1, 49, 256]               0\n",
      " WindowAttention-961              [-1, 49, 256]               0\n",
      "        DropPath-962             [-1, 784, 256]               0\n",
      "       LayerNorm-963             [-1, 784, 256]             512\n",
      "          Linear-964            [-1, 784, 1024]         263,168\n",
      "            GELU-965            [-1, 784, 1024]               0\n",
      "         Dropout-966            [-1, 784, 1024]               0\n",
      "          Linear-967             [-1, 784, 256]         262,400\n",
      "         Dropout-968             [-1, 784, 256]               0\n",
      "             Mlp-969             [-1, 784, 256]               0\n",
      "        DropPath-970             [-1, 784, 256]               0\n",
      "SwinTransformerBlock-971             [-1, 784, 256]               0\n",
      "       LayerNorm-972            [-1, 196, 1024]           2,048\n",
      "          Linear-973             [-1, 196, 512]         524,288\n",
      "    PatchMerging-974             [-1, 196, 512]               0\n",
      "      BasicLayer-975             [-1, 196, 512]               0\n",
      "       LayerNorm-976             [-1, 196, 512]           1,024\n",
      "          Linear-977             [-1, 49, 1536]         787,968\n",
      "         Softmax-978           [-1, 16, 49, 49]               0\n",
      "         Dropout-979           [-1, 16, 49, 49]               0\n",
      "          Linear-980              [-1, 49, 512]         262,656\n",
      "         Dropout-981              [-1, 49, 512]               0\n",
      " WindowAttention-982              [-1, 49, 512]               0\n",
      "        DropPath-983             [-1, 196, 512]               0\n",
      "       LayerNorm-984             [-1, 196, 512]           1,024\n",
      "          Linear-985            [-1, 196, 2048]       1,050,624\n",
      "            GELU-986            [-1, 196, 2048]               0\n",
      "         Dropout-987            [-1, 196, 2048]               0\n",
      "          Linear-988             [-1, 196, 512]       1,049,088\n",
      "         Dropout-989             [-1, 196, 512]               0\n",
      "             Mlp-990             [-1, 196, 512]               0\n",
      "        DropPath-991             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-992             [-1, 196, 512]               0\n",
      "       LayerNorm-993             [-1, 196, 512]           1,024\n",
      "          Linear-994             [-1, 49, 1536]         787,968\n",
      "         Softmax-995           [-1, 16, 49, 49]               0\n",
      "         Dropout-996           [-1, 16, 49, 49]               0\n",
      "          Linear-997              [-1, 49, 512]         262,656\n",
      "         Dropout-998              [-1, 49, 512]               0\n",
      " WindowAttention-999              [-1, 49, 512]               0\n",
      "       DropPath-1000             [-1, 196, 512]               0\n",
      "      LayerNorm-1001             [-1, 196, 512]           1,024\n",
      "         Linear-1002            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1003            [-1, 196, 2048]               0\n",
      "        Dropout-1004            [-1, 196, 2048]               0\n",
      "         Linear-1005             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1006             [-1, 196, 512]               0\n",
      "            Mlp-1007             [-1, 196, 512]               0\n",
      "       DropPath-1008             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1009             [-1, 196, 512]               0\n",
      "      LayerNorm-1010             [-1, 196, 512]           1,024\n",
      "         Linear-1011             [-1, 49, 1536]         787,968\n",
      "        Softmax-1012           [-1, 16, 49, 49]               0\n",
      "        Dropout-1013           [-1, 16, 49, 49]               0\n",
      "         Linear-1014              [-1, 49, 512]         262,656\n",
      "        Dropout-1015              [-1, 49, 512]               0\n",
      "WindowAttention-1016              [-1, 49, 512]               0\n",
      "       DropPath-1017             [-1, 196, 512]               0\n",
      "      LayerNorm-1018             [-1, 196, 512]           1,024\n",
      "         Linear-1019            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1020            [-1, 196, 2048]               0\n",
      "        Dropout-1021            [-1, 196, 2048]               0\n",
      "         Linear-1022             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1023             [-1, 196, 512]               0\n",
      "            Mlp-1024             [-1, 196, 512]               0\n",
      "       DropPath-1025             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1026             [-1, 196, 512]               0\n",
      "      LayerNorm-1027             [-1, 196, 512]           1,024\n",
      "         Linear-1028             [-1, 49, 1536]         787,968\n",
      "        Softmax-1029           [-1, 16, 49, 49]               0\n",
      "        Dropout-1030           [-1, 16, 49, 49]               0\n",
      "         Linear-1031              [-1, 49, 512]         262,656\n",
      "        Dropout-1032              [-1, 49, 512]               0\n",
      "WindowAttention-1033              [-1, 49, 512]               0\n",
      "       DropPath-1034             [-1, 196, 512]               0\n",
      "      LayerNorm-1035             [-1, 196, 512]           1,024\n",
      "         Linear-1036            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1037            [-1, 196, 2048]               0\n",
      "        Dropout-1038            [-1, 196, 2048]               0\n",
      "         Linear-1039             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1040             [-1, 196, 512]               0\n",
      "            Mlp-1041             [-1, 196, 512]               0\n",
      "       DropPath-1042             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1043             [-1, 196, 512]               0\n",
      "      LayerNorm-1044             [-1, 196, 512]           1,024\n",
      "         Linear-1045             [-1, 49, 1536]         787,968\n",
      "        Softmax-1046           [-1, 16, 49, 49]               0\n",
      "        Dropout-1047           [-1, 16, 49, 49]               0\n",
      "         Linear-1048              [-1, 49, 512]         262,656\n",
      "        Dropout-1049              [-1, 49, 512]               0\n",
      "WindowAttention-1050              [-1, 49, 512]               0\n",
      "       DropPath-1051             [-1, 196, 512]               0\n",
      "      LayerNorm-1052             [-1, 196, 512]           1,024\n",
      "         Linear-1053            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1054            [-1, 196, 2048]               0\n",
      "        Dropout-1055            [-1, 196, 2048]               0\n",
      "         Linear-1056             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1057             [-1, 196, 512]               0\n",
      "            Mlp-1058             [-1, 196, 512]               0\n",
      "       DropPath-1059             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1060             [-1, 196, 512]               0\n",
      "      LayerNorm-1061             [-1, 196, 512]           1,024\n",
      "         Linear-1062             [-1, 49, 1536]         787,968\n",
      "        Softmax-1063           [-1, 16, 49, 49]               0\n",
      "        Dropout-1064           [-1, 16, 49, 49]               0\n",
      "         Linear-1065              [-1, 49, 512]         262,656\n",
      "        Dropout-1066              [-1, 49, 512]               0\n",
      "WindowAttention-1067              [-1, 49, 512]               0\n",
      "       DropPath-1068             [-1, 196, 512]               0\n",
      "      LayerNorm-1069             [-1, 196, 512]           1,024\n",
      "         Linear-1070            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1071            [-1, 196, 2048]               0\n",
      "        Dropout-1072            [-1, 196, 2048]               0\n",
      "         Linear-1073             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1074             [-1, 196, 512]               0\n",
      "            Mlp-1075             [-1, 196, 512]               0\n",
      "       DropPath-1076             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1077             [-1, 196, 512]               0\n",
      "      LayerNorm-1078             [-1, 196, 512]           1,024\n",
      "         Linear-1079             [-1, 49, 1536]         787,968\n",
      "        Softmax-1080           [-1, 16, 49, 49]               0\n",
      "        Dropout-1081           [-1, 16, 49, 49]               0\n",
      "         Linear-1082              [-1, 49, 512]         262,656\n",
      "        Dropout-1083              [-1, 49, 512]               0\n",
      "WindowAttention-1084              [-1, 49, 512]               0\n",
      "       DropPath-1085             [-1, 196, 512]               0\n",
      "      LayerNorm-1086             [-1, 196, 512]           1,024\n",
      "         Linear-1087            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1088            [-1, 196, 2048]               0\n",
      "        Dropout-1089            [-1, 196, 2048]               0\n",
      "         Linear-1090             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1091             [-1, 196, 512]               0\n",
      "            Mlp-1092             [-1, 196, 512]               0\n",
      "       DropPath-1093             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1094             [-1, 196, 512]               0\n",
      "      LayerNorm-1095             [-1, 196, 512]           1,024\n",
      "         Linear-1096             [-1, 49, 1536]         787,968\n",
      "        Softmax-1097           [-1, 16, 49, 49]               0\n",
      "        Dropout-1098           [-1, 16, 49, 49]               0\n",
      "         Linear-1099              [-1, 49, 512]         262,656\n",
      "        Dropout-1100              [-1, 49, 512]               0\n",
      "WindowAttention-1101              [-1, 49, 512]               0\n",
      "       DropPath-1102             [-1, 196, 512]               0\n",
      "      LayerNorm-1103             [-1, 196, 512]           1,024\n",
      "         Linear-1104            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1105            [-1, 196, 2048]               0\n",
      "        Dropout-1106            [-1, 196, 2048]               0\n",
      "         Linear-1107             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1108             [-1, 196, 512]               0\n",
      "            Mlp-1109             [-1, 196, 512]               0\n",
      "       DropPath-1110             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1111             [-1, 196, 512]               0\n",
      "      LayerNorm-1112             [-1, 196, 512]           1,024\n",
      "         Linear-1113             [-1, 49, 1536]         787,968\n",
      "        Softmax-1114           [-1, 16, 49, 49]               0\n",
      "        Dropout-1115           [-1, 16, 49, 49]               0\n",
      "         Linear-1116              [-1, 49, 512]         262,656\n",
      "        Dropout-1117              [-1, 49, 512]               0\n",
      "WindowAttention-1118              [-1, 49, 512]               0\n",
      "       DropPath-1119             [-1, 196, 512]               0\n",
      "      LayerNorm-1120             [-1, 196, 512]           1,024\n",
      "         Linear-1121            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1122            [-1, 196, 2048]               0\n",
      "        Dropout-1123            [-1, 196, 2048]               0\n",
      "         Linear-1124             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1125             [-1, 196, 512]               0\n",
      "            Mlp-1126             [-1, 196, 512]               0\n",
      "       DropPath-1127             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1128             [-1, 196, 512]               0\n",
      "      LayerNorm-1129             [-1, 196, 512]           1,024\n",
      "         Linear-1130             [-1, 49, 1536]         787,968\n",
      "        Softmax-1131           [-1, 16, 49, 49]               0\n",
      "        Dropout-1132           [-1, 16, 49, 49]               0\n",
      "         Linear-1133              [-1, 49, 512]         262,656\n",
      "        Dropout-1134              [-1, 49, 512]               0\n",
      "WindowAttention-1135              [-1, 49, 512]               0\n",
      "       DropPath-1136             [-1, 196, 512]               0\n",
      "      LayerNorm-1137             [-1, 196, 512]           1,024\n",
      "         Linear-1138            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1139            [-1, 196, 2048]               0\n",
      "        Dropout-1140            [-1, 196, 2048]               0\n",
      "         Linear-1141             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1142             [-1, 196, 512]               0\n",
      "            Mlp-1143             [-1, 196, 512]               0\n",
      "       DropPath-1144             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1145             [-1, 196, 512]               0\n",
      "      LayerNorm-1146             [-1, 196, 512]           1,024\n",
      "         Linear-1147             [-1, 49, 1536]         787,968\n",
      "        Softmax-1148           [-1, 16, 49, 49]               0\n",
      "        Dropout-1149           [-1, 16, 49, 49]               0\n",
      "         Linear-1150              [-1, 49, 512]         262,656\n",
      "        Dropout-1151              [-1, 49, 512]               0\n",
      "WindowAttention-1152              [-1, 49, 512]               0\n",
      "       DropPath-1153             [-1, 196, 512]               0\n",
      "      LayerNorm-1154             [-1, 196, 512]           1,024\n",
      "         Linear-1155            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1156            [-1, 196, 2048]               0\n",
      "        Dropout-1157            [-1, 196, 2048]               0\n",
      "         Linear-1158             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1159             [-1, 196, 512]               0\n",
      "            Mlp-1160             [-1, 196, 512]               0\n",
      "       DropPath-1161             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1162             [-1, 196, 512]               0\n",
      "      LayerNorm-1163             [-1, 196, 512]           1,024\n",
      "         Linear-1164             [-1, 49, 1536]         787,968\n",
      "        Softmax-1165           [-1, 16, 49, 49]               0\n",
      "        Dropout-1166           [-1, 16, 49, 49]               0\n",
      "         Linear-1167              [-1, 49, 512]         262,656\n",
      "        Dropout-1168              [-1, 49, 512]               0\n",
      "WindowAttention-1169              [-1, 49, 512]               0\n",
      "       DropPath-1170             [-1, 196, 512]               0\n",
      "      LayerNorm-1171             [-1, 196, 512]           1,024\n",
      "         Linear-1172            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1173            [-1, 196, 2048]               0\n",
      "        Dropout-1174            [-1, 196, 2048]               0\n",
      "         Linear-1175             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1176             [-1, 196, 512]               0\n",
      "            Mlp-1177             [-1, 196, 512]               0\n",
      "       DropPath-1178             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1179             [-1, 196, 512]               0\n",
      "      LayerNorm-1180             [-1, 196, 512]           1,024\n",
      "         Linear-1181             [-1, 49, 1536]         787,968\n",
      "        Softmax-1182           [-1, 16, 49, 49]               0\n",
      "        Dropout-1183           [-1, 16, 49, 49]               0\n",
      "         Linear-1184              [-1, 49, 512]         262,656\n",
      "        Dropout-1185              [-1, 49, 512]               0\n",
      "WindowAttention-1186              [-1, 49, 512]               0\n",
      "       DropPath-1187             [-1, 196, 512]               0\n",
      "      LayerNorm-1188             [-1, 196, 512]           1,024\n",
      "         Linear-1189            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1190            [-1, 196, 2048]               0\n",
      "        Dropout-1191            [-1, 196, 2048]               0\n",
      "         Linear-1192             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1193             [-1, 196, 512]               0\n",
      "            Mlp-1194             [-1, 196, 512]               0\n",
      "       DropPath-1195             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1196             [-1, 196, 512]               0\n",
      "      LayerNorm-1197             [-1, 196, 512]           1,024\n",
      "         Linear-1198             [-1, 49, 1536]         787,968\n",
      "        Softmax-1199           [-1, 16, 49, 49]               0\n",
      "        Dropout-1200           [-1, 16, 49, 49]               0\n",
      "         Linear-1201              [-1, 49, 512]         262,656\n",
      "        Dropout-1202              [-1, 49, 512]               0\n",
      "WindowAttention-1203              [-1, 49, 512]               0\n",
      "       DropPath-1204             [-1, 196, 512]               0\n",
      "      LayerNorm-1205             [-1, 196, 512]           1,024\n",
      "         Linear-1206            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1207            [-1, 196, 2048]               0\n",
      "        Dropout-1208            [-1, 196, 2048]               0\n",
      "         Linear-1209             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1210             [-1, 196, 512]               0\n",
      "            Mlp-1211             [-1, 196, 512]               0\n",
      "       DropPath-1212             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1213             [-1, 196, 512]               0\n",
      "      LayerNorm-1214             [-1, 196, 512]           1,024\n",
      "         Linear-1215             [-1, 49, 1536]         787,968\n",
      "        Softmax-1216           [-1, 16, 49, 49]               0\n",
      "        Dropout-1217           [-1, 16, 49, 49]               0\n",
      "         Linear-1218              [-1, 49, 512]         262,656\n",
      "        Dropout-1219              [-1, 49, 512]               0\n",
      "WindowAttention-1220              [-1, 49, 512]               0\n",
      "       DropPath-1221             [-1, 196, 512]               0\n",
      "      LayerNorm-1222             [-1, 196, 512]           1,024\n",
      "         Linear-1223            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1224            [-1, 196, 2048]               0\n",
      "        Dropout-1225            [-1, 196, 2048]               0\n",
      "         Linear-1226             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1227             [-1, 196, 512]               0\n",
      "            Mlp-1228             [-1, 196, 512]               0\n",
      "       DropPath-1229             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1230             [-1, 196, 512]               0\n",
      "      LayerNorm-1231             [-1, 196, 512]           1,024\n",
      "         Linear-1232             [-1, 49, 1536]         787,968\n",
      "        Softmax-1233           [-1, 16, 49, 49]               0\n",
      "        Dropout-1234           [-1, 16, 49, 49]               0\n",
      "         Linear-1235              [-1, 49, 512]         262,656\n",
      "        Dropout-1236              [-1, 49, 512]               0\n",
      "WindowAttention-1237              [-1, 49, 512]               0\n",
      "       DropPath-1238             [-1, 196, 512]               0\n",
      "      LayerNorm-1239             [-1, 196, 512]           1,024\n",
      "         Linear-1240            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1241            [-1, 196, 2048]               0\n",
      "        Dropout-1242            [-1, 196, 2048]               0\n",
      "         Linear-1243             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1244             [-1, 196, 512]               0\n",
      "            Mlp-1245             [-1, 196, 512]               0\n",
      "       DropPath-1246             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1247             [-1, 196, 512]               0\n",
      "      LayerNorm-1248             [-1, 196, 512]           1,024\n",
      "         Linear-1249             [-1, 49, 1536]         787,968\n",
      "        Softmax-1250           [-1, 16, 49, 49]               0\n",
      "        Dropout-1251           [-1, 16, 49, 49]               0\n",
      "         Linear-1252              [-1, 49, 512]         262,656\n",
      "        Dropout-1253              [-1, 49, 512]               0\n",
      "WindowAttention-1254              [-1, 49, 512]               0\n",
      "       DropPath-1255             [-1, 196, 512]               0\n",
      "      LayerNorm-1256             [-1, 196, 512]           1,024\n",
      "         Linear-1257            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1258            [-1, 196, 2048]               0\n",
      "        Dropout-1259            [-1, 196, 2048]               0\n",
      "         Linear-1260             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1261             [-1, 196, 512]               0\n",
      "            Mlp-1262             [-1, 196, 512]               0\n",
      "       DropPath-1263             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1264             [-1, 196, 512]               0\n",
      "      LayerNorm-1265             [-1, 196, 512]           1,024\n",
      "         Linear-1266             [-1, 49, 1536]         787,968\n",
      "        Softmax-1267           [-1, 16, 49, 49]               0\n",
      "        Dropout-1268           [-1, 16, 49, 49]               0\n",
      "         Linear-1269              [-1, 49, 512]         262,656\n",
      "        Dropout-1270              [-1, 49, 512]               0\n",
      "WindowAttention-1271              [-1, 49, 512]               0\n",
      "       DropPath-1272             [-1, 196, 512]               0\n",
      "      LayerNorm-1273             [-1, 196, 512]           1,024\n",
      "         Linear-1274            [-1, 196, 2048]       1,050,624\n",
      "           GELU-1275            [-1, 196, 2048]               0\n",
      "        Dropout-1276            [-1, 196, 2048]               0\n",
      "         Linear-1277             [-1, 196, 512]       1,049,088\n",
      "        Dropout-1278             [-1, 196, 512]               0\n",
      "            Mlp-1279             [-1, 196, 512]               0\n",
      "       DropPath-1280             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-1281             [-1, 196, 512]               0\n",
      "      LayerNorm-1282             [-1, 49, 2048]           4,096\n",
      "         Linear-1283             [-1, 49, 1024]       2,097,152\n",
      "   PatchMerging-1284             [-1, 49, 1024]               0\n",
      "     BasicLayer-1285             [-1, 49, 1024]               0\n",
      "      LayerNorm-1286             [-1, 49, 1024]           2,048\n",
      "         Linear-1287             [-1, 49, 3072]       3,148,800\n",
      "        Softmax-1288           [-1, 32, 49, 49]               0\n",
      "        Dropout-1289           [-1, 32, 49, 49]               0\n",
      "         Linear-1290             [-1, 49, 1024]       1,049,600\n",
      "        Dropout-1291             [-1, 49, 1024]               0\n",
      "WindowAttention-1292             [-1, 49, 1024]               0\n",
      "       DropPath-1293             [-1, 49, 1024]               0\n",
      "      LayerNorm-1294             [-1, 49, 1024]           2,048\n",
      "         Linear-1295             [-1, 49, 4096]       4,198,400\n",
      "           GELU-1296             [-1, 49, 4096]               0\n",
      "        Dropout-1297             [-1, 49, 4096]               0\n",
      "         Linear-1298             [-1, 49, 1024]       4,195,328\n",
      "        Dropout-1299             [-1, 49, 1024]               0\n",
      "            Mlp-1300             [-1, 49, 1024]               0\n",
      "       DropPath-1301             [-1, 49, 1024]               0\n",
      "SwinTransformerBlock-1302             [-1, 49, 1024]               0\n",
      "      LayerNorm-1303             [-1, 49, 1024]           2,048\n",
      "         Linear-1304             [-1, 49, 3072]       3,148,800\n",
      "        Softmax-1305           [-1, 32, 49, 49]               0\n",
      "        Dropout-1306           [-1, 32, 49, 49]               0\n",
      "         Linear-1307             [-1, 49, 1024]       1,049,600\n",
      "        Dropout-1308             [-1, 49, 1024]               0\n",
      "WindowAttention-1309             [-1, 49, 1024]               0\n",
      "       DropPath-1310             [-1, 49, 1024]               0\n",
      "      LayerNorm-1311             [-1, 49, 1024]           2,048\n",
      "         Linear-1312             [-1, 49, 4096]       4,198,400\n",
      "           GELU-1313             [-1, 49, 4096]               0\n",
      "        Dropout-1314             [-1, 49, 4096]               0\n",
      "         Linear-1315             [-1, 49, 1024]       4,195,328\n",
      "        Dropout-1316             [-1, 49, 1024]               0\n",
      "            Mlp-1317             [-1, 49, 1024]               0\n",
      "       DropPath-1318             [-1, 49, 1024]               0\n",
      "SwinTransformerBlock-1319             [-1, 49, 1024]               0\n",
      "     BasicLayer-1320             [-1, 49, 1024]               0\n",
      "      LayerNorm-1321             [-1, 49, 1024]           2,048\n",
      "AdaptiveAvgPool1d-1322              [-1, 1024, 1]               0\n",
      "       Identity-1323                 [-1, 1024]               0\n",
      "SwinTransformer-1324                 [-1, 1024]               0\n",
      "         Linear-1325                    [-1, 1]           1,025\n",
      "================================================================\n",
      "Total params: 120,165,905\n",
      "Trainable params: 120,165,905\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 4088.77\n",
      "Params size (MB): 458.40\n",
      "Estimated Total Size (MB): 4549.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3,448,448))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1843],\n",
      "        [-0.1658],\n",
      "        [-0.2310],\n",
      "        [-0.2037],\n",
      "        [-0.2477]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "img = torch.randn(5, 3, CONFIG['img_size'], CONFIG['img_size']).to(CONFIG['device'])\n",
    "print(\n",
    "model(img)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, targets):\n",
    "    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    # train 모드로 변경\n",
    "    model.train()\n",
    "\n",
    "    # for the Mixed Precision \n",
    "    # Pytorch 예제 : https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    for step, (images, targets) in bar:\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        with amp.autocast(enabled=True):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs,targets)\n",
    "            loss = loss / CONFIG['n_accumulate']\n",
    "\n",
    "        # loss를 Scale\n",
    "        # Scaled Grdients를 계산(call)하기 위해 scaled loss를 backward() \n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # loss.item()은 loss를 Python Float으로 반환\n",
    "        # loss.item()은 batch data의 average loss이므로, sum of loss를 구하기 위해 batch_size를 곱해준다\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        bar.set_postfix(Epoch = epoch, Train_Loss = epoch_loss, LR = optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Garbage Collector\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size= 0\n",
    "    running_loss = 0\n",
    "\n",
    "    TARGETS= []\n",
    "    PREDS = []\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n",
    "\n",
    "    for step, (images, targets) in bar:\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n",
    "        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n",
    "\n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, LR = optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    TARGETS = np.concatenate(TARGETS)\n",
    "    PREDS = np.concatenate(PREDS)\n",
    "    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    return epoch_loss, val_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimzier, scheduler, device, num_epochs):\n",
    "    # To automatically log graidents\n",
    "    wandb.watch(model, log_freq=100)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU:{}\\n\".format(torch.cuda.get_device_name()))\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_rmse=np.inf\n",
    "    history=defaultdict(list)\n",
    "\n",
    "    # num_epochs만큼, train과 val을 실행한다\n",
    "    for epoch in range(1,num_epochs +1):\n",
    "        gc.collect()\n",
    "        \n",
    "        # train one epoch\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader=train_loader, devcie=CONFIG['device'], epoch=epoch)\n",
    "        val_epoch_loss, val_epoch_rmse = valid_one_epoch(model, valid_loader, device=CONFIG['device'], epoch=epoch)\n",
    "\n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        history['Valid RMSE'].append(val_epoch_rmse)\n",
    "\n",
    "        # Log the metrics\n",
    "        wandb.log({\"Train Loss\" : train_epoch_loss})\n",
    "        wandb.log({'Valid Loss' : val_epoch_loss})\n",
    "        wandb.log({'Valid RMSE' : val_epoch_rmse})\n",
    "\n",
    "        print(f'Valid RMSE : {val_epoch_rmse}')\n",
    "\n",
    "\n",
    "        # deep copy the model\n",
    "        if val_epoch_rmse <= best_epoch_rmse:\n",
    "            print(f'Validation Loss improved( {best_epoch_rmse} ---> {val_epoch_rmse}  )')\n",
    "            best_epoch_rmse = val_epoch_rmse\n",
    "            run.summary['Best RMSE'] = best_epoch_rmse\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = 'RMSE{:.4f}_epoch{:.0f}.bin'.format(best_epoch_rmse, epoch)\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            wandb.save(PATH)\n",
    "            print(f'Model Saved')\n",
    "\n",
    "        print()\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print('Best RMSE: {:.4f}'.format(best_epoch_rmse))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
